{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b171763c",
   "metadata": {},
   "source": [
    "# Investigation of the impact of causality constraints on a CNN-LSTM sleep staging model\n",
    "\n",
    "### Project goals:\n",
    "##### Classify sleep stages using multi-modal sensor data (BVP, accelerometer, timestamps, temperature).\n",
    "##### Compare model performance and computation between non-causal versus causal architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402878f",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4da426bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchmetrics.classification import MulticlassCohenKappa\n",
    "from IPython.display import clear_output\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53dc944f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2671111/190206057.py:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  demo_df = pd.read_csv(demo_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TIMESTAMP', 'BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP', 'EDA', 'HR',\n",
      "       'IBI', 'Sleep_Stage', 'Obstructive_Apnea', 'Central_Apnea', 'Hypopnea',\n",
      "       'Multiple_Events'],\n",
      "      dtype='object')\n",
      "TIMESTAMP            float64\n",
      "BVP                  float64\n",
      "ACC_X                float64\n",
      "ACC_Y                 object\n",
      "ACC_Z                float64\n",
      "TEMP                 float64\n",
      "EDA                  float64\n",
      "HR                   float64\n",
      "IBI                  float64\n",
      "Sleep_Stage           object\n",
      "Obstructive_Apnea    float64\n",
      "Central_Apnea        float64\n",
      "Hypopnea             float64\n",
      "Multiple_Events      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# demo csv, get columns\n",
    "demo_csv_path = '/gpfs/data/oermannlab/users/slj9342/dl4med_25/data/physionet.org/files/dreamt/2.0.0/data_64Hz/S016_whole_df.csv'\n",
    "demo_df = pd.read_csv(demo_csv_path)\n",
    "print(demo_df.columns)\n",
    "col_dtypes = demo_df.dtypes\n",
    "print(col_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d51dae",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e38a702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TIMESTAMP', 'BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP', 'EDA', 'HR', 'IBI', 'Sleep_Stage', 'Obstructive_Apnea', 'Central_Apnea', 'Hypopnea', 'Multiple_Events']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2671111/2388715554.py:24: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(datadir_64Hz, file))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTAMP            float64\n",
      "BVP                  float64\n",
      "ACC_X                float64\n",
      "ACC_Y                 object\n",
      "ACC_Z                float64\n",
      "TEMP                 float64\n",
      "EDA                  float64\n",
      "HR                   float64\n",
      "IBI                  float64\n",
      "Sleep_Stage           object\n",
      "Obstructive_Apnea    float64\n",
      "Central_Apnea        float64\n",
      "Hypopnea             float64\n",
      "Multiple_Events      float64\n",
      "dtype: object\n",
      "❌ Column ACC_Y failed: Unable to parse string \"N1\" at position 2054657\n",
      "❌ Column Sleep_Stage failed: Unable to parse string \"P\" at position 0\n"
     ]
    }
   ],
   "source": [
    "datadir_64Hz = '/gpfs/data/oermannlab/users/slj9342/dl4med_25/data/physionet.org/files/dreamt/2.0.0/data_64Hz/' # working with 64Hz data\n",
    "\n",
    "dtype_dict = {\n",
    "    'TIMESTAMP': np.float32,\n",
    "    'BVP': np.float32,\n",
    "    'ACC_X': np.float32,\n",
    "    'ACC_Y': np.float32,\n",
    "    'ACC_Z': np.float32,\n",
    "    'TEMP': np.float32,\n",
    "    'EDA': np.float32,\n",
    "    'HR': np.float32,\n",
    "    'IBI': np.float32,  # <-- missing but important\n",
    "    'Sleep_Stage': 'category',\n",
    "    'Obstructive_Apnea': 'Int64',  # Nullable integer\n",
    "    'Central_Apnea': 'Int64',\n",
    "    'Hypopnea': 'Int64',\n",
    "    'Multiple_Events': 'Int64'\n",
    "}\n",
    "\n",
    "file = 'S016_whole_df.csv'\n",
    "df_head = pd.read_csv(os.path.join(datadir_64Hz, file), nrows=5)\n",
    "print(df_head.columns.tolist())\n",
    "\n",
    "df = pd.read_csv(os.path.join(datadir_64Hz, file))\n",
    "print(df.dtypes)\n",
    "\n",
    "df = pd.read_csv(os.path.join(datadir_64Hz, file), low_memory=False)\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        pd.to_numeric(df[col], errors='raise')\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Column {col} failed: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14a23638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max sequence length\n",
    "def safe_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "numeric_columns = [\n",
    "    'TIMESTAMP', 'BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP',\n",
    "    'EDA', 'HR', 'IBI'\n",
    "]\n",
    "converters = {col: safe_float for col in numeric_columns}\n",
    "'''\n",
    "max_length = 0\n",
    "for file in os.listdir(datadir_64Hz):\n",
    "    if file.endswith('_whole_df.csv'):\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(datadir_64Hz, file),\n",
    "            dtype={'Sleep_Stage': 'category'},\n",
    "            converters=converters,\n",
    "            low_memory=True\n",
    "        )\n",
    "        max_length = max(max_length, len(df))\n",
    "print(f\"Max sequence length: {max_length}\")\n",
    "'''\n",
    "max_length = 2493810"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fcf3ce",
   "metadata": {},
   "source": [
    "### Split subjects into train, val, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82a0a89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of subjects in train: 80\n",
      "number of subjects in val: 10\n",
      "number of subjects in test: 10\n"
     ]
    }
   ],
   "source": [
    "participant_info_df = pd.read_csv('/gpfs/data/oermannlab/users/slj9342/dl4med_25/data/physionet.org/files/dreamt/2.0.0/participant_info.csv')\n",
    "subjects_all = participant_info_df['SID']\n",
    "\n",
    "subjects_all_shuffled = participant_info_df['SID'].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "subjects_train = subjects_all_shuffled[:int(len(subjects_all_shuffled)*0.8)]\n",
    "subjects_val = subjects_all_shuffled[int(len(subjects_all_shuffled)*0.8):int(len(subjects_all_shuffled)*0.9)]\n",
    "subjects_test = subjects_all_shuffled[int(len(subjects_all_shuffled)*0.9):]\n",
    "print(f\"number of subjects in train: {len(subjects_train)}\")\n",
    "print(f\"number of subjects in val: {len(subjects_val)}\")\n",
    "print(f\"number of subjects in test: {len(subjects_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059ce4b",
   "metadata": {},
   "source": [
    "### Non-windowed dataset class\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a3ba2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLEEP_STAGE_MAPPING = {\n",
    "    \"W\": 0,    # Wake\n",
    "    \"N1\": 1,   # non-REM stage 1\n",
    "    \"N2\": 2,   # non-REM stage 2\n",
    "    \"N3\": 3,   # non-REM stage 3\n",
    "    \"R\": 4,    # REM\n",
    "    \"Missing\": -1  # Missing label\n",
    "}\n",
    "\n",
    "def forward_fill(x):\n",
    "    \"\"\"\n",
    "    Performs forward fill on a tensor. If x is 1D (shape [T]),\n",
    "    it's temporarily unsqueezed to [T, 1] and then processed.\n",
    "    Assumes the first value is valid, or fills it with zero if needed.\n",
    "    \"\"\"\n",
    "    single_channel = False\n",
    "    if x.dim() == 1:\n",
    "        x = x.unsqueeze(1)\n",
    "        single_channel = True\n",
    "    \n",
    "    T, C = x.shape\n",
    "    for c in range(C):\n",
    "        # Optionally, handle the first element if it's NaN\n",
    "        if torch.isnan(x[0, c]):\n",
    "            x[0, c] = 0.0  # or choose another default value\n",
    "        for t in range(1, T):\n",
    "            if torch.isnan(x[t, c]):\n",
    "                x[t, c] = x[t - 1, c]\n",
    "    \n",
    "    if single_channel:\n",
    "        x = x.squeeze(1)\n",
    "    return x\n",
    "\n",
    "class SleepDataset(Dataset):\n",
    "    def __init__(self, subjects_list, data_dir, max_length, downsample_freq=64, debug=False):\n",
    "        self.subjects = [{} for _ in range(len(subjects_list))]\n",
    "        self.downsample = int(64 // downsample_freq)  # Downsample factor\n",
    "        self.max_length = int(max_length // self.downsample)\n",
    "\n",
    "        for subjectNo, SID in enumerate(subjects_list):\n",
    "            # Load the data for each subject\n",
    "            file_path = os.path.join(data_dir, f\"{SID}_whole_df.csv\")\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(\n",
    "                    file_path,\n",
    "                    dtype={'Sleep_Stage': 'category'},\n",
    "                    converters=converters,\n",
    "                    low_memory=True\n",
    "                )\n",
    "                if debug:\n",
    "                    print(f\"loaded data for {SID}:\")\n",
    "\n",
    "                # Downsample the data if needed\n",
    "                if self.downsample != 1:\n",
    "                    df = df.iloc[::self.downsample].reset_index(drop=True)\n",
    "                    if debug:\n",
    "                        print(f\"After downsampling by factor {self.downsample}, rows: {len(df)}\")\n",
    "                \n",
    "                df = df[df['Sleep_Stage'] != 'P'] # remove data before PSG start\n",
    "                for col in ['ACC_X', 'ACC_Y', 'ACC_Z','BVP', 'TEMP', 'TIMESTAMP']:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                ACC = np.sqrt(df['ACC_X']**2 + df['ACC_Y']**2 + df['ACC_Z']**2) # assuming its unlikely each acc channel really carries important information\n",
    "                df_X = df[['TIMESTAMP', 'BVP', 'TEMP']].copy()\n",
    "                df_X['ACC'] = ACC\n",
    "                # Normalize the features (z-score normalization per subject)\n",
    "                TEMP_norm = (df_X['TEMP'] - df_X['TEMP'].mean()) / df_X['TEMP'].std()\n",
    "                df_X['TEMP'] = TEMP_norm\n",
    "                BVP_norm = (df_X['BVP'] - df_X['BVP'].mean()) / df_X['BVP'].std()\n",
    "                df_X['BVP'] = BVP_norm\n",
    "                df['Sleep_Stage'] = df['Sleep_Stage'].astype(str).str.strip()\n",
    "                df_Y = df['Sleep_Stage'].map(SLEEP_STAGE_MAPPING)\n",
    "                \n",
    "                # Pad/truncate the data to the downsampled max_length\n",
    "                if len(df_X) > max_length:\n",
    "                    if debug:\n",
    "                        print(f\"Truncating data for {SID} from {len(df_X)} to {max_length} samples.\")\n",
    "                    df_X = df_X.iloc[:max_length]\n",
    "                    df_Y = df_Y.iloc[:max_length]\n",
    "                else:\n",
    "                    padding_length = max_length - len(df_X)\n",
    "                    padding = pd.DataFrame(np.nan, index=np.arange(padding_length), columns=df_X.columns)\n",
    "                    df_X = pd.concat([df_X, padding], ignore_index=True)\n",
    "                    df_Y = pd.concat([df_Y, pd.Series([-1] * padding_length)], ignore_index=True)\n",
    "                self.subjects[subjectNo] = {\n",
    "                    'data': df_X.values.astype(np.float32),  # shape: [T, C]\n",
    "                    'labels': df_Y.to_numpy(),                 # shape: [T]\n",
    "                    'SID': SID\n",
    "                }\n",
    "                if debug:\n",
    "                    print(f\"Data shape for {SID}: {df_X.shape}, Labels shape: {df_Y.shape}\")\n",
    "            else:\n",
    "                warning(f\"File {file_path} does not exist. Skipping subject {SID}.\")\n",
    "    def __len__(self):\n",
    "        return len(self.subjects)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject = self.subjects[idx]\n",
    "        data = torch.tensor(subject['data'], dtype=torch.float32)\n",
    "        labels = torch.tensor(subject['labels'], dtype=torch.long)\n",
    "\n",
    "        data = forward_fill(data) # fill NaNs with previous values\n",
    "        labels = forward_fill(labels) # fill NaNs with previous values\n",
    "        return data, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7bda52",
   "metadata": {},
   "source": [
    "### Chunked Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c010b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep stage mapping as before\n",
    "SLEEP_STAGE_MAPPING = {\n",
    "    \"W\": 0,    # Wake\n",
    "    \"N1\": 1,   # non-REM stage 1\n",
    "    \"N2\": 2,   # non-REM stage 2\n",
    "    \"N3\": 3,   # non-REM stage 3\n",
    "    \"R\": 4,    # REM\n",
    "    \"Missing\": -1  # Missing label\n",
    "}\n",
    "\n",
    "def forward_fill(x):\n",
    "    \"\"\"\n",
    "    Performs forward fill on a tensor.\n",
    "    If x is 1D (shape [T]), it is temporarily unsqueezed to [T, 1].\n",
    "    Assumes the first value is valid, or fills it with zero if needed.\n",
    "    \"\"\"\n",
    "    single_channel = False\n",
    "    if x.dim() == 1:\n",
    "        x = x.unsqueeze(1)\n",
    "        single_channel = True\n",
    "\n",
    "    T, C = x.shape\n",
    "    for c in range(C):\n",
    "        if torch.isnan(x[0, c]):\n",
    "            x[0, c] = 0.0\n",
    "        for t in range(1, T):\n",
    "            if torch.isnan(x[t, c]):\n",
    "                x[t, c] = x[t - 1, c]\n",
    "    if single_channel:\n",
    "        x = x.squeeze(1)\n",
    "    return x\n",
    "\n",
    "numeric_columns = [\n",
    "    'TIMESTAMP', 'BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP',\n",
    "    'EDA', 'HR', 'IBI'\n",
    "]\n",
    "converters = {col: safe_float for col in numeric_columns}\n",
    "\n",
    "class SleepChunkDataset(Dataset):\n",
    "    def __init__(self, subjects_list, data_dir, chunk_duration=600, chunk_stride=300, downsample_freq=64, debug=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            subjects_list (list): List of subject IDs, e.g. [\"SID1\", \"SID2\", ...].\n",
    "            data_dir (str): Directory where files like \"SID_whole_df.csv\" are stored.\n",
    "            chunk_duration (int): Chunk length in seconds (default 600 s for 10 minutes).\n",
    "            chunk_stride (int): Time in seconds to step forward between chunks (default 300 s, for 50% overlap).\n",
    "            downsample_freq (int): Desired sampling frequency after downsampling (original data are at 64 Hz).\n",
    "            debug (bool): If True, print status messages.\n",
    "        \"\"\"\n",
    "        self.chunks = []  # List to store each generated chunk (with its corresponding data, labels, and SID)\n",
    "        # Compute downsample factor (original sampling rate is 64 Hz)\n",
    "        self.downsample = int(64 // downsample_freq)\n",
    "        # Effective sampling rate after downsampling becomes downsample_freq Hz.\n",
    "        self.chunk_length = int(chunk_duration * downsample_freq)\n",
    "        self.stride = int(chunk_stride * downsample_freq)\n",
    "\n",
    "        for SID in subjects_list:\n",
    "            file_path = os.path.join(data_dir, f\"{SID}_whole_df.csv\")\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path, dtype={'Sleep_Stage': 'category'}, converters=converters, low_memory=True)\n",
    "                if debug:\n",
    "                    print(f\"Loaded data for subject {SID}\")\n",
    "                \n",
    "                # Downsample: every self.downsample-th row\n",
    "                if self.downsample != 1:\n",
    "                    df = df.iloc[::self.downsample].reset_index(drop=True)\n",
    "                    if debug:\n",
    "                        print(f\"After downsampling (factor {self.downsample}), rows: {len(df)}\")\n",
    "                \n",
    "                # Remove rows with \"Preparation\" phase if labeled 'P'\n",
    "                df = df[df['Sleep_Stage'] != 'P']\n",
    "\n",
    "                # Ensure numeric conversion for required columns\n",
    "                for col in ['ACC_X', 'ACC_Y', 'ACC_Z', 'BVP', 'TEMP', 'TIMESTAMP', 'HR', 'IBI']:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                \n",
    "                # Compute accelerometer magnitude from three axes\n",
    "                ACC = np.sqrt(df['ACC_X']**2 + df['ACC_Y']**2 + df['ACC_Z']**2)\n",
    "                \n",
    "                # Prepare the features: TIMESTAMP, BVP, TEMP and the computed ACC\n",
    "                df_X = df[['TIMESTAMP', 'BVP', 'TEMP', 'HR', 'IBI']].copy()\n",
    "                df_X['ACC'] = ACC\n",
    "                # Normalize the features (z-score normalization per subject)\n",
    "                #TEMP_norm = (df_X['TEMP'] - df_X['TEMP'].mean()) / df_X['TEMP'].std()\n",
    "                #df_X['TEMP'] = TEMP_norm\n",
    "                #BVP_norm = (df_X['BVP'] - df_X['BVP'].mean()) / df_X['BVP'].std()\n",
    "                #df_X['BVP'] = BVP_norm\n",
    "\n",
    "                \n",
    "                # Process sleep stage labels: trim whitespace and map to integer\n",
    "                df['Sleep_Stage'] = df['Sleep_Stage'].astype(str).str.strip()\n",
    "                df_Y = df['Sleep_Stage'].map(SLEEP_STAGE_MAPPING)\n",
    "                \n",
    "                # Convert features and labels to numpy arrays\n",
    "                data_arr = df_X.values.astype(np.float32)  # shape: [T, C]\n",
    "                labels_arr = df_Y.to_numpy()                # shape: [T]\n",
    "                T = data_arr.shape[0]\n",
    "\n",
    "                # If the record is too short (less than one chunk), pad it with NaNs (-1 for labels)\n",
    "                if T < self.chunk_length:\n",
    "                    pad_size = self.chunk_length - T\n",
    "                    padding_data = np.full((pad_size, data_arr.shape[1]), np.nan, dtype=np.float32)\n",
    "                    data_arr = np.concatenate([data_arr, padding_data], axis=0)\n",
    "                    padding_labels = np.full((pad_size,), -1)\n",
    "                    labels_arr = np.concatenate([labels_arr, padding_labels], axis=0)\n",
    "                    T = self.chunk_length  # update length\n",
    "\n",
    "                # Slide a window over the data with the defined stride to create overlapping chunks\n",
    "                for start in range(0, T - self.chunk_length + 1, self.stride):\n",
    "                    end = start + self.chunk_length\n",
    "                    chunk_data = data_arr[start:end, :]\n",
    "                    chunk_labels = labels_arr[start:end]\n",
    "                    self.chunks.append({\n",
    "                        'data': chunk_data,\n",
    "                        'labels': chunk_labels,\n",
    "                        'SID': SID\n",
    "                    })\n",
    "                if debug:\n",
    "                    num_chunks = (T - self.chunk_length) // self.stride + 1\n",
    "                    print(f\"Subject {SID}: {T} samples processed, generated {num_chunks} chunks\")\n",
    "            else:\n",
    "                print(f\"File {file_path} does not exist. Skipping subject {SID}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.chunks[idx]\n",
    "        data = torch.tensor(chunk['data'], dtype=torch.float32)\n",
    "        labels = torch.tensor(chunk['labels'], dtype=torch.long)\n",
    "        # Use forward_fill to replace any NaNs with previous values.\n",
    "        data = forward_fill(data)\n",
    "        labels = forward_fill(labels)\n",
    "        return data, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb17bf",
   "metadata": {},
   "source": [
    "### Construct train, val, and test datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d268017",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mtrain_dataset_windowed = SleepDataset(subjects_list=subjects_train,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m                                 data_dir=datadir_64Hz,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m                                 debug=False)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSleepDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubjects_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubjects_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatadir_64Hz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdownsample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# downsample to 8Hz\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m SleepDataset(subjects_list\u001b[38;5;241m=\u001b[39msubjects_val,\n\u001b[1;32m     29\u001b[0m                                  data_dir\u001b[38;5;241m=\u001b[39mdatadir_64Hz,\n\u001b[1;32m     30\u001b[0m                                  max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m     31\u001b[0m                                  downsample_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, \u001b[38;5;66;03m# downsample to 8Hz\u001b[39;00m\n\u001b[1;32m     32\u001b[0m                                  debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m SleepDataset(subjects_list\u001b[38;5;241m=\u001b[39msubjects_test,\n\u001b[1;32m     34\u001b[0m                                  data_dir\u001b[38;5;241m=\u001b[39mdatadir_64Hz,\n\u001b[1;32m     35\u001b[0m                                  max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m     36\u001b[0m                                  downsample_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, \u001b[38;5;66;03m# downsample to 8Hz\u001b[39;00m\n\u001b[1;32m     37\u001b[0m                                  debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[64], line 44\u001b[0m, in \u001b[0;36mSleepDataset.__init__\u001b[0;34m(self, subjects_list, data_dir, max_length, downsample_freq, debug)\u001b[0m\n\u001b[1;32m     42\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_whole_df.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file_path):\n\u001b[0;32m---> 44\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSleep_Stage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:921\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:1045\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2116\u001b[0m, in \u001b[0;36mpandas._libs.parsers._apply_converter\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[62], line 4\u001b[0m, in \u001b[0;36msafe_float\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msafe_float\u001b[39m(x):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(x)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "train_dataset_windowed = SleepDataset(subjects_list=subjects_train,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 window_size_ms= 20000, # 20 seconds of data\n",
    "                                 stride_ms=5000,        # 5 seconds overlap\n",
    "                                 downsample_freq=8, # downsample to 16Hz\n",
    "                                 debug=False)\n",
    "val_dataset_windowed = SleepDataset(subjects_list=subjects_val,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 window_size_ms= 20000, # 20 seconds of data\n",
    "                                 stride_ms=5000,        # 5 seconds overlap\n",
    "                                 downsample_freq=8, # downsample to 16Hz\n",
    "                                 debug=False)\n",
    "test_dataset_windowed = SleepDataset(subjects_list=subjects_test,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 window_size_ms= 20000, # 20 seconds of data\n",
    "                                 stride_ms=5000,        # 5 second stride\n",
    "                                 downsample_freq=8, # downsample to 16Hz\n",
    "                                 debug=False)\n",
    "'''\n",
    "\n",
    "train_dataset = SleepDataset(subjects_list=subjects_train,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 max_length=max_length,\n",
    "                                 downsample_freq=8, # downsample to 8Hz\n",
    "                                 debug=False)\n",
    "\n",
    "val_dataset = SleepDataset(subjects_list=subjects_val,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 max_length=max_length,\n",
    "                                 downsample_freq=8, # downsample to 8Hz\n",
    "                                 debug=False)\n",
    "test_dataset = SleepDataset(subjects_list=subjects_test,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 max_length=max_length,\n",
    "                                 downsample_freq=8, # downsample to 8Hz\n",
    "                                 debug=False)\n",
    "\n",
    "print(f\"Total samples in train dataset: {len(train_dataset)}\")\n",
    "print(f\"Total samples in val dataset: {len(val_dataset)}\")\n",
    "print(f\"Total samples in test dataset: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ab676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in train chunk dataset: 6547\n",
      "Total samples in val chunk dataset: 821\n",
      "Total samples in test chunk dataset: 845\n"
     ]
    }
   ],
   "source": [
    "train_chunk_dataset = SleepChunkDataset(subjects_list=subjects_train,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 chunk_duration=600,  # 10 minutes\n",
    "                                 chunk_stride=300,    # 5 minutes\n",
    "                                 downsample_freq=0.2,   # downsample to 0.2 Hz\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in train chunk dataset: {len(train_chunk_dataset)}\")\n",
    "val_chunk_dataset = SleepChunkDataset(subjects_list=subjects_val,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 chunk_duration=600,  # 10 minutes\n",
    "                                 chunk_stride=300,    # 5 minutes\n",
    "                                 downsample_freq=0.2,   # downsample to 0.2 Hz\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in val chunk dataset: {len(val_chunk_dataset)}\")\n",
    "test_chunk_dataset = SleepChunkDataset(subjects_list=subjects_test,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 chunk_duration=600,  # 10 minutes\n",
    "                                 chunk_stride=300,    # 5 minutes\n",
    "                                 downsample_freq=0.2,   # downsample to 0.2 Hz\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in test chunk dataset: {len(test_chunk_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "92f119ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: Counter({np.int64(2): 405337, np.int64(0): 182608, np.int64(4): 87953, np.int64(1): 82910, np.int64(3): 26098})\n",
      "Class weights: [0.85966223 1.89339284 0.38728564 6.01506629 1.78483053]\n"
     ]
    }
   ],
   "source": [
    "# get class weights for weighted loss\n",
    "all_labels = []\n",
    "for batch in DataLoader(train_chunk_dataset, batch_size=1):\n",
    "    labels = batch[1].numpy()\n",
    "    all_labels.extend(labels.flatten())\n",
    "all_labels = np.array(all_labels)\n",
    "valid_labels = all_labels[all_labels != -1]\n",
    "classes = np.unique(valid_labels)\n",
    "class_counts = Counter(valid_labels)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=valid_labels\n",
    ")\n",
    "print(f\"Class counts: {class_counts}\")\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036b8a4",
   "metadata": {},
   "source": [
    "## CNN downsampling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfebfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence length x input channels -> CNN -> shortened sequence length x num hidden channels -> LSTM -> shortened sequence length x num sleep stages\n",
    "\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self, in_channels=4, cnn_output_channels=128):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=5, stride=2, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv1d(64, 64, kernel_size=5, stride=2, padding=1)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv4 = nn.Conv1d(64, cnn_output_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn = nn.BatchNorm1d(cnn_output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Global average pooling to collapse \n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert not torch.isnan(x).any(), \"NaN detected in CNN input\"\n",
    "        # Expect x of shape (batch, epoch_samples, channels)\n",
    "        x = x.permute(0, 2, 1)  # Rearrange to (batch, channels, epoch_samples)\n",
    "        #print(f\"Input shape after permutation: {x.shape}\")\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        #print(f\"Shape after conv1 and pool1: {x.shape}\")\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        #print(f\"Shape after conv2 and pool2: {x.shape}\")\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        #print(f\"Shape after conv3 and pool3: {x.shape}\")\n",
    "        x = self.relu(self.conv4(x))\n",
    "        #print(f\"Shape after conv4: {x.shape}\")\n",
    "        x = self.bn(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SleepStageLSTM(nn.Module):\n",
    "    def __init__(self, cnn_output_channels=128, hidden_size=64, num_layers=2, num_sleep_stages=5):\n",
    "        super(SleepStageLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=cnn_output_channels,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_size, num_sleep_stages)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert not torch.isnan(x).any(), \"NaN detected in LSTM input\"\n",
    "        # x is of shape (batch, cnn_output_channels, samples)\n",
    "        # LSTM expects input shape of (samples, batch, cnn_output_channels)\n",
    "        x = x.permute(2, 0, 1) # (batch, cnn_output_channels, samples) -> (samples, batch, cnn_output_channels)\n",
    "        #print(f\"Input shape after permutation: {x.shape}\")\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        #print(f\"Shape after LSTM: {lstm_out.shape}\")\n",
    "        # Option 1: produce a prediction for every epoch (each time step)\n",
    "        out = self.fc(lstm_out)   # shape: (batch, num_epochs, num_sleep_stages)\n",
    "        #print(f\"Shape after fully connected layer: {out.shape}\")\n",
    "        \n",
    "        # Option 2: if you want a prediction only for the current epoch,\n",
    "        # you may take the output of the last time step:\n",
    "        #predictions = self.fc(lstm_out[:, -1, :])  # shape: (batch, num_sleep_stages)\n",
    "        return out\n",
    "\n",
    "class OnlineSleepStagingModel(pl.LightningModule):\n",
    "    def __init__(self, in_channels, cnn_output_channels, lstm_hidden_size, num_layers=2, num_sleep_stages=5, learning_rate=0.001, class_weights=None):\n",
    "        super(OnlineSleepStagingModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.feature_extractor = FeatureExtractorCNN(in_channels=in_channels, cnn_output_channels=cnn_output_channels)\n",
    "        self.lstm_model = SleepStageLSTM(cnn_output_channels=cnn_output_channels, hidden_size=lstm_hidden_size, num_layers=num_layers, num_sleep_stages=num_sleep_stages)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Ignore the \"Missing\" label (-1)\n",
    "        if class_weights is not None:\n",
    "            self.criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32), ignore_index=-1)\n",
    "        self.num_sleep_stages = num_sleep_stages\n",
    "        self.cnn_output_channels = cnn_output_channels\n",
    "\n",
    "        self.val_class_counts = Counter()\n",
    "        self.pred_class_counts = Counter()\n",
    "        self.kappa = MulticlassCohenKappa(num_classes=self.num_sleep_stages)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.lstm_model(x) # (samples, batch_size, num_sleep_stages)\n",
    "        assert x.shape[2] == self.num_sleep_stages, \"LSTM output shape != num_sleep_stages\"\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch  # y shape: [batch_size, T]\n",
    "        y_hat = self(x)  # y_hat shape: [output_length, batch_size, num_sleep_stages]\n",
    "        # Check for NaNs in the network output\n",
    "        assert not torch.isnan(y).any(), \"NaN detected in labels\"\n",
    "        assert not torch.isnan(y_hat).any(), \"NaN detected in network output\"\n",
    "    \n",
    "        # Permute to batch first\n",
    "        y_hat = y_hat.permute(1, 0, 2)\n",
    "        output_length = y_hat.shape[1]\n",
    "        y_expanded = y.unsqueeze(1)\n",
    "        # Downsample y to match y_hat\n",
    "        y_resampled = torch.nn.functional.interpolate(\n",
    "            y_expanded.float(),\n",
    "            size = (output_length,),\n",
    "            mode = 'nearest'\n",
    "        )\n",
    "        y_resampled = y_resampled.squeeze(1).long()\n",
    "\n",
    "        # Flatten y_hat and y_resampled for loss calculation\n",
    "        batch_size, output_length, num_sleep_stages = y_hat.shape\n",
    "        y_hat_flat = y_hat.reshape(batch_size * output_length, num_sleep_stages)\n",
    "        y_resampled_flat = y_resampled.reshape(batch_size * output_length)\n",
    "        # Calculate loss\n",
    "        loss = self.criterion(y_hat_flat, y_resampled_flat)\n",
    "        # Check loss for finiteness\n",
    "        assert torch.isfinite(loss), \"Loss is not finite\"\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch  # y shape: [batch_size, T]\n",
    "        y_hat = self(x)  # y_hat shape: [output_length, batch_size, num_sleep_stages]\n",
    "        # Permute to batch first\n",
    "        y_hat = y_hat.permute(1, 0, 2)\n",
    "        output_length = y_hat.shape[1]\n",
    "        y_expanded = y.unsqueeze(1)\n",
    "        # Downsample y to match y_hat\n",
    "        y_resampled = torch.nn.functional.interpolate(\n",
    "            y_expanded.float(),\n",
    "            size = (output_length,),\n",
    "            mode = 'nearest'\n",
    "        )\n",
    "        y_resampled = y_resampled.squeeze(1).long()\n",
    "\n",
    "        # Flatten y_hat and y_resampled for loss calculation\n",
    "        batch_size, output_length, num_sleep_stages = y_hat.shape\n",
    "        y_hat_flat = y_hat.reshape(batch_size * output_length, num_sleep_stages)\n",
    "        y_resampled_flat = y_resampled.reshape(batch_size * output_length)\n",
    "        predictions = torch.argmax(y_hat_flat, dim=1)\n",
    "        # Calculate Cohen's Kappa\n",
    "        assert predictions.shape[0] == y_resampled_flat.shape[0], f\"Predictions and labels have different shapes (dim 0) {predictions.shape[0]} vs {y_resampled_flat.shape[0]}\"\n",
    "        cohen_kappa_score = self.kappa(predictions, y_resampled_flat)\n",
    "        self.log(\"val_cohen_kappa\", cohen_kappa_score, prog_bar=True)\n",
    "        # Calculate loss\n",
    "        loss = self.criterion(y_hat_flat, y_resampled_flat)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "\n",
    "        # Update class counts\n",
    "        # y_resampled_flat: [batch_size * output_length]\n",
    "        # predictions: [batch_size * output_length]\n",
    "\n",
    "        mask = y_resampled_flat != -1\n",
    "        y_valid = y_resampled_flat[mask]\n",
    "        preds_valid = predictions[mask]\n",
    "\n",
    "        if y_valid.numel() > 0:\n",
    "            self.kappa.update(preds_valid, y_valid)\n",
    "            self.val_class_counts.update(y_valid.cpu().tolist())\n",
    "            self.pred_class_counts.update(preds_valid.cpu().tolist())\n",
    "\n",
    "        return loss\n",
    "  \n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Log Cohen's Kappa\n",
    "        if self.kappa.confmat.sum() > 0:\n",
    "            cohen_kappa_score = self.kappa.compute()\n",
    "            self.log(\"val_cohen_kappa\", cohen_kappa_score, prog_bar=True)\n",
    "            self.kappa.reset()\n",
    "        else:\n",
    "            self.log(\"val_cohen_kappa\", 0.0, prog_bar=True)\n",
    "\n",
    "        # W&B class distribution bar plots\n",
    "        class_labels = list(range(self.num_sleep_stages))\n",
    "        val_counts = [self.val_class_counts.get(c, 0) for c in class_labels]\n",
    "        pred_counts = [self.pred_class_counts.get(c, 0) for c in class_labels]\n",
    "\n",
    "        self.log({\n",
    "            \"True Label Distribution\": wandb.plot.bar(\n",
    "                wandb.Table(data=[[str(c), v] for c, v in zip(class_labels, val_counts)],\n",
    "                            columns=[\"Class\", \"Count\"]),\n",
    "                \"Class\", \"Count\", title=\"True Label Distribution\"\n",
    "            ),\n",
    "            \"Predicted Label Distribution\": wandb.plot.bar(\n",
    "                wandb.Table(data=[[str(c), p] for c, p in zip(class_labels, pred_counts)],\n",
    "                            columns=[\"Class\", \"Count\"]),\n",
    "                \"Class\", \"Count\", title=\"Predicted Label Distribution\"\n",
    "            )\n",
    "        }, step=self.current_epoch)\n",
    "\n",
    "        self.val_class_counts.clear()\n",
    "        self.pred_class_counts.clear()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "class CNNClassifier(pl.LightningModule):\n",
    "    def __init__(self, in_channels, cnn_output_channels, lstm_hidden_size, num_layers=2, num_sleep_stages=5, learning_rate=0.001):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.feature_extractor = FeatureExtractorCNN(in_channels=in_channels, cnn_output_channels=cnn_output_channels)\n",
    "        self.classifier = nn.Linear(cnn_output_channels, num_sleep_stages)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Ignore the \"Missing\" label (-1)\n",
    "        self.num_sleep_stages = num_sleep_stages\n",
    "        self.cnn_output_channels = cnn_output_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        assert x.shape[2] == self.num_sleep_stages, \"CNN output shape != num_sleep_stages\"\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df131758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([120, 6]) (epoch_samples, channels)\n",
      "Output shape: torch.Size([1, 128, 1]) (batch_size, cnn_output_channels, epoch_samples)\n",
      "Combined model output shape: torch.Size([1, 1, 5]) (samples, batch_size, num_sleep_stages)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# demo of model elements\n",
    "temp_input = train_chunk_dataset[0][0]\n",
    "print(f\"Input shape: {temp_input.shape} (epoch_samples, channels)\")\n",
    "CNN_model = FeatureExtractorCNN(in_channels=6, cnn_output_channels=128)\n",
    "CNN_model.eval()\n",
    "cnn_output = CNN_model(temp_input.unsqueeze(0))\n",
    "print(f\"Output shape: {cnn_output.shape} (batch_size, cnn_output_channels, epoch_samples)\")\n",
    "LSTM_model = SleepStageLSTM(cnn_output_channels=128, hidden_size=64, num_layers=2, num_sleep_stages=5)\n",
    "LSTM_output = LSTM_model(cnn_output)\n",
    "\n",
    "# demo of combined model\n",
    "combined_model = OnlineSleepStagingModel(in_channels=6, cnn_output_channels=128, lstm_hidden_size=64, num_layers=2, num_sleep_stages=5)\n",
    "combined_model.eval()\n",
    "combined_output = combined_model(temp_input.unsqueeze(0))\n",
    "print(f\"Combined model output shape: {combined_output.shape} (samples, batch_size, num_sleep_stages)\")\n",
    "print(combined_output.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a171e2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁███</td></tr><tr><td>train_loss_epoch</td><td>▁</td></tr><tr><td>train_loss_step</td><td>▇▆▇█▅▇▆▆▆▆▅▇▄▁▇██▆▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>val_cohen_kappa</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_loss_epoch</td><td>1.58238</td></tr><tr><td>train_loss_step</td><td>1.57287</td></tr><tr><td>trainer/global_step</td><td>949</td></tr><tr><td>val_cohen_kappa</td><td>0.02932</td></tr><tr><td>val_loss</td><td>1.55869</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">online_sleep_staging_model</strong> at: <a href='https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification/runs/fnqxkv23' target=\"_blank\">https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification/runs/fnqxkv23</a><br> View project at: <a href='https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification' target=\"_blank\">https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification</a><br>Synced 5 W&B file(s), 4 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250415_123919-fnqxkv23/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(temp_input.shape[0] / combined_output.shape[0])\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c3cdddd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4m ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250415_123919-fnqxkv23</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification/runs/fnqxkv23' target=\"_blank\">online_sleep_staging_model</a></strong> to <a href='https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification' target=\"_blank\">https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification/runs/fnqxkv23' target=\"_blank\">https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification/runs/fnqxkv23</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /gpfs/data/oermannlab/users/slj9342/dl4med_final_project/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                 | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | feature_extractor | FeatureExtractorCNN  | 35.0 K | train\n",
      "1 | lstm_model        | SleepStageLSTM       | 54.6 K | train\n",
      "2 | criterion         | CrossEntropyLoss     | 0      | train\n",
      "3 | kappa             | MulticlassCohenKappa | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "89.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "89.6 K    Total params\n",
      "0.358     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0029687881469726562,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1654d467755a46cab20fb417a001c6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0029845237731933594,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c848fd8df7346bcb85aa9c87800297b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.003286600112915039,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7daf5a9e1b08410e8f0dd406666eb776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.559\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    595\u001b[0m     ckpt_path,\n\u001b[1;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m )\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1056\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:282\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    281\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py:134\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py:61\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators[i])\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[65], line 132\u001b[0m, in \u001b[0;36mSleepChunkDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Use forward_fill to replace any NaNs with previous values.\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mforward_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m labels \u001b[38;5;241m=\u001b[39m forward_fill(labels)\n",
      "Cell \u001b[0;32mIn[65], line 27\u001b[0m, in \u001b[0;36mforward_fill\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, T):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     28\u001b[0m         x[t, c] \u001b[38;5;241m=\u001b[39m x[t \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, c]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_chunk_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_chunk_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Load the best model\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:65\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     64\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     68\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the combined model\n",
    "wandb_logger = WandbLogger(name=\"online_sleep_staging_model\", project=\"sleep_stage_classification\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='best-checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    devices=1,\n",
    "    accelerator='gpu',\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback]\n",
    ")\n",
    "model = OnlineSleepStagingModel(in_channels=6, cnn_output_channels=16, lstm_hidden_size=64, num_layers=2, num_sleep_stages=5, learning_rate=1e-4, class_weights=class_weights)\n",
    "train_loader = DataLoader(train_chunk_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_chunk_dataset, batch_size=8, shuffle=False)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "wandb.finish()\n",
    "# Load the best model\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "best_model = OnlineSleepStagingModel.load_from_checkpoint(best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b731ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 13:11:50,112] Trial 4 finished with value: 1.5019652843475342 and parameters: {'cnn_output_channels': 32, 'lstm_hidden_size': 128, 'learning_rate': 0.0012144111933744526}. Best is trial 3 with value: 1.4949907064437866.\n",
      "/tmp/ipykernel_2671111/1253607370.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
      "/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4m ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250415_131150-vfwcnlb0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lakej98-nyu-langone-health/optuna_sleep_stage_classification/runs/vfwcnlb0' target=\"_blank\">CNN64_hs128_lr1.305077065195548e-05</a></strong> to <a href='https://wandb.ai/lakej98-nyu-langone-health/optuna_sleep_stage_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lakej98-nyu-langone-health/optuna_sleep_stage_classification' target=\"_blank\">https://wandb.ai/lakej98-nyu-langone-health/optuna_sleep_stage_classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lakej98-nyu-langone-health/optuna_sleep_stage_classification/runs/vfwcnlb0' target=\"_blank\">https://wandb.ai/lakej98-nyu-langone-health/optuna_sleep_stage_classification/runs/vfwcnlb0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                 | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | feature_extractor | FeatureExtractorCNN  | 44.3 K | train\n",
      "1 | lstm_model        | SleepStageLSTM       | 232 K  | train\n",
      "2 | criterion         | CrossEntropyLoss     | 0      | train\n",
      "3 | kappa             | MulticlassCohenKappa | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "276 K     Trainable params\n",
      "0         Non-trainable params\n",
      "276 K     Total params\n",
      "1.106     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.002897977828979492,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c953e924b04aac93652d6a0846e8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0029265880584716797,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176fcfa47d4e4834b24d5371ebbbac40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0032668113708496094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e4d2e528364f7dabb77971b9405a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 8. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    cnn_output_channels = trial.suggest_categorical(\"cnn_output_channels\", [8, 16, 32, 64])\n",
    "    lstm_hidden_size = trial.suggest_categorical(\"lstm_hidden_size\", [32, 64, 128])\n",
    "    num_layers = 2\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "\n",
    "    wandb_logger = WandbLogger(name=f\"CNN{cnn_output_channels}_hs{lstm_hidden_size}_lr{learning_rate}\", project=\"optuna_sleep_stage_classification\")\n",
    "    # DataLoaders (resample based on batch size)\n",
    "    train_loader = DataLoader(train_chunk_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_chunk_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = OnlineSleepStagingModel(\n",
    "        in_channels=6,\n",
    "        cnn_output_channels=cnn_output_channels,\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        num_sleep_stages=5,\n",
    "        learning_rate=learning_rate,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "\n",
    "    # Trainer with pruning callback\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,\n",
    "        devices=1,\n",
    "        accelerator=\"gpu\",\n",
    "        logger=wandb_logger,\n",
    "        enable_checkpointing=False,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    wandb.finish()\n",
    "    clear_output()\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "best_trial = study.best_trial\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d3983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project overview\n",
    "# construct dataset class\n",
    "# - train CNN -> LSTM sleep staging model with causal conv and unidirectional LSTM\n",
    "# - train CNN -> LSTM sleep staging model with non-causal conv and bidirectional LSTM\n",
    "# - compare performance at 64Hz\n",
    "# - compare performance and computational cost at lower sampling rates\n",
    "\n",
    "# unstructured notes\n",
    "# - likely best to combine ACC columns into a single variable. Can't imagine they offer much additional information\n",
    "# - CNN model should use 2D CNNs to extract features across both channels and short-term time windows\n",
    "# - will need to pad data on both sides to ensure input and output sequences for the CNN are the same length\n",
    "\n",
    "\n",
    "# CNN model - input: seq_len x num_channels -> output: seq_len x hidden_size\n",
    "\n",
    "\n",
    "# LSTM model - input: seq_len x hidden_size -> output: seq_len x num_classes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data into train val and test by subject\n",
    "participant_info_df = pd.read_csv('/gpfs/data/oermannlab/users/slj9342/dl4med_25/data/physionet.org/files/dreamt/2.0.0/participant_info.csv')\n",
    "subject_ids = participant_info_df['subject_id'].unique()\n",
    "train_subjects = subject_ids[:int(len(subject_ids)*0.8)]\n",
    "val_subjects = subject_ids[int(len(subject_ids)*0.8):int(len(subject_ids)*0.9)]\n",
    "test_subjects = subject_ids[int(len(subject_ids)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc8b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demo_df.columns)\n",
    "print(demo_df['Sleep_Stage'].unique())\n",
    "print(demo_df['TIMESTAMP'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680457e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "sleep_stage_labels = le.fit_transform(demo_df['Sleep_Stage'])\n",
    "plt.plot(sleep_stage_labels)\n",
    "plt.title('Sleep Stage Labels')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sleep Stage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c5822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "cols_X = ['TIMESTAMP', 'BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP']\n",
    "cols_Y = ['Sleep_Stage']\n",
    "\n",
    "class DreamtDataset(Dataset):\n",
    "    def __init__(self, data_dir, subject_ids, cols_X, cols_Y):\n",
    "        self.data_dir = data_dir\n",
    "        self.subject_ids = subject_ids\n",
    "        self.cols_X = cols_X\n",
    "        self.cols_Y = cols_Y\n",
    "        self.data = []\n",
    "        for subject_id in subject_ids:\n",
    "            df = pd.read_csv(data_dir + f'S{subject_id:03d}_whole_df.csv')\n",
    "            df = df[cols_X + cols_Y]\n",
    "            df['subject_id'] = subject_id\n",
    "            self.data.append(df)\n",
    "        self.data = pd.concat(self.data, ignore_index=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        X = torch.tensor(row[self.cols_X].values, dtype=torch.float32)\n",
    "        y = torch.tensor(row[self.cols_Y].values, dtype=torch.long)\n",
    "        return X, y\n",
    "\n",
    "# CNN model with 4 2D conv layers, relu activation, dropout of 0.2, and 2 fully connected layers, first one has size 4096, second with 1500, both fc layers have dropout of 0.5\n",
    "# last fc layer has output size of 5, the number of channels to feed into the LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input shape: (batch_size, 1, num_channels, time_window)\n",
    "# Example: 6 EEG channels, 128-sample time windows → (batch, 1, 6, 128)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sliding_windows(x, window_size, stride=1):\n",
    "    # x: (batch, channels, time)\n",
    "    batch, channels, total_len = x.shape\n",
    "\n",
    "    # Pad on both sides: same number of windows as original time length\n",
    "    pad = (window_size - 1) // 2\n",
    "    x_padded = F.pad(x, pad=(pad, pad), mode='replicate')\n",
    "\n",
    "    # Now generate sliding windows\n",
    "    windows = x_padded.unfold(dimension=2, size=window_size, step=stride)  # (batch, channels, time, window_size)\n",
    "    windows = windows.permute(0, 2, 1, 3)  # (batch, time, channels, window_size)\n",
    "    return windows  # shape: (batch, time, channels, window_size)\n",
    "\n",
    "\n",
    "\n",
    "class DreamtCNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_chans_out):\n",
    "        super(DreamtCNN, self).__init__()\n",
    "        c, t = input_shape  # c = number of channels (e.g. 6), t = time_window size (e.g. 128)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 5), stride=1, padding=(1, 2))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 5), stride=1, padding=(1, 2))\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 5), stride=1, padding=(1, 2))\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=(3, 5), stride=1, padding=(1, 2))\n",
    "\n",
    "        self.conv_dropout = nn.Dropout(0.2)\n",
    "        self.ff_dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Output dims after conv layers will be (batch, 256, c, t)\n",
    "        self.fc1 = nn.Linear(256 * c * t, 4096)\n",
    "        self.fc2 = nn.Linear(4096, num_chans_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, 1, channels, time_window)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv_dropout(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv_dropout(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.conv_dropout(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.conv_dropout(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.ff_dropout(x)\n",
    "        x = self.fc2(x)  # output shape: (batch_size, num_chans_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define test parameters\n",
    "batch_size = 4\n",
    "num_channels = 6\n",
    "time_window = 128\n",
    "seq_len = 1000  # e.g., length of the sequence to be fed into the LSTM\n",
    "num_chans_out = 5  # e.g., number of channels to feed into the LSTM\n",
    "\n",
    "# create dummy imput \n",
    "x = torch.randn(batch_size, 1, num_channels, time_window)  # (batch_size, 1, num_channels, time_window)\n",
    "\n",
    "# window input\n",
    "x = x.squeeze(1)  # (batch_size, num_channels, time_window)\n",
    "x = sliding_windows(x, window_size=time_window, stride=1)\n",
    "\n",
    "print(f\"Shape after sliding windows: {x.shape}\")\n",
    "\n",
    "# Instantiate the model\n",
    "model = DreamtCNN(input_shape=(num_channels, time_window), num_chans_out=num_chans_out)\n",
    "\n",
    "# Forward pass\n",
    "# x shape: (batch, time, channels, window)\n",
    "x = x.unsqueeze(2)  # -> (batch, time, 1, channels, window)\n",
    "\n",
    "# Reshape for CNN: merge batch and time into one dim\n",
    "b, t, one, c, w = x.shape\n",
    "x = x.view(b * t, 1, c, w)  # (b * t, 1, channels, window)\n",
    "\n",
    "# Feed into CNN\n",
    "out = model(x)  # shape: (b * t, output_dim)\n",
    "\n",
    "# Restore sequence shape\n",
    "out = out.view(b, t, -1)  # (batch, time, output_dim)\n",
    "\n",
    "\n",
    "# Print shape\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl4med_25)",
   "language": "python",
   "name": "dl4med_25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
