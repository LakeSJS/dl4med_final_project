{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b171763c",
   "metadata": {},
   "source": [
    "# Investigation of the impact of causality constraints on a CNN-LSTM sleep staging model\n",
    "\n",
    "### Project goals:\n",
    "##### Classify sleep stages using multi-modal sensor data (BVP, accelerometer, timestamps, temperature).\n",
    "##### Compare model performance and computation between non-causal versus causal architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402878f",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4da426bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchmetrics.classification import MulticlassCohenKappa\n",
    "from IPython.display import clear_output\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "53dc944f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2671111/190206057.py:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  demo_df = pd.read_csv(demo_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TIMESTAMP', 'BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP', 'EDA', 'HR',\n",
      "       'IBI', 'Sleep_Stage', 'Obstructive_Apnea', 'Central_Apnea', 'Hypopnea',\n",
      "       'Multiple_Events'],\n",
      "      dtype='object')\n",
      "TIMESTAMP            float64\n",
      "BVP                  float64\n",
      "ACC_X                float64\n",
      "ACC_Y                 object\n",
      "ACC_Z                float64\n",
      "TEMP                 float64\n",
      "EDA                  float64\n",
      "HR                   float64\n",
      "IBI                  float64\n",
      "Sleep_Stage           object\n",
      "Obstructive_Apnea    float64\n",
      "Central_Apnea        float64\n",
      "Hypopnea             float64\n",
      "Multiple_Events      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# demo csv, get columns\n",
    "demo_csv_path = '/gpfs/data/oermannlab/users/slj9342/dl4med_25/data/physionet.org/files/dreamt/2.0.0/data_64Hz/S016_whole_df.csv'\n",
    "demo_df = pd.read_csv(demo_csv_path)\n",
    "print(demo_df.columns)\n",
    "col_dtypes = demo_df.dtypes\n",
    "print(col_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d51dae",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e38a702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TIMESTAMP', 'BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP', 'EDA', 'HR', 'IBI', 'Sleep_Stage', 'Obstructive_Apnea', 'Central_Apnea', 'Hypopnea', 'Multiple_Events']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2671111/2388715554.py:24: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(datadir_64Hz, file))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTAMP            float64\n",
      "BVP                  float64\n",
      "ACC_X                float64\n",
      "ACC_Y                 object\n",
      "ACC_Z                float64\n",
      "TEMP                 float64\n",
      "EDA                  float64\n",
      "HR                   float64\n",
      "IBI                  float64\n",
      "Sleep_Stage           object\n",
      "Obstructive_Apnea    float64\n",
      "Central_Apnea        float64\n",
      "Hypopnea             float64\n",
      "Multiple_Events      float64\n",
      "dtype: object\n",
      "❌ Column ACC_Y failed: Unable to parse string \"N1\" at position 2054657\n",
      "❌ Column Sleep_Stage failed: Unable to parse string \"P\" at position 0\n"
     ]
    }
   ],
   "source": [
    "datadir_64Hz = '/gpfs/data/oermannlab/users/slj9342/dl4med_25/data/physionet.org/files/dreamt/2.0.0/data_64Hz/' # working with 64Hz data\n",
    "\n",
    "dtype_dict = {\n",
    "    'TIMESTAMP': np.float32,\n",
    "    'BVP': np.float32,\n",
    "    'ACC_X': np.float32,\n",
    "    'ACC_Y': np.float32,\n",
    "    'ACC_Z': np.float32,\n",
    "    'TEMP': np.float32,\n",
    "    'EDA': np.float32,\n",
    "    'HR': np.float32,\n",
    "    'IBI': np.float32,  # <-- missing but important\n",
    "    'Sleep_Stage': 'category',\n",
    "    'Obstructive_Apnea': 'Int64',  # Nullable integer\n",
    "    'Central_Apnea': 'Int64',\n",
    "    'Hypopnea': 'Int64',\n",
    "    'Multiple_Events': 'Int64'\n",
    "}\n",
    "\n",
    "file = 'S016_whole_df.csv'\n",
    "df_head = pd.read_csv(os.path.join(datadir_64Hz, file), nrows=5)\n",
    "print(df_head.columns.tolist())\n",
    "\n",
    "df = pd.read_csv(os.path.join(datadir_64Hz, file))\n",
    "print(df.dtypes)\n",
    "\n",
    "df = pd.read_csv(os.path.join(datadir_64Hz, file), low_memory=False)\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        pd.to_numeric(df[col], errors='raise')\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Column {col} failed: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "14a23638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max sequence length\n",
    "def safe_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "numeric_columns = [\n",
    "    'TIMESTAMP', 'BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP',\n",
    "    'EDA', 'HR', 'IBI'\n",
    "]\n",
    "converters = {col: safe_float for col in numeric_columns}\n",
    "'''\n",
    "max_length = 0\n",
    "for file in os.listdir(datadir_64Hz):\n",
    "    if file.endswith('_whole_df.csv'):\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(datadir_64Hz, file),\n",
    "            dtype={'Sleep_Stage': 'category'},\n",
    "            converters=converters,\n",
    "            low_memory=True\n",
    "        )\n",
    "        max_length = max(max_length, len(df))\n",
    "print(f\"Max sequence length: {max_length}\")\n",
    "'''\n",
    "max_length = 2493810"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fcf3ce",
   "metadata": {},
   "source": [
    "### Split subjects into train, val, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "82a0a89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of subjects in train: 80\n",
      "number of subjects in val: 10\n",
      "number of subjects in test: 10\n"
     ]
    }
   ],
   "source": [
    "participant_info_df = pd.read_csv('/gpfs/data/oermannlab/users/slj9342/dl4med_25/data/physionet.org/files/dreamt/2.0.0/participant_info.csv')\n",
    "subjects_all = participant_info_df['SID']\n",
    "\n",
    "subjects_all_shuffled = participant_info_df['SID'].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "subjects_train = subjects_all_shuffled[:int(len(subjects_all_shuffled)*0.8)]\n",
    "subjects_val = subjects_all_shuffled[int(len(subjects_all_shuffled)*0.8):int(len(subjects_all_shuffled)*0.9)]\n",
    "subjects_test = subjects_all_shuffled[int(len(subjects_all_shuffled)*0.9):]\n",
    "print(f\"number of subjects in train: {len(subjects_train)}\")\n",
    "print(f\"number of subjects in val: {len(subjects_val)}\")\n",
    "print(f\"number of subjects in test: {len(subjects_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059ce4b",
   "metadata": {},
   "source": [
    "### Non-windowed dataset class\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a3ba2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLEEP_STAGE_MAPPING = {\n",
    "    \"W\": 0,    # Wake\n",
    "    \"N1\": 1,   # non-REM stage 1\n",
    "    \"N2\": 2,   # non-REM stage 2\n",
    "    \"N3\": 3,   # non-REM stage 3\n",
    "    \"R\": 4,    # REM\n",
    "    \"Missing\": -1  # Missing label\n",
    "}\n",
    "\n",
    "def forward_fill(x):\n",
    "    \"\"\"\n",
    "    Performs forward fill on a tensor. If x is 1D (shape [T]),\n",
    "    it's temporarily unsqueezed to [T, 1] and then processed.\n",
    "    Assumes the first value is valid, or fills it with zero if needed.\n",
    "    \"\"\"\n",
    "    single_channel = False\n",
    "    if x.dim() == 1:\n",
    "        x = x.unsqueeze(1)\n",
    "        single_channel = True\n",
    "    \n",
    "    T, C = x.shape\n",
    "    for c in range(C):\n",
    "        # Optionally, handle the first element if it's NaN\n",
    "        if torch.isnan(x[0, c]):\n",
    "            x[0, c] = 0.0  # or choose another default value\n",
    "        for t in range(1, T):\n",
    "            if torch.isnan(x[t, c]):\n",
    "                x[t, c] = x[t - 1, c]\n",
    "    \n",
    "    if single_channel:\n",
    "        x = x.squeeze(1)\n",
    "    return x\n",
    "\n",
    "class SleepDataset(Dataset):\n",
    "    def __init__(self, subjects_list, data_dir, max_length, downsample_freq=64, debug=False):\n",
    "        self.subjects = [{} for _ in range(len(subjects_list))]\n",
    "        self.downsample = int(64 // downsample_freq)  # Downsample factor\n",
    "        self.max_length = int(max_length // self.downsample)\n",
    "\n",
    "        for subjectNo, SID in enumerate(subjects_list):\n",
    "            # Load the data for each subject\n",
    "            file_path = os.path.join(data_dir, f\"{SID}_whole_df.csv\")\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(\n",
    "                    file_path,\n",
    "                    dtype={'Sleep_Stage': 'category'},\n",
    "                    converters=converters,\n",
    "                    low_memory=True\n",
    "                )\n",
    "                if debug:\n",
    "                    print(f\"loaded data for {SID}:\")\n",
    "\n",
    "                # Downsample the data if needed\n",
    "                if self.downsample != 1:\n",
    "                    df = df.iloc[::self.downsample].reset_index(drop=True)\n",
    "                    if debug:\n",
    "                        print(f\"After downsampling by factor {self.downsample}, rows: {len(df)}\")\n",
    "                \n",
    "                df = df[df['Sleep_Stage'] != 'P'] # remove data before PSG start\n",
    "                for col in ['ACC_X', 'ACC_Y', 'ACC_Z','BVP', 'TEMP', 'TIMESTAMP']:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                ACC = np.sqrt(df['ACC_X']**2 + df['ACC_Y']**2 + df['ACC_Z']**2) # assuming its unlikely each acc channel really carries important information\n",
    "                df_X = df[['TIMESTAMP', 'BVP', 'TEMP']].copy()\n",
    "                df_X['ACC'] = ACC\n",
    "                # Normalize the features (z-score normalization per subject)\n",
    "                TEMP_norm = (df_X['TEMP'] - df_X['TEMP'].mean()) / df_X['TEMP'].std()\n",
    "                df_X['TEMP'] = TEMP_norm\n",
    "                BVP_norm = (df_X['BVP'] - df_X['BVP'].mean()) / df_X['BVP'].std()\n",
    "                df_X['BVP'] = BVP_norm\n",
    "                df['Sleep_Stage'] = df['Sleep_Stage'].astype(str).str.strip()\n",
    "                df_Y = df['Sleep_Stage'].map(SLEEP_STAGE_MAPPING)\n",
    "                \n",
    "                # Pad/truncate the data to the downsampled max_length\n",
    "                if len(df_X) > self.max_length:\n",
    "                    if debug:\n",
    "                        print(f\"Truncating data for {SID} from {len(df_X)} to {self.max_length} samples.\")\n",
    "                    df_X = df_X.iloc[:self.max_length]\n",
    "                    df_Y = df_Y.iloc[:self.max_length]\n",
    "                else:\n",
    "                    padding_length = self.max_length - len(df_X)\n",
    "                    padding = pd.DataFrame(np.nan, index=np.arange(padding_length), columns=df_X.columns)\n",
    "                    df_X = pd.concat([df_X, padding], ignore_index=True)\n",
    "                    df_Y = pd.concat([df_Y, pd.Series([-1] * padding_length)], ignore_index=True)\n",
    "                self.subjects[subjectNo] = {\n",
    "                    'data': df_X.values.astype(np.float32),  # shape: [T, C]\n",
    "                    'labels': df_Y.to_numpy(),                 # shape: [T]\n",
    "                    'SID': SID\n",
    "                }\n",
    "                if debug:\n",
    "                    print(f\"Data shape for {SID}: {df_X.shape}, Labels shape: {df_Y.shape}\")\n",
    "            else:\n",
    "                warning(f\"File {file_path} does not exist. Skipping subject {SID}.\")\n",
    "    def __len__(self):\n",
    "        return len(self.subjects)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject = self.subjects[idx]\n",
    "        data = torch.tensor(subject['data'], dtype=torch.float32)\n",
    "        labels = torch.tensor(subject['labels'], dtype=torch.long)\n",
    "\n",
    "        data = forward_fill(data) # fill NaNs with previous values\n",
    "        labels = forward_fill(labels) # fill NaNs with previous values\n",
    "        return data, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7bda52",
   "metadata": {},
   "source": [
    "### Chunked Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c010b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep stage mapping as before\n",
    "SLEEP_STAGE_MAPPING = {\n",
    "    \"W\": 0,    # Wake\n",
    "    \"N1\": 1,   # non-REM stage 1\n",
    "    \"N2\": 2,   # non-REM stage 2\n",
    "    \"N3\": 3,   # non-REM stage 3\n",
    "    \"R\": 4,    # REM\n",
    "    \"Missing\": -1  # Missing label\n",
    "}\n",
    "\n",
    "def forward_fill(x):\n",
    "    \"\"\"\n",
    "    Performs forward fill on a tensor.\n",
    "    If x is 1D (shape [T]), it is temporarily unsqueezed to [T, 1].\n",
    "    Assumes the first value is valid, or fills it with zero if needed.\n",
    "    \"\"\"\n",
    "    single_channel = False\n",
    "    if x.dim() == 1:\n",
    "        x = x.unsqueeze(1)\n",
    "        single_channel = True\n",
    "\n",
    "    T, C = x.shape\n",
    "    for c in range(C):\n",
    "        if torch.isnan(x[0, c]):\n",
    "            x[0, c] = 0.0\n",
    "        for t in range(1, T):\n",
    "            if torch.isnan(x[t, c]):\n",
    "                x[t, c] = x[t - 1, c]\n",
    "    if single_channel:\n",
    "        x = x.squeeze(1)\n",
    "    return x\n",
    "\n",
    "numeric_columns = [\n",
    "    'TIMESTAMP', 'BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP',\n",
    "    'EDA', 'HR', 'IBI'\n",
    "]\n",
    "converters = {col: safe_float for col in numeric_columns}\n",
    "\n",
    "class SleepChunkDataset(Dataset):\n",
    "    def __init__(self, subjects_list, data_dir, chunk_duration=600, chunk_stride=300, downsample_freq=64, debug=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            subjects_list (list): List of subject IDs, e.g. [\"SID1\", \"SID2\", ...].\n",
    "            data_dir (str): Directory where files like \"SID_whole_df.csv\" are stored.\n",
    "            chunk_duration (int): Chunk length in seconds (default 600 s for 10 minutes).\n",
    "            chunk_stride (int): Time in seconds to step forward between chunks (default 300 s, for 50% overlap).\n",
    "            downsample_freq (int): Desired sampling frequency after downsampling (original data are at 64 Hz).\n",
    "            debug (bool): If True, print status messages.\n",
    "        \"\"\"\n",
    "        self.chunks = []  # List to store each generated chunk (with its corresponding data, labels, and SID)\n",
    "        # Compute downsample factor (original sampling rate is 64 Hz)\n",
    "        self.downsample = int(64 // downsample_freq)\n",
    "        # Effective sampling rate after downsampling becomes downsample_freq Hz.\n",
    "        self.chunk_length = int(chunk_duration * downsample_freq)\n",
    "        self.stride = int(chunk_stride * downsample_freq)\n",
    "\n",
    "        for SID in subjects_list:\n",
    "            file_path = os.path.join(data_dir, f\"{SID}_whole_df.csv\")\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path, dtype={'Sleep_Stage': 'category'}, converters=converters, low_memory=True)\n",
    "                if debug:\n",
    "                    print(f\"Loaded data for subject {SID}\")\n",
    "                \n",
    "                # Downsample: every self.downsample-th row\n",
    "                if self.downsample != 1:\n",
    "                    df = df.iloc[::self.downsample].reset_index(drop=True)\n",
    "                    if debug:\n",
    "                        print(f\"After downsampling (factor {self.downsample}), rows: {len(df)}\")\n",
    "                \n",
    "                # Remove rows with \"Preparation\" phase if labeled 'P'\n",
    "                df = df[df['Sleep_Stage'] != 'P']\n",
    "\n",
    "                # Ensure numeric conversion for required columns\n",
    "                for col in ['ACC_X', 'ACC_Y', 'ACC_Z', 'BVP', 'TEMP', 'TIMESTAMP', 'HR', 'IBI']:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                \n",
    "                # Compute accelerometer magnitude from three axes\n",
    "                ACC = np.sqrt(df['ACC_X']**2 + df['ACC_Y']**2 + df['ACC_Z']**2)\n",
    "                \n",
    "                # Prepare the features: TIMESTAMP, BVP, TEMP and the computed ACC\n",
    "                df_X = df[['TIMESTAMP', 'BVP', 'TEMP', 'HR', 'IBI']].copy()\n",
    "                df_X['ACC'] = ACC\n",
    "                # Normalize the features (z-score normalization per subject)\n",
    "                #TEMP_norm = (df_X['TEMP'] - df_X['TEMP'].mean()) / df_X['TEMP'].std()\n",
    "                #df_X['TEMP'] = TEMP_norm\n",
    "                #BVP_norm = (df_X['BVP'] - df_X['BVP'].mean()) / df_X['BVP'].std()\n",
    "                #df_X['BVP'] = BVP_norm\n",
    "\n",
    "                \n",
    "                # Process sleep stage labels: trim whitespace and map to integer\n",
    "                df['Sleep_Stage'] = df['Sleep_Stage'].astype(str).str.strip()\n",
    "                df_Y = df['Sleep_Stage'].map(SLEEP_STAGE_MAPPING)\n",
    "                \n",
    "                # Convert features and labels to numpy arrays\n",
    "                data_arr = df_X.values.astype(np.float32)  # shape: [T, C]\n",
    "                labels_arr = df_Y.to_numpy()                # shape: [T]\n",
    "                T = data_arr.shape[0]\n",
    "\n",
    "                # If the record is too short (less than one chunk), pad it with NaNs (-1 for labels)\n",
    "                if T < self.chunk_length:\n",
    "                    pad_size = self.chunk_length - T\n",
    "                    padding_data = np.full((pad_size, data_arr.shape[1]), np.nan, dtype=np.float32)\n",
    "                    data_arr = np.concatenate([data_arr, padding_data], axis=0)\n",
    "                    padding_labels = np.full((pad_size,), -1)\n",
    "                    labels_arr = np.concatenate([labels_arr, padding_labels], axis=0)\n",
    "                    T = self.chunk_length  # update length\n",
    "\n",
    "                # Slide a window over the data with the defined stride to create overlapping chunks\n",
    "                for start in range(0, T - self.chunk_length + 1, self.stride):\n",
    "                    end = start + self.chunk_length\n",
    "                    chunk_data = data_arr[start:end, :]\n",
    "                    chunk_labels = labels_arr[start:end]\n",
    "                    self.chunks.append({\n",
    "                        'data': chunk_data,\n",
    "                        'labels': chunk_labels,\n",
    "                        'SID': SID\n",
    "                    })\n",
    "                if debug:\n",
    "                    num_chunks = (T - self.chunk_length) // self.stride + 1\n",
    "                    print(f\"Subject {SID}: {T} samples processed, generated {num_chunks} chunks\")\n",
    "            else:\n",
    "                print(f\"File {file_path} does not exist. Skipping subject {SID}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.chunks[idx]\n",
    "        data = torch.tensor(chunk['data'], dtype=torch.float32)\n",
    "        labels = torch.tensor(chunk['labels'], dtype=torch.long)\n",
    "        # Use forward_fill to replace any NaNs with previous values.\n",
    "        data = forward_fill(data)\n",
    "        labels = forward_fill(labels)\n",
    "        return data, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb17bf",
   "metadata": {},
   "source": [
    "### Construct train, val, and test datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d268017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in train dataset: 80\n",
      "Total samples in val dataset: 10\n",
      "Total samples in test dataset: 10\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "train_dataset_windowed = SleepDataset(subjects_list=subjects_train,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 window_size_ms= 20000, # 20 seconds of data\n",
    "                                 stride_ms=5000,        # 5 seconds overlap\n",
    "                                 downsample_freq=8, # downsample to 16Hz\n",
    "                                 debug=False)\n",
    "val_dataset_windowed = SleepDataset(subjects_list=subjects_val,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 window_size_ms= 20000, # 20 seconds of data\n",
    "                                 stride_ms=5000,        # 5 seconds overlap\n",
    "                                 downsample_freq=8, # downsample to 16Hz\n",
    "                                 debug=False)\n",
    "test_dataset_windowed = SleepDataset(subjects_list=subjects_test,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 window_size_ms= 20000, # 20 seconds of data\n",
    "                                 stride_ms=5000,        # 5 second stride\n",
    "                                 downsample_freq=8, # downsample to 16Hz\n",
    "                                 debug=False)\n",
    "'''\n",
    "\n",
    "train_dataset = SleepDataset(subjects_list=subjects_train,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 max_length=max_length,\n",
    "                                 downsample_freq=8, # downsample to 8Hz\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in train dataset: {len(train_dataset)}\")\n",
    "val_dataset = SleepDataset(subjects_list=subjects_val,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 max_length=max_length,\n",
    "                                 downsample_freq=8, # downsample to 8Hz\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in val dataset: {len(val_dataset)}\") \n",
    "test_dataset = SleepDataset(subjects_list=subjects_test,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 max_length=max_length,\n",
    "                                 downsample_freq=8, # downsample to 8Hz\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in test dataset: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fc8ab676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in train chunk dataset: 5107\n",
      "Total samples in val chunk dataset: 641\n",
      "Total samples in test chunk dataset: 665\n"
     ]
    }
   ],
   "source": [
    "train_chunk_dataset = SleepChunkDataset(subjects_list=subjects_train,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 chunk_duration=6000,  # 100 minutes\n",
    "                                 chunk_stride=300,    # 5 minutes\n",
    "                                 downsample_freq=0.2,   # downsample to 0.2 Hz\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in train chunk dataset: {len(train_chunk_dataset)}\")\n",
    "val_chunk_dataset = SleepChunkDataset(subjects_list=subjects_val,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 chunk_duration=6000,  # 100 minutes\n",
    "                                 chunk_stride=300,    # 5 minutes\n",
    "                                 downsample_freq=0.2,   # downsample to 0.2 Hz\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in val chunk dataset: {len(val_chunk_dataset)}\")\n",
    "test_chunk_dataset = SleepChunkDataset(subjects_list=subjects_test,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 chunk_duration=6000,  # 100 minutes\n",
    "                                 chunk_stride=300,    # 5 minutes\n",
    "                                 downsample_freq=0.2,   # downsample to 0.2 Hz\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in test chunk dataset: {len(test_chunk_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f119ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get class weights for weighted loss\n",
    "all_labels = []\n",
    "for batch in DataLoader(train_chunk_dataset, batch_size=1):\n",
    "    labels = batch[1].numpy()\n",
    "    all_labels.extend(labels.flatten())\n",
    "all_labels = np.array(all_labels)\n",
    "valid_labels = all_labels[all_labels != -1]\n",
    "classes = np.unique(valid_labels)\n",
    "class_counts = Counter(valid_labels)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=valid_labels\n",
    ")\n",
    "print(f\"Class counts: {class_counts}\")\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036b8a4",
   "metadata": {},
   "source": [
    "## CNN downsampling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfebfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence length x input channels -> CNN -> shortened sequence length x num hidden channels -> LSTM -> shortened sequence length x num sleep stages\n",
    "\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self, in_channels=4, cnn_output_channels=128):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=5, stride=2, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv1d(64, 64, kernel_size=5, stride=2, padding=1)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv4 = nn.Conv1d(64, cnn_output_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn = nn.BatchNorm1d(cnn_output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Global average pooling to collapse \n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert not torch.isnan(x).any(), \"NaN detected in CNN input\"\n",
    "        # Expect x of shape (batch, epoch_samples, channels)\n",
    "        x = x.permute(0, 2, 1)  # Rearrange to (batch, channels, epoch_samples)\n",
    "        #print(f\"Input shape after permutation: {x.shape}\")\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        #print(f\"Shape after conv1 and pool1: {x.shape}\")\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        #print(f\"Shape after conv2 and pool2: {x.shape}\")\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        #print(f\"Shape after conv3 and pool3: {x.shape}\")\n",
    "        x = self.relu(self.conv4(x))\n",
    "        #print(f\"Shape after conv4: {x.shape}\")\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class SleepStageLSTM(nn.Module):\n",
    "    def __init__(self, cnn_output_channels=128, hidden_size=64, num_layers=2, num_sleep_stages=5):\n",
    "        super(SleepStageLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=cnn_output_channels,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_size, num_sleep_stages)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert not torch.isnan(x).any(), \"NaN detected in LSTM input\"\n",
    "        # x is of shape (batch, cnn_output_channels, samples)\n",
    "        # LSTM expects input shape of (samples, batch, cnn_output_channels)\n",
    "        x = x.permute(2, 0, 1) # (batch, cnn_output_channels, samples) -> (samples, batch, cnn_output_channels)\n",
    "        #print(f\"Input shape after permutation: {x.shape}\")\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        #print(f\"Shape after LSTM: {lstm_out.shape}\")\n",
    "        # Option 1: produce a prediction for every epoch (each time step)\n",
    "        out = self.fc(lstm_out)   # shape: (batch, num_epochs, num_sleep_stages)\n",
    "        #print(f\"Shape after fully connected layer: {out.shape}\")\n",
    "        \n",
    "        # Option 2: if you want a prediction only for the current epoch,\n",
    "        # you may take the output of the last time step:\n",
    "        #predictions = self.fc(lstm_out[:, -1, :])  # shape: (batch, num_sleep_stages)\n",
    "        return out\n",
    "\n",
    "class OnlineSleepStagingModel(pl.LightningModule):\n",
    "    def __init__(self, in_channels, cnn_output_channels, lstm_hidden_size, num_layers=2, num_sleep_stages=5, learning_rate=0.001, class_weights=None):\n",
    "        super(OnlineSleepStagingModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.feature_extractor = FeatureExtractorCNN(in_channels=in_channels, cnn_output_channels=cnn_output_channels)\n",
    "        self.lstm_model = SleepStageLSTM(cnn_output_channels=cnn_output_channels, hidden_size=lstm_hidden_size, num_layers=num_layers, num_sleep_stages=num_sleep_stages)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Ignore the \"Missing\" label (-1)\n",
    "        if class_weights is not None:\n",
    "            self.criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32), ignore_index=-1)\n",
    "        self.num_sleep_stages = num_sleep_stages\n",
    "        self.cnn_output_channels = cnn_output_channels\n",
    "\n",
    "        self.val_class_counts = Counter()\n",
    "        self.pred_class_counts = Counter()\n",
    "        self.kappa = MulticlassCohenKappa(num_classes=self.num_sleep_stages)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.lstm_model(x) # (samples, batch_size, num_sleep_stages)\n",
    "        assert x.shape[2] == self.num_sleep_stages, \"LSTM output shape != num_sleep_stages\"\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch  # y shape: [batch_size, T]\n",
    "        y_hat = self(x)  # y_hat shape: [output_length, batch_size, num_sleep_stages]\n",
    "        # Check for NaNs in the network output\n",
    "        assert not torch.isnan(y).any(), \"NaN detected in labels\"\n",
    "        assert not torch.isnan(y_hat).any(), \"NaN detected in network output\"\n",
    "    \n",
    "        # Permute to batch first\n",
    "        y_hat = y_hat.permute(1, 0, 2)\n",
    "        output_length = y_hat.shape[1]\n",
    "        y_expanded = y.unsqueeze(1)\n",
    "        # Downsample y to match y_hat\n",
    "        y_resampled = torch.nn.functional.interpolate(\n",
    "            y_expanded.float(),\n",
    "            size = (output_length,),\n",
    "            mode = 'nearest'\n",
    "        )\n",
    "        y_resampled = y_resampled.squeeze(1).long()\n",
    "\n",
    "        # Flatten y_hat and y_resampled for loss calculation\n",
    "        batch_size, output_length, num_sleep_stages = y_hat.shape\n",
    "        y_hat_flat = y_hat.reshape(batch_size * output_length, num_sleep_stages)\n",
    "        y_resampled_flat = y_resampled.reshape(batch_size * output_length)\n",
    "        # Calculate loss\n",
    "        loss = self.criterion(y_hat_flat, y_resampled_flat)\n",
    "        # Check loss for finiteness\n",
    "        assert torch.isfinite(loss), \"Loss is not finite\"\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch  # y shape: [batch_size, T]\n",
    "        y_hat = self(x)  # y_hat shape: [output_length, batch_size, num_sleep_stages]\n",
    "        # Permute to batch first\n",
    "        y_hat = y_hat.permute(1, 0, 2)\n",
    "        output_length = y_hat.shape[1]\n",
    "        y_expanded = y.unsqueeze(1)\n",
    "        # Downsample y to match y_hat\n",
    "        y_resampled = torch.nn.functional.interpolate(\n",
    "            y_expanded.float(),\n",
    "            size = (output_length,),\n",
    "            mode = 'nearest'\n",
    "        )\n",
    "        y_resampled = y_resampled.squeeze(1).long()\n",
    "\n",
    "        # Flatten y_hat and y_resampled for loss calculation\n",
    "        batch_size, output_length, num_sleep_stages = y_hat.shape\n",
    "        y_hat_flat = y_hat.reshape(batch_size * output_length, num_sleep_stages)\n",
    "        y_resampled_flat = y_resampled.reshape(batch_size * output_length)\n",
    "        predictions = torch.argmax(y_hat_flat, dim=1)\n",
    "        # Calculate Cohen's Kappa\n",
    "        assert predictions.shape[0] == y_resampled_flat.shape[0], f\"Predictions and labels have different shapes (dim 0) {predictions.shape[0]} vs {y_resampled_flat.shape[0]}\"\n",
    "        cohen_kappa_score = self.kappa(predictions, y_resampled_flat)\n",
    "        self.log(\"val_cohen_kappa\", cohen_kappa_score, prog_bar=True)\n",
    "        # Calculate loss\n",
    "        loss = self.criterion(y_hat_flat, y_resampled_flat)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "\n",
    "        # Update class counts\n",
    "        # y_resampled_flat: [batch_size * output_length]\n",
    "        # predictions: [batch_size * output_length]\n",
    "\n",
    "        mask = y_resampled_flat != -1\n",
    "        y_valid = y_resampled_flat[mask]\n",
    "        preds_valid = predictions[mask]\n",
    "\n",
    "        if y_valid.numel() > 0:\n",
    "            self.kappa.update(preds_valid, y_valid)\n",
    "            self.val_class_counts.update(y_valid.cpu().tolist())\n",
    "            self.pred_class_counts.update(preds_valid.cpu().tolist())\n",
    "\n",
    "        return loss\n",
    "  \n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Log Cohen's Kappa\n",
    "        if self.kappa.confmat.sum() > 0:\n",
    "            cohen_kappa_score = self.kappa.compute()\n",
    "            self.log(\"val_cohen_kappa\", cohen_kappa_score, prog_bar=True)\n",
    "            self.kappa.reset()\n",
    "        else:\n",
    "            self.log(\"val_cohen_kappa\", 0.0, prog_bar=True)\n",
    "\n",
    "        # W&B class distribution bar plots\n",
    "        class_labels = list(range(self.num_sleep_stages))\n",
    "        val_counts = [self.val_class_counts.get(c, 0) for c in class_labels]\n",
    "        pred_counts = [self.pred_class_counts.get(c, 0) for c in class_labels]\n",
    "\n",
    "        for c in range(self.num_sleep_stages):\n",
    "            self.log(f\"class_count_true/{c}\", self.val_class_counts.get(c, 0), on_epoch=True, prog_bar=False)\n",
    "            self.log(f\"class_count_pred/{c}\", self.pred_class_counts.get(c, 0), on_epoch=True, prog_bar=False)\n",
    "\n",
    "        self.val_class_counts.clear()\n",
    "        self.pred_class_counts.clear()\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "class CNNClassifier(pl.LightningModule):\n",
    "    def __init__(self, in_channels, cnn_output_channels, lstm_hidden_size, num_layers=2, num_sleep_stages=5, learning_rate=0.001):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.feature_extractor = FeatureExtractorCNN(in_channels=in_channels, cnn_output_channels=cnn_output_channels)\n",
    "        self.classifier = nn.Linear(cnn_output_channels, num_sleep_stages)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Ignore the \"Missing\" label (-1)\n",
    "        self.num_sleep_stages = num_sleep_stages\n",
    "        self.cnn_output_channels = cnn_output_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        assert x.shape[2] == self.num_sleep_stages, \"CNN output shape != num_sleep_stages\"\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df131758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1200, 6]) (epoch_samples, channels)\n",
      "Output shape: torch.Size([1, 128, 18]) (batch_size, cnn_output_channels, epoch_samples)\n",
      "Combined model output shape: torch.Size([18, 1, 5]) (samples, batch_size, num_sleep_stages)\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# demo of model elements\n",
    "temp_input = train_chunk_dataset[0][0]\n",
    "print(f\"Input shape: {temp_input.shape} (epoch_samples, channels)\")\n",
    "CNN_model = FeatureExtractorCNN(in_channels=6, cnn_output_channels=128)\n",
    "CNN_model.eval()\n",
    "cnn_output = CNN_model(temp_input.unsqueeze(0))\n",
    "print(f\"Output shape: {cnn_output.shape} (batch_size, cnn_output_channels, epoch_samples)\")\n",
    "LSTM_model = SleepStageLSTM(cnn_output_channels=128, hidden_size=64, num_layers=2, num_sleep_stages=5)\n",
    "LSTM_output = LSTM_model(cnn_output)\n",
    "\n",
    "# demo of combined model\n",
    "combined_model = OnlineSleepStagingModel(in_channels=6, cnn_output_channels=128, lstm_hidden_size=64, num_layers=2, num_sleep_stages=5)\n",
    "combined_model.eval()\n",
    "combined_output = combined_model(temp_input.unsqueeze(0))\n",
    "print(f\"Combined model output shape: {combined_output.shape} (samples, batch_size, num_sleep_stages)\")\n",
    "print(combined_output.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171e2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.66666666666667\n"
     ]
    }
   ],
   "source": [
    "print(temp_input.shape[0] / combined_output.shape[0])\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cdddd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4m ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                 | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | feature_extractor | FeatureExtractorCNN  | 35.0 K | train\n",
      "1 | lstm_model        | SleepStageLSTM       | 54.6 K | train\n",
      "2 | criterion         | CrossEntropyLoss     | 0      | train\n",
      "3 | kappa             | MulticlassCohenKappa | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "89.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "89.6 K    Total params\n",
      "0.358     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.003631591796875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd66340cd3546dbaa87363a0da27ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.004706382751464844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7461e8477c54cc181f84c06f2b6d5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.004597663879394531,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc8a262364f4882be8e8d09e4541b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.556\n"
     ]
    }
   ],
   "source": [
    "# Train the combined model\n",
    "wandb_logger = WandbLogger(name=\"online_sleep_staging_model\", project=\"sleep_stage_classification\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='best-checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    devices=1,\n",
    "    accelerator='gpu',\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback]\n",
    ")\n",
    "model = OnlineSleepStagingModel(in_channels=6, cnn_output_channels=32, lstm_hidden_size=128, num_layers=2, num_sleep_stages=5, learning_rate=1e-4, class_weights=class_weights)\n",
    "train_loader = DataLoader(train_chunk_dataset, batch_size=8, num_workers=8, shuffle=True)\n",
    "val_loader = DataLoader(val_chunk_dataset, batch_size=8, num_workers=8, shuffle=False)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "wandb.finish()\n",
    "# Load the best model\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "best_model = OnlineSleepStagingModel.load_from_checkpoint(best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b731ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    cnn_output_channels = trial.suggest_categorical(\"cnn_output_channels\", [8, 16, 32, 64])\n",
    "    lstm_hidden_size = trial.suggest_categorical(\"lstm_hidden_size\", [32, 64, 128])\n",
    "    num_layers = 2\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "\n",
    "    wandb_logger = WandbLogger(name=f\"CNN{cnn_output_channels}_hs{lstm_hidden_size}_lr{learning_rate}\", project=\"optuna_sleep_stage_classification\")\n",
    "    # DataLoaders (resample based on batch size)\n",
    "    train_loader = DataLoader(train_chunk_dataset, batch_size=16, num_workers=8, shuffle=True)\n",
    "val_loader = DataLoader(val_chunk_dataset, batch_size=16, num_workers=8, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = OnlineSleepStagingModel(\n",
    "        in_channels=6,\n",
    "        cnn_output_channels=cnn_output_channels,\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        num_sleep_stages=5,\n",
    "        learning_rate=learning_rate,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "\n",
    "    # Trainer with pruning callback\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,\n",
    "        devices=1,\n",
    "        accelerator=\"gpu\",\n",
    "        logger=wandb_logger,\n",
    "        enable_checkpointing=False,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    wandb.finish()\n",
    "    clear_output()\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "best_trial = study.best_trial\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d3983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project overview\n",
    "# construct dataset class\n",
    "# - train CNN -> LSTM sleep staging model with causal conv and unidirectional LSTM\n",
    "# - train CNN -> LSTM sleep staging model with non-causal conv and bidirectional LSTM\n",
    "# - compare performance at 64Hz\n",
    "# - compare performance and computational cost at lower sampling rates\n",
    "\n",
    "# unstructured notes\n",
    "# - likely best to combine ACC columns into a single variable. Can't imagine they offer much additional information\n",
    "# - CNN model should use 2D CNNs to extract features across both channels and short-term time windows\n",
    "# - will need to pad data on both sides to ensure input and output sequences for the CNN are the same length\n",
    "\n",
    "\n",
    "# CNN model - input: seq_len x num_channels -> output: seq_len x hidden_size\n",
    "\n",
    "\n",
    "# LSTM model - input: seq_len x hidden_size -> output: seq_len x num_classes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc4f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data into train val and test by subject\n",
    "participant_info_df = pd.read_csv('/gpfs/data/oermannlab/users/slj9342/dl4med_25/data/physionet.org/files/dreamt/2.0.0/participant_info.csv')\n",
    "subject_ids = participant_info_df['subject_id'].unique()\n",
    "train_subjects = subject_ids[:int(len(subject_ids)*0.8)]\n",
    "val_subjects = subject_ids[int(len(subject_ids)*0.8):int(len(subject_ids)*0.9)]\n",
    "test_subjects = subject_ids[int(len(subject_ids)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc8b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demo_df.columns)\n",
    "print(demo_df['Sleep_Stage'].unique())\n",
    "print(demo_df['TIMESTAMP'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680457e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "sleep_stage_labels = le.fit_transform(demo_df['Sleep_Stage'])\n",
    "plt.plot(sleep_stage_labels)\n",
    "plt.title('Sleep Stage Labels')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sleep Stage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c5822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "cols_X = ['TIMESTAMP', 'BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP']\n",
    "cols_Y = ['Sleep_Stage']\n",
    "\n",
    "class DreamtDataset(Dataset):\n",
    "    def __init__(self, data_dir, subject_ids, cols_X, cols_Y):\n",
    "        self.data_dir = data_dir\n",
    "        self.subject_ids = subject_ids\n",
    "        self.cols_X = cols_X\n",
    "        self.cols_Y = cols_Y\n",
    "        self.data = []\n",
    "        for subject_id in subject_ids:\n",
    "            df = pd.read_csv(data_dir + f'S{subject_id:03d}_whole_df.csv')\n",
    "            df = df[cols_X + cols_Y]\n",
    "            df['subject_id'] = subject_id\n",
    "            self.data.append(df)\n",
    "        self.data = pd.concat(self.data, ignore_index=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        X = torch.tensor(row[self.cols_X].values, dtype=torch.float32)\n",
    "        y = torch.tensor(row[self.cols_Y].values, dtype=torch.long)\n",
    "        return X, y\n",
    "\n",
    "# CNN model with 4 2D conv layers, relu activation, dropout of 0.2, and 2 fully connected layers, first one has size 4096, second with 1500, both fc layers have dropout of 0.5\n",
    "# last fc layer has output size of 5, the number of channels to feed into the LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input shape: (batch_size, 1, num_channels, time_window)\n",
    "# Example: 6 EEG channels, 128-sample time windows → (batch, 1, 6, 128)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sliding_windows(x, window_size, stride=1):\n",
    "    # x: (batch, channels, time)\n",
    "    batch, channels, total_len = x.shape\n",
    "\n",
    "    # Pad on both sides: same number of windows as original time length\n",
    "    pad = (window_size - 1) // 2\n",
    "    x_padded = F.pad(x, pad=(pad, pad), mode='replicate')\n",
    "\n",
    "    # Now generate sliding windows\n",
    "    windows = x_padded.unfold(dimension=2, size=window_size, step=stride)  # (batch, channels, time, window_size)\n",
    "    windows = windows.permute(0, 2, 1, 3)  # (batch, time, channels, window_size)\n",
    "    return windows  # shape: (batch, time, channels, window_size)\n",
    "\n",
    "\n",
    "\n",
    "class DreamtCNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_chans_out):\n",
    "        super(DreamtCNN, self).__init__()\n",
    "        c, t = input_shape  # c = number of channels (e.g. 6), t = time_window size (e.g. 128)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 5), stride=1, padding=(1, 2))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 5), stride=1, padding=(1, 2))\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 5), stride=1, padding=(1, 2))\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=(3, 5), stride=1, padding=(1, 2))\n",
    "\n",
    "        self.conv_dropout = nn.Dropout(0.2)\n",
    "        self.ff_dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Output dims after conv layers will be (batch, 256, c, t)\n",
    "        self.fc1 = nn.Linear(256 * c * t, 4096)\n",
    "        self.fc2 = nn.Linear(4096, num_chans_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, 1, channels, time_window)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv_dropout(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv_dropout(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.conv_dropout(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.conv_dropout(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.ff_dropout(x)\n",
    "        x = self.fc2(x)  # output shape: (batch_size, num_chans_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define test parameters\n",
    "batch_size = 4\n",
    "num_channels = 6\n",
    "time_window = 128\n",
    "seq_len = 1000  # e.g., length of the sequence to be fed into the LSTM\n",
    "num_chans_out = 5  # e.g., number of channels to feed into the LSTM\n",
    "\n",
    "# create dummy imput \n",
    "x = torch.randn(batch_size, 1, num_channels, time_window)  # (batch_size, 1, num_channels, time_window)\n",
    "\n",
    "# window input\n",
    "x = x.squeeze(1)  # (batch_size, num_channels, time_window)\n",
    "x = sliding_windows(x, window_size=time_window, stride=1)\n",
    "\n",
    "print(f\"Shape after sliding windows: {x.shape}\")\n",
    "\n",
    "# Instantiate the model\n",
    "model = DreamtCNN(input_shape=(num_channels, time_window), num_chans_out=num_chans_out)\n",
    "\n",
    "# Forward pass\n",
    "# x shape: (batch, time, channels, window)\n",
    "x = x.unsqueeze(2)  # -> (batch, time, 1, channels, window)\n",
    "\n",
    "# Reshape for CNN: merge batch and time into one dim\n",
    "b, t, one, c, w = x.shape\n",
    "x = x.view(b * t, 1, c, w)  # (b * t, 1, channels, window)\n",
    "\n",
    "# Feed into CNN\n",
    "out = model(x)  # shape: (b * t, output_dim)\n",
    "\n",
    "# Restore sequence shape\n",
    "out = out.view(b, t, -1)  # (batch, time, output_dim)\n",
    "\n",
    "\n",
    "# Print shape\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl4med_25)",
   "language": "python",
   "name": "dl4med_25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
