{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1306768",
   "metadata": {},
   "source": [
    "# Title and overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94600e31",
   "metadata": {},
   "source": [
    "# Imports and Defintions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c59be11",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchmetrics.classification import MulticlassCohenKappa\n",
    "from IPython.display import clear_output\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, confusion_matrix, roc_auc_score\n",
    "from collections import Counter\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "import wandb\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8184e68",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_float(x): # Helper to safely convert strings to floats\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def forward_fill(x: torch.Tensor): # Forward‑fill NaNs in each channel\n",
    "    single = False\n",
    "    if x.dim() == 1:\n",
    "        x = x.unsqueeze(1)\n",
    "        single = True\n",
    "    T, C = x.shape\n",
    "    for c in range(C):\n",
    "        if torch.isnan(x[0, c]):\n",
    "            x[0, c] = 0.0\n",
    "        for t in range(1, T):\n",
    "            if torch.isnan(x[t, c]):\n",
    "                x[t, c] = x[t - 1, c]\n",
    "    return x.squeeze(1) if single else x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26f35c3",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58038a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualFreqDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 subjects_list,\n",
    "                 data_dir,\n",
    "                 chunk_duration: float = 600,\n",
    "                 chunk_stride: float = 300,\n",
    "                 high_freq: int = 32,\n",
    "                 low_freq: int = 8,\n",
    "                 hf_features: list = None,\n",
    "                 lf_features: list = None,\n",
    "                 debug: bool = False):\n",
    "        self.hf_downsample = int(64 // high_freq) # downsample factor for high frequency data\n",
    "        self.lf_downsample = int(64 // low_freq) # downsample factor for low frequency data\n",
    "\n",
    "        SLEEP_STAGE_MAPPING = {\n",
    "            \"W\": 0,    # Wake\n",
    "            \"N1\": 1,   # non-REM stage 1 (light sleep_)\n",
    "            \"N2\": 1,   # non-REM stage 2 (light sleep)\n",
    "            \"N3\": 2,   # non-REM stage 3 (deep sleep)\n",
    "            \"R\": 3,    # REM\n",
    "            \"Missing\": -1  # Missing label → ignore\n",
    "        }\n",
    "        numeric_columns = ['TIMESTAMP', 'BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP', 'EDA', 'HR', 'IBI']\n",
    "        converters = {col: safe_float for col in numeric_columns}\n",
    "\n",
    "        self.chunks = []\n",
    "        for SID in subjects_list:\n",
    "            path = os.path.join(data_dir, f\"{SID}_whole_df.csv\")\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"File {path} does not exist.\")\n",
    "            # Load data for subject\n",
    "            df = pd.read_csv(path,\n",
    "                             dtype={'Sleep_Stage': 'category'},\n",
    "                             converters=converters,\n",
    "                             low_memory=True)\n",
    "            \n",
    "            # drop preparation phase, map labels\n",
    "            df = df[df['Sleep_Stage'] != 'P']\n",
    "            df['Sleep_Stage'] = df['Sleep_Stage'].astype(str).str.strip()\n",
    "            labels_arr = (\n",
    "                df['Sleep_Stage']\n",
    "                  .map(SLEEP_STAGE_MAPPING)\n",
    "                  .fillna(-1)\n",
    "                  .astype(int)\n",
    "                  .to_numpy()\n",
    "            )\n",
    "            # combine ACC_X, ACC_Y, ACC_Z into a single feature\n",
    "            df['ACC'] = np.sqrt(df['ACC_X']**2 + df['ACC_Y']**2 + df['ACC_Z']**2)\n",
    "            # separate high and low frequency data\n",
    "            df_high = df[hf_features].copy()\n",
    "            df_low = df[lf_features].copy()\n",
    "            # downsample data and labels\n",
    "            df_high = df_high.iloc[::self.hf_downsample, :].reset_index(drop=True)\n",
    "            df_low = df_low.iloc[::self.lf_downsample, :].reset_index(drop=True)\n",
    "            labels_arr = labels_arr[::self.lf_downsample]\n",
    "            # normalize data\n",
    "            df_high = (df_high - df_high.mean()) / (df_high.std().replace(0, 1e-6))\n",
    "            df_low = (df_low - df_low.mean()) / (df_low.std().replace(0, 1e-6))\n",
    "            # create chunks\n",
    "            total_time = int(len(df_high) / high_freq)\n",
    "            n_chunks = int((total_time - chunk_duration) // chunk_stride) + 1\n",
    "            for i in range(n_chunks):\n",
    "                start_time = i * chunk_stride\n",
    "                end_time = start_time + chunk_duration\n",
    "                \n",
    "                start_low = int(start_time * low_freq)\n",
    "                end_low = int(end_time * low_freq)\n",
    "                start_high = int(start_time * high_freq)\n",
    "                end_high = int(end_time * high_freq)\n",
    "\n",
    "                lf_chunk = df_low .iloc[start_low: end_low ].values.astype(np.float32)\n",
    "                hf_chunk = df_high.iloc[start_high:end_high].values.astype(np.float32)\n",
    "                labels_chunk = labels_arr[start_low: end_low]\n",
    "\n",
    "                lf_chunk = forward_fill(torch.tensor(lf_chunk, dtype=torch.float32))\n",
    "                hf_chunk = forward_fill(torch.tensor(hf_chunk, dtype=torch.float32))\n",
    "                labels_chunk = torch.tensor(labels_chunk, dtype=torch.long)\n",
    "                \n",
    "                if (labels_chunk != -1).any():\n",
    "                    self.chunks.append({\n",
    "                        'high': hf_chunk,\n",
    "                        'low': lf_chunk,\n",
    "                        'labels': labels_chunk,\n",
    "                    })\n",
    "        if debug:\n",
    "            print(f\"Loaded {len(self.chunks)} chunks from {len(subjects_list)} subjects.\")\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.chunks[idx]\n",
    "        hf = chunk['high']\n",
    "        lf = chunk['low']\n",
    "        labels = chunk['labels']\n",
    "        return hf, lf, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2d09d",
   "metadata": {},
   "source": [
    "## Optuna objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f1d69",
   "metadata": {},
   "source": [
    "### LSTM Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037c643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    hf_input_channels = len(hf_features)\n",
    "    lf_input_channels = len(lf_features)\n",
    "    lstm_hidden_size = trial.suggest_categorical(\"lstm_hidden_size\", [32, 64, 128, 256])\n",
    "    lstm_num_layers = trial.suggest_categorical(\"lstm_num_layers\",[2,4,6])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    num_sleep_stages = 4\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    label_smoothing = trial.suggest_float('label_smoothing', 0.0, 0.3)\n",
    "\n",
    "    # create the logger and callbacks\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"LSTM-Sleep-Stager\",\n",
    "        name=f\"optuna-{trial.number}\",\n",
    "        log_model=True,\n",
    "        save_dir=\"wandb_logs\"\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/LSTM/Optuna',\n",
    "    filename='best-checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    "    )\n",
    "    early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    "    )\n",
    "    # Create the model\n",
    "    model = LSTMSleepStager(\n",
    "        hf_input_channels=hf_input_channels,\n",
    "        lf_input_channels=lf_input_channels,\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_num_layers=lstm_num_layers,\n",
    "        lstm_bidirectional=True,\n",
    "        dropout=dropout,\n",
    "        num_sleep_stages=num_sleep_stages,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing=label_smoothing,\n",
    "        weight_tensor=torch.tensor(class_weights, dtype=torch.float32),\n",
    "        debug=False\n",
    "    )\n",
    "\n",
    "    # Define the trainer\n",
    "    trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    devices=1,\n",
    "    accelerator='gpu',\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    log_every_n_steps=1,\n",
    "    precision=\"16-mixed\",\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # return best val loss\n",
    "    best_val_loss = checkpoint_callback.best_model_score.item()\n",
    "    \n",
    "    wandb.finish()\n",
    "    clear_output()\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992759c",
   "metadata": {},
   "source": [
    "### TCN Only Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0912214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tcn_objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    hf_input_channels = len(hf_features)\n",
    "    lf_input_channels = len(lf_features)\n",
    "    cnn_output_channels = trial.suggest_categorical(\"cnn_output_channels\", [8, 16, 32, 64, 128])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    num_sleep_stages = 4\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    label_smoothing = trial.suggest_float('label_smoothing', 0.0, 0.3)\n",
    "    num_tcn_blocks = trial.suggest_int(\"num_tcn_blocks\", 3, 8)\n",
    "    tcn_kernel_size = trial.suggest_categorical(\"tcn_kernel_size\", [3, 5, 7])\n",
    "\n",
    "    # create the logger and callbacks\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"TCN-Sleep-Stager\",\n",
    "        name=f\"optuna-{trial.number}\",\n",
    "        log_model=True,\n",
    "        save_dir=\"wandb_logs\"\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/TCN/Optuna',\n",
    "    filename='best-checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    "    )\n",
    "    early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    "    )\n",
    "    # Create the model\n",
    "    model = ConvSleepStager(\n",
    "        hf_input_channels=hf_input_channels,\n",
    "        lf_input_channels=lf_input_channels,\n",
    "        cnn_output_channels=cnn_output_channels,\n",
    "        dropout=dropout,\n",
    "        num_sleep_stages=num_sleep_stages,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing=label_smoothing,\n",
    "        weight_tensor=torch.tensor(class_weights, dtype=torch.float32),\n",
    "        convnet='TCN',\n",
    "        num_tcn_blocks=num_tcn_blocks,\n",
    "        tcn_kernel_size=tcn_kernel_size,\n",
    "        debug=False\n",
    "    )\n",
    "\n",
    "    # Define the trainer\n",
    "    trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    devices=1,\n",
    "    accelerator='gpu',\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    log_every_n_steps=1,\n",
    "    precision=\"16-mixed\",\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # return best val loss\n",
    "    best_val_loss = checkpoint_callback.best_model_score.item()\n",
    "    \n",
    "    wandb.finish()\n",
    "    clear_output()\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e844a",
   "metadata": {},
   "source": [
    "### TCN-LSTM Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tcn_lstm_objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    hf_input_channels = len(hf_features)\n",
    "    lf_input_channels = len(lf_features)\n",
    "    cnn_output_channels = trial.suggest_categorical(\"cnn_output_channels\", [8, 16, 32, 64, 128])\n",
    "    lstm_hidden_size = trial.suggest_categorical(\"lstm_hidden_size\", [32, 64, 128, 256])\n",
    "    lstm_num_layers = trial.suggest_categorical(\"lstm_num_layers\",[2,4,6])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    num_sleep_stages = 4\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    label_smoothing = trial.suggest_float('label_smoothing', 0.0, 0.3)\n",
    "    num_tcn_blocks = trial.suggest_int(\"num_tcn_blocks\", 3, 8)\n",
    "    tcn_kernel_size = trial.suggest_categorical(\"tcn_kernel_size\", [3, 5, 7])\n",
    "\n",
    "    # create the logger and callbacks\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"TCN-LSTM-Sleep-Stager\",\n",
    "        name=f\"optuna-{trial.number}\",\n",
    "        log_model=True,\n",
    "        save_dir=\"wandb_logs\"\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/TCN-LSTM/Optuna',\n",
    "    filename='best-checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    "    )\n",
    "    early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    "    )\n",
    "    # Create the model\n",
    "    model = DualFreqSleepStager(\n",
    "        hf_input_channels=hf_input_channels,\n",
    "        lf_input_channels=lf_input_channels,\n",
    "        cnn_output_channels=cnn_output_channels,\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_num_layers=lstm_num_layers,\n",
    "        lstm_bidirectional=True,\n",
    "        dropout=dropout,\n",
    "        num_sleep_stages=num_sleep_stages,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing=label_smoothing,\n",
    "        weight_tensor=torch.tensor(class_weights, dtype=torch.float32),\n",
    "        convnet='TCN',\n",
    "        num_tcn_blocks=num_tcn_blocks,\n",
    "        tcn_kernel_size=tcn_kernel_size,\n",
    "        debug=False\n",
    "    )\n",
    "\n",
    "    # Define the trainer\n",
    "    trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    devices=1,\n",
    "    accelerator='gpu',\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    log_every_n_steps=1,\n",
    "    precision=\"16-mixed\",\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # return best val loss\n",
    "    best_val_loss = checkpoint_callback.best_model_score.item()\n",
    "    \n",
    "    wandb.finish()\n",
    "    clear_output()\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e25929",
   "metadata": {},
   "source": [
    "### CNN-LSTM Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_lstm_objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    hf_input_channels = len(hf_features)\n",
    "    lf_input_channels = len(lf_features)\n",
    "    cnn_output_channels = trial.suggest_categorical(\"cnn_output_channels\", [8, 16, 32, 64, 128])\n",
    "    lstm_hidden_size = trial.suggest_categorical(\"lstm_hidden_size\", [32, 64, 128, 256])\n",
    "    lstm_num_layers = trial.suggest_categorical(\"num_layers\",[2,4,6])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    num_sleep_stages = 4\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    label_smoothing = trial.suggest_float('label_smoothing', 0.0, 0.3)\n",
    "\n",
    "    # create the logger and callbacks\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"CNN-LSTM-Sleep-Stager\",\n",
    "        name=f\"optuna-{trial.number}\",\n",
    "        log_model=True,\n",
    "        save_dir=\"wandb_logs\"\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/CNN-LSTM/Optuna',\n",
    "    filename='best-checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    "    )\n",
    "    early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=8,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    "    )\n",
    "    # Create the model\n",
    "    model = DualFreqSleepStager(\n",
    "        hf_input_channels=hf_input_channels,\n",
    "        lf_input_channels=lf_input_channels,\n",
    "        cnn_output_channels=cnn_output_channels,\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_num_layers=lstm_num_layers,\n",
    "        lstm_bidirectional=True,\n",
    "        dropout=dropout,\n",
    "        num_sleep_stages=num_sleep_stages,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing=label_smoothing,\n",
    "        weight_tensor=torch.tensor(class_weights, dtype=torch.float32),\n",
    "        convnet='CNN',\n",
    "        debug=False\n",
    "    )\n",
    "\n",
    "    # Define the trainer\n",
    "    trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    devices=1,\n",
    "    accelerator='gpu',\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    log_every_n_steps=1,\n",
    "    precision=\"16-mixed\",\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # return best val loss\n",
    "    best_val_loss = checkpoint_callback.best_model_score.item()\n",
    "    \n",
    "    wandb.finish()\n",
    "    clear_output()\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cea6ec",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12f4d2",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSleepStager(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 hf_input_channels=2,\n",
    "                 lf_input_channels=5,\n",
    "                 lstm_hidden_size=64,\n",
    "                 lstm_num_layers=2,\n",
    "                 lstm_bidirectional=True,\n",
    "                 dropout=0.1,\n",
    "                 num_sleep_stages=5,\n",
    "                 learning_rate=1e-3,\n",
    "                 weight_decay=1e-5,\n",
    "                 label_smoothing=0.0,\n",
    "                 weight_tensor=None,\n",
    "                 debug=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lstm = nn.LSTM(\n",
    "                            input_size= hf_input_channels + lf_input_channels,\n",
    "                            hidden_size=lstm_hidden_size,\n",
    "                            num_layers=lstm_num_layers,\n",
    "                            bidirectional=lstm_bidirectional,\n",
    "                            dropout=dropout,\n",
    "                            batch_first=False)\n",
    "        \n",
    "        if lstm_bidirectional:\n",
    "            self.classifier = nn.Linear(lstm_hidden_size * 2, num_sleep_stages, )\n",
    "        else:\n",
    "            self.classifier = nn.Linear(lstm_hidden_size, num_sleep_stages)\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.kappa = MulticlassCohenKappa(num_classes=num_sleep_stages)\n",
    "        self.debug = debug\n",
    "        \n",
    "        if weight_tensor is not None:\n",
    "            assert weight_tensor.shape[0] == num_sleep_stages, \\\n",
    "                f\"Weight tensor shape {weight_tensor.shape[0]} does not match number of sleep stages {num_sleep_stages}\"\n",
    "        self.train_criterion = nn.CrossEntropyLoss(weight=weight_tensor, ignore_index=-1,label_smoothing=label_smoothing)\n",
    "        self.val_criterion = nn.CrossEntropyLoss(weight=weight_tensor, ignore_index=-1)\n",
    "\n",
    "    def forward(self, hf, lf):\n",
    "        # assert no nan values in input\n",
    "        assert not torch.isnan(hf).any(), \"NaN detected in CNN input\"\n",
    "        assert not torch.isnan(lf).any(), \"NaN detected in LSTM input\"\n",
    "        if self.debug:\n",
    "            print(f\"HF input shape: {hf.shape}\")\n",
    "            print(f\"LF input shape: {lf.shape}\")\n",
    "\n",
    "        # downsample longer sequence\n",
    "        hf_output_length = hf.shape[1]\n",
    "        lf_output_length = lf.shape[1]\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] hf length {hf_output_length} > lf length {lf_output_length}, downsampling\")\n",
    "        hf = F.interpolate(\n",
    "                hf.permute(0, 2, 1), # (batch_size, hf_input_channels, sequence_length)\n",
    "                size=lf_output_length,\n",
    "            )\n",
    "        hf = hf.permute(0, 2, 1) # (batch_size, sequence_length, hf_input_channels)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] hf features shape: {hf.shape}\")\n",
    "            print(f\"[DEBUG] lf features shape: {lf.shape}\")\n",
    "        \n",
    "        # concatenate high and low frequency features\n",
    "        a = hf.permute(1,0,2) # (sequence_length, batch_size, hf_input_channels)\n",
    "        b = lf.permute(1,0,2) # (sequence_length, batch_size, lf_input_channels)\n",
    "        x = torch.cat((a, b), dim=2)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] lstm input shape: {x.shape}\")\n",
    "        \n",
    "        # pass through LSTM + classifier\n",
    "        x, _ = self.lstm(x)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] lstm output shape: {x.shape}\")\n",
    "        x = self.classifier(x)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] classifier output shape: {x.shape}\")\n",
    "        return x\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        hf, lf, labels = batch\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] training step batch {batch_idx}\")\n",
    "            print(f\"[DEBUG] hf shape: {hf.shape}\")\n",
    "            print(f\"[DEBUG] lf shape: {lf.shape}\")\n",
    "            print(f\"[DEBUG] labels shape: {labels.shape}\")\n",
    "        \n",
    "        logits = self(hf, lf)\n",
    "        logits = logits.permute(1, 0, 2) # should be (batch_size, seq_len, num_classes)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] logits shape after permute: {logits.shape}\")\n",
    "\n",
    "        # flatten\n",
    "        batch_size, seq_len, num_classes = logits.shape\n",
    "        logits_flat = logits.reshape(batch_size * seq_len, num_classes)\n",
    "        labels_flat = labels.reshape(batch_size * seq_len)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = self.train_criterion(logits_flat, labels_flat)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] loss: {loss.item()}\")\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        hf, lf, labels = batch\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] validation step batch {batch_idx}\")\n",
    "            print(f\"[DEBUG] hf shape: {hf.shape}\")\n",
    "            print(f\"[DEBUG] lf shape: {lf.shape}\")\n",
    "            print(f\"[DEBUG] labels shape: {labels.shape}\")\n",
    "\n",
    "        logits = self(hf, lf)\n",
    "        logits = logits.permute(1, 0, 2)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] logits shape after permute: {logits.shape}\")\n",
    "        # flatten\n",
    "        batch_size, seq_len, num_classes = logits.shape\n",
    "        logits_flat = logits.reshape(batch_size * seq_len, num_classes)\n",
    "        labels_flat = labels.reshape(batch_size * seq_len)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] logits_flat shape: {logits_flat.shape}\")\n",
    "            print(f\"[DEBUG] labels_flat shape: {labels_flat.shape}\")\n",
    "        # calculate loss\n",
    "        loss = self.val_criterion(logits_flat, labels_flat)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] validation loss: {loss.item()}\")\n",
    "        # calculate accuracy\n",
    "        preds = torch.argmax(logits_flat, dim=1)\n",
    "        mask = labels_flat != -1\n",
    "        masked_preds = preds[mask]\n",
    "        masked_labels = labels_flat[mask]\n",
    "        if masked_labels.numel() > 0:\n",
    "            acc = (masked_preds == masked_labels).float().mean().item()\n",
    "        else:\n",
    "            acc = 0.0\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] validation accuracy: {acc}\")\n",
    "        # calculate kappa\n",
    "        self.kappa.update(masked_preds, masked_labels)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] validation kappa: {kappa}\")\n",
    "\n",
    "        # log metrics\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "        return {\n",
    "            'val_loss': loss,\n",
    "            'val_acc': acc\n",
    "        }\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        kappa = self.kappa.compute()\n",
    "        self.log('val_cohen_kappa', torch.nan_to_num(kappa,0.0), prog_bar=True)\n",
    "        self.kappa.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8029235b",
   "metadata": {},
   "source": [
    "### TCN\n",
    "As defined in Bai et al https://arxiv.org/pdf/1803.01271, TCN blocks are residual blocks, each containing two dilated convolutions with relu activation and batch normalization. Subsequent blocks have increasing dilations, allowing for the capture of patterns at increasing timescales. Often these are causally padded but for our use case, we decided to forego this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "     input_channels, output_channels, kernel_size, dilation, stride=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) * dilation // 2 # preserve sequence length\n",
    "        self.conv1 = nn.Conv1d(input_channels, output_channels, kernel_size,\n",
    "                               stride=stride, padding=padding,\n",
    "                               dilation=dilation)\n",
    "        self.bn1   = nn.BatchNorm1d(output_channels)\n",
    "        self.conv2 = nn.Conv1d(output_channels, output_channels, kernel_size,\n",
    "                               stride=1, padding=padding,\n",
    "                               dilation=dilation)\n",
    "        self.bn2   = nn.BatchNorm1d(output_channels)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        # 1×1 conv to match channels/stride if needed\n",
    "        self.downsample = (nn.Conv1d(input_channels, output_channels, 1, stride=stride)\n",
    "                           if (stride!=1 or input_channels!=output_channels) else None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x arrives as (batch, channels, seq_len)\n",
    "        identity = x                     # save the original for the skip path\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)          \n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.dropout(out)\n",
    "        # downsampled if needed\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "        # add & activate\n",
    "        return self.relu(out + identity)\n",
    "\n",
    "class HFFeatureExtractorTCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 num_blocks=5,\n",
    "                 kernel_size=3,\n",
    "                 base_channels=16,\n",
    "                 final_down=64,     # match CNN’s total downsample factor (~64)\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        ch = in_channels\n",
    "        # build dilated residual blocks (no downsampling here)\n",
    "        for i in range(num_blocks):\n",
    "            layers.append(\n",
    "                TemporalBlock(ch, base_channels,\n",
    "                              kernel_size=kernel_size,\n",
    "                              dilation=2**i,\n",
    "                              stride=1,\n",
    "                              dropout=dropout)\n",
    "            )\n",
    "            ch = base_channels\n",
    "        # final 1×1 conv with stride=final_down to downsample by 64\n",
    "        layers.append(nn.Conv1d(ch, out_channels,\n",
    "                                kernel_size=1,\n",
    "                                stride=final_down,\n",
    "                                padding=0))\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, in_channels)\n",
    "        x = x.permute(0,2,1)  # to (batch, channels, seq_len)\n",
    "        y = self.tcn(x)       # returns (batch, out_channels, seq_len/64)\n",
    "        return y             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af1eb51",
   "metadata": {},
   "source": [
    "### CNN\n",
    "As laid out in DeepActiNet paper - using a 1d CNN to extract features from acceleration and in this case BVP as well, this is done with a series of convolutions with very large kernels and no padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd029142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFFeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "     input_channels,\n",
    "     output_channels,\n",
    "     dropout=0.1):\n",
    "        super(HFFeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=output_channels, kernel_size=512, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=output_channels, out_channels=output_channels, kernel_size=256, stride=2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=output_channels, out_channels=output_channels, kernel_size=256, stride=2)\n",
    "        self.conv4 = nn.Conv1d(in_channels=output_channels, out_channels=output_channels, kernel_size=32, stride=2)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(output_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(output_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(output_channels)\n",
    "        self.bn4 = nn.BatchNorm1d(output_channels)\n",
    "\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert not torch.isnan(x).any(), \"NaN detected in CNN input\"\n",
    "        # Expect x of shape (batch, epoch_samples, channels)\n",
    "        x = x.permute(0, 2, 1)  # (batch, channels (1), epoch_samples)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff37f4",
   "metadata": {},
   "source": [
    "### Convolution Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31019482",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvSleepStager(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 hf_input_channels=5,\n",
    "                 lf_input_channels=5,\n",
    "                 cnn_output_channels=16,\n",
    "                 dropout=0.1,\n",
    "                 num_sleep_stages=5,\n",
    "                 learning_rate=1e-3,\n",
    "                 weight_decay=1e-5,\n",
    "                 label_smoothing=0.0,\n",
    "                 weight_tensor=None,\n",
    "                 convnet='CNN',\n",
    "                 num_tcn_blocks=5,\n",
    "                 tcn_kernel_size=3,\n",
    "                 debug=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        if convnet == 'CNN':\n",
    "            self.cnn = HFFeatureExtractorCNN(input_channels=hf_input_channels + lf_input_channels,\n",
    "                                        output_channels=cnn_output_channels,\n",
    "                                        dropout=dropout)\n",
    "        elif convnet == 'TCN':\n",
    "            self.cnn = HFFeatureExtractorTCN(in_channels=hf_input_channels + lf_input_channels,\n",
    "                                            out_channels=cnn_output_channels,\n",
    "                                            num_blocks=num_tcn_blocks,\n",
    "                                            kernel_size=tcn_kernel_size,\n",
    "                                            base_channels=cnn_output_channels,\n",
    "                                            final_down=64,     # match CNN’s total downsample factor\n",
    "                                            dropout=dropout)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown convnet type: {convnet}\")\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.kappa = MulticlassCohenKappa(num_classes=num_sleep_stages)\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.classifier = nn.Linear(cnn_output_channels, num_sleep_stages)\n",
    "\n",
    "        if weight_tensor is not None:\n",
    "            assert weight_tensor.shape[0] == num_sleep_stages, \\\n",
    "                f\"Weight tensor shape {weight_tensor.shape[0]} does not match number of sleep stages {num_sleep_stages}\"\n",
    "        self.train_criterion = nn.CrossEntropyLoss(weight=weight_tensor, ignore_index=-1,label_smoothing=label_smoothing)\n",
    "        self.val_criterion = nn.CrossEntropyLoss(weight=weight_tensor, ignore_index=-1)\n",
    "\n",
    "    def forward(self, hf, lf):\n",
    "        # assert no nan values in input\n",
    "        assert not torch.isnan(hf).any(), \"NaN detected in CNN input\"\n",
    "        assert not torch.isnan(lf).any(), \"NaN detected in LSTM input\"\n",
    "        if self.debug:\n",
    "            print(f\"HF input shape: {hf.shape}\")\n",
    "            print(f\"LF input shape: {lf.shape}\")\n",
    "        \n",
    "        # upsample shorter sequence\n",
    "        hf_output_length = hf.shape[1]\n",
    "        lf_output_length = lf.shape[1]\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] hf output length {hf_output_length} < lf output length {lf_output_length}, upsampling\")\n",
    "        lf = F.interpolate(\n",
    "                lf.permute(0, 2, 1), # (batch_size, hf_input_channels, sequence_length)\n",
    "                size=hf_output_length,\n",
    "            )\n",
    "        hf = hf.permute(0, 2, 1) # (batch_size, sequence_length, hf_input_channels)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] hf features shape: {hf.shape}\")\n",
    "            print(f\"[DEBUG] lf features shape: {lf.shape}\")\n",
    "        \n",
    "        # concatenate high and low frequency features\n",
    "        x = torch.cat((hf, lf), dim=1)\n",
    "        x = x.permute(0, 2, 1) # (batch_size, sequence_length, hf_input_channels + lf_input_channels)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] conv input shape: {x.shape}\")\n",
    "\n",
    "        # pass through cnn\n",
    "        x = self.cnn(x)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] conv output shape: {x.shape}\")\n",
    "\n",
    "        # downsample to match original sequence length\n",
    "        x = F.interpolate(\n",
    "            x,\n",
    "            size=lf_output_length,\n",
    "        )\n",
    "\n",
    "        # pass through classifier\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.classifier(x)\n",
    "        x = x.permute(1, 0, 2) # (batch_size, num_classes, sequence_length)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] classifier output shape: {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        hf, lf, labels = batch\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] training step batch {batch_idx}\")\n",
    "            print(f\"[DEBUG] hf shape: {hf.shape}\")\n",
    "            print(f\"[DEBUG] lf shape: {lf.shape}\")\n",
    "            print(f\"[DEBUG] labels shape: {labels.shape}\")\n",
    "        \n",
    "        logits = self(hf, lf)\n",
    "        logits = logits.permute(1, 0, 2) # should be (batch_size, seq_len, num_classes)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] logits shape after permute: {logits.shape}\")\n",
    "\n",
    "        # flatten\n",
    "        batch_size, seq_len, num_classes = logits.shape\n",
    "        logits_flat = logits.reshape(batch_size * seq_len, num_classes)\n",
    "        labels_flat = labels.reshape(batch_size * seq_len)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = self.train_criterion(logits_flat, labels_flat)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] loss: {loss.item()}\")\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        hf, lf, labels = batch\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] validation step batch {batch_idx}\")\n",
    "            print(f\"[DEBUG] hf shape: {hf.shape}\")\n",
    "            print(f\"[DEBUG] lf shape: {lf.shape}\")\n",
    "            print(f\"[DEBUG] labels shape: {labels.shape}\")\n",
    "\n",
    "        logits = self(hf, lf)\n",
    "        logits = logits.permute(1, 0, 2)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] logits shape after permute: {logits.shape}\")\n",
    "        # flatten\n",
    "        batch_size, seq_len, num_classes = logits.shape\n",
    "        logits_flat = logits.reshape(batch_size * seq_len, num_classes)\n",
    "        labels_flat = labels.reshape(batch_size * seq_len)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] logits_flat shape: {logits_flat.shape}\")\n",
    "            print(f\"[DEBUG] labels_flat shape: {labels_flat.shape}\")\n",
    "        # calculate loss\n",
    "        loss = self.val_criterion(logits_flat, labels_flat)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] validation loss: {loss.item()}\")\n",
    "        # calculate accuracy\n",
    "        preds = torch.argmax(logits_flat, dim=1)\n",
    "        mask = labels_flat != -1\n",
    "        masked_preds = preds[mask]\n",
    "        masked_labels = labels_flat[mask]\n",
    "        if masked_labels.numel() > 0:\n",
    "            acc = (masked_preds == masked_labels).float().mean().item()\n",
    "        else:\n",
    "            acc = 0.0\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] validation accuracy: {acc}\")\n",
    "        # calculate kappa\n",
    "        kappa = self.kappa.update(masked_preds, masked_labels)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] validation kappa: {kappa}\")\n",
    "\n",
    "        # log metrics\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "        return {\n",
    "            'val_loss': loss,\n",
    "            'val_acc': acc\n",
    "        }\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        kappa = self.kappa.compute()\n",
    "        self.log('val_cohen_kappa', torch.nan_to_num(kappa,0.0), prog_bar=True)\n",
    "        self.kappa.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619b678a",
   "metadata": {},
   "source": [
    "### Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70393b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualFreqSleepStager(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 hf_input_channels=5,\n",
    "                 lf_input_channels=5,\n",
    "                 cnn_output_channels=16,\n",
    "                 lstm_hidden_size=64,\n",
    "                 lstm_num_layers=2,\n",
    "                 lstm_bidirectional=True,\n",
    "                 dropout=0.1,\n",
    "                 num_sleep_stages=5,\n",
    "                 learning_rate=1e-3,\n",
    "                 weight_decay=1e-5,\n",
    "                 label_smoothing=0.0,\n",
    "                 weight_tensor=None,\n",
    "                 convnet='CNN',\n",
    "                 num_tcn_blocks=5,\n",
    "                 tcn_kernel_size=3,\n",
    "                 debug=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        if convnet == 'CNN':\n",
    "            self.cnn = HFFeatureExtractorCNN(input_channels=hf_input_channels,\n",
    "                                        output_channels=cnn_output_channels,\n",
    "                                        dropout=dropout)\n",
    "        elif convnet == 'TCN':\n",
    "            self.cnn = HFFeatureExtractorTCN(in_channels=hf_input_channels,\n",
    "                                            out_channels=cnn_output_channels,\n",
    "                                            num_blocks=num_tcn_blocks,\n",
    "                                            kernel_size=tcn_kernel_size,\n",
    "                                            base_channels=cnn_output_channels,\n",
    "                                            final_down=64,     # match CNN’s total downsample factor\n",
    "                                            dropout=dropout)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown convnet type: {convnet}\")\n",
    "        self.lstm = nn.LSTM(\n",
    "                            input_size=cnn_output_channels + lf_input_channels,\n",
    "                            hidden_size=lstm_hidden_size,\n",
    "                            num_layers=lstm_num_layers,\n",
    "                            bidirectional=lstm_bidirectional,\n",
    "                            dropout=dropout,\n",
    "                            batch_first=False)\n",
    "        \n",
    "        if lstm_bidirectional:\n",
    "            self.classifier = nn.Linear(lstm_hidden_size * 2, num_sleep_stages, )\n",
    "        else:\n",
    "            self.classifier = nn.Linear(lstm_hidden_size, num_sleep_stages)\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.kappa = MulticlassCohenKappa(num_classes=num_sleep_stages)\n",
    "        self.debug = debug\n",
    "        \n",
    "\n",
    "        if weight_tensor is not None:\n",
    "            assert weight_tensor.shape[0] == num_sleep_stages, \\\n",
    "                f\"Weight tensor shape {weight_tensor.shape[0]} does not match number of sleep stages {num_sleep_stages}\"\n",
    "        self.train_criterion = nn.CrossEntropyLoss(weight=weight_tensor, ignore_index=-1,label_smoothing=label_smoothing)\n",
    "        self.val_criterion = nn.CrossEntropyLoss(weight=weight_tensor, ignore_index=-1)\n",
    "\n",
    "    def forward(self, hf, lf):\n",
    "        # assert no nan values in input\n",
    "        assert not torch.isnan(hf).any(), \"NaN detected in CNN input\"\n",
    "        assert not torch.isnan(lf).any(), \"NaN detected in LSTM input\"\n",
    "        if self.debug:\n",
    "            print(f\"HF input shape: {hf.shape}\")\n",
    "            print(f\"LF input shape: {lf.shape}\")\n",
    "        \n",
    "        # pass high frequency data through CNN    \n",
    "        cnn_features = self.cnn(hf)\n",
    "        if self.debug:\n",
    "            print(f\"cnn output shape: {cnn_features.shape}\")\n",
    "\n",
    "        # downsample longer sequence\n",
    "        cnn_output_length = cnn_features.shape[2]\n",
    "        lf_output_length = lf.shape[1]\n",
    "        if cnn_output_length > lf_output_length:\n",
    "            if self.debug:\n",
    "                print(f\"[DEBUG] cnn output length {cnn_output_length} > lf output length {lf_output_length}, downsampling\")\n",
    "            cnn_features = F.interpolate(\n",
    "                cnn_features,\n",
    "                size=lf_output_length,\n",
    "            )\n",
    "        elif cnn_output_length < lf_output_length:\n",
    "            if self.debug:\n",
    "                print(f\"[DEBUG] cnn output length {cnn_output_length} < lf output length {lf_output_length}, downsampling\")\n",
    "            lf = F.interpolate(\n",
    "                lf,\n",
    "                size=cnn_output_length,\n",
    "            )\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] hf features shape: {cnn_features.shape}\")\n",
    "            print(f\"[DEBUG] lf features shape: {lf.shape}\")\n",
    "        \n",
    "        # concatenate high and low frequency features\n",
    "        a = cnn_features.permute(2,0,1) # (sequence_length, batch_size, cnn_output_channels)\n",
    "        b = lf.permute(1,0,2) # (sequence_length, batch_size, lf_input_channels)\n",
    "        x = torch.cat((a, b), dim=2)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] lstm input shape: {x.shape}\")\n",
    "        \n",
    "        # pass through LSTM + classifier\n",
    "        x, _ = self.lstm(x)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] lstm output shape: {x.shape}\")\n",
    "        x = self.classifier(x)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] classifier output shape: {x.shape}\")\n",
    "        return x\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        hf, lf, labels = batch\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] training step batch {batch_idx}\")\n",
    "            print(f\"[DEBUG] hf shape: {hf.shape}\")\n",
    "            print(f\"[DEBUG] lf shape: {lf.shape}\")\n",
    "            print(f\"[DEBUG] labels shape: {labels.shape}\")\n",
    "        \n",
    "        logits = self(hf, lf)\n",
    "        logits = logits.permute(1, 0, 2) # should be (batch_size, seq_len, num_classes)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] logits shape after permute: {logits.shape}\")\n",
    "\n",
    "        # flatten\n",
    "        batch_size, seq_len, num_classes = logits.shape\n",
    "        logits_flat = logits.reshape(batch_size * seq_len, num_classes)\n",
    "        labels_flat = labels.reshape(batch_size * seq_len)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = self.train_criterion(logits_flat, labels_flat)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] loss: {loss.item()}\")\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        hf, lf, labels = batch\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] validation step batch {batch_idx}\")\n",
    "            print(f\"[DEBUG] hf shape: {hf.shape}\")\n",
    "            print(f\"[DEBUG] lf shape: {lf.shape}\")\n",
    "            print(f\"[DEBUG] labels shape: {labels.shape}\")\n",
    "\n",
    "        logits = self(hf, lf)\n",
    "        logits = logits.permute(1, 0, 2)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] logits shape after permute: {logits.shape}\")\n",
    "        # flatten\n",
    "        batch_size, seq_len, num_classes = logits.shape\n",
    "        logits_flat = logits.reshape(batch_size * seq_len, num_classes)\n",
    "        labels_flat = labels.reshape(batch_size * seq_len)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] logits_flat shape: {logits_flat.shape}\")\n",
    "            print(f\"[DEBUG] labels_flat shape: {labels_flat.shape}\")\n",
    "        # calculate loss\n",
    "        loss = self.val_criterion(logits_flat, labels_flat)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] validation loss: {loss.item()}\")\n",
    "        # calculate accuracy\n",
    "        preds = torch.argmax(logits_flat, dim=1)\n",
    "        mask = labels_flat != -1\n",
    "        masked_preds = preds[mask]\n",
    "        masked_labels = labels_flat[mask]\n",
    "        if masked_labels.numel() > 0:\n",
    "            acc = (masked_preds == masked_labels).float().mean().item()\n",
    "        else:\n",
    "            acc = 0.0\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] validation accuracy: {acc}\")\n",
    "        # calculate kappa\n",
    "        kappa = self.kappa.update(masked_preds, masked_labels)\n",
    "        if self.debug:\n",
    "            print(f\"[DEBUG] validation kappa: {kappa}\")\n",
    "\n",
    "        # log metrics\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "        return {\n",
    "            'val_loss': loss,\n",
    "            'val_acc': acc\n",
    "        }\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        kappa = self.kappa.compute()\n",
    "        self.log('val_cohen_kappa', torch.nan_to_num(kappa,0.0), prog_bar=True)\n",
    "        self.kappa.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        '''\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss',\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "        '''\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ea401",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    # figure out where the model lives (cpu or cuda)\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds  = []\n",
    "    all_probs  = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for non_acc, acc, labels in dataloader:\n",
    "            # move inputs to model's device\n",
    "            non_acc = non_acc.to(device)\n",
    "            acc     = acc.to(device)\n",
    "\n",
    "            # forward\n",
    "            y_hat = model(non_acc, acc)           # (seq_len, batch, C)\n",
    "            y_hat = y_hat.permute(1, 0, 2)        # (batch, seq_len, C)\n",
    "            B, T, C = y_hat.shape\n",
    "\n",
    "            # flatten\n",
    "            logits = y_hat.reshape(B*T, C)\n",
    "            probs  = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "            preds  = probs.argmax(axis=1)\n",
    "            labs   = labels.reshape(-1).numpy()\n",
    "\n",
    "            # filter out padding (-1)\n",
    "            mask = (labs != -1)\n",
    "            all_labels.extend(labs[mask])\n",
    "            all_preds.extend(preds[mask])\n",
    "            all_probs.append(probs[mask])\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    acc   = accuracy_score(all_labels, all_preds)\n",
    "    kappa = cohen_kappa_score(all_labels, all_preds)\n",
    "    auroc = roc_auc_score(all_labels, all_probs, multi_class='ovo', average='macro')\n",
    "    cm    = confusion_matrix(all_labels, all_preds,normalize='true')\n",
    "    return acc, kappa, auroc, cm\n",
    "\n",
    "def plot_confusion_matrix(cm: np.ndarray, classes: list[str]):\n",
    "    \"\"\"\n",
    "    cm:          square confusion matrix of counts or floats\n",
    "    classes:     list of class‐label strings, length == cm.shape[0]\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm)           # default colormap, channels-last format\n",
    "    fig.colorbar(im, ax=ax)      # add a colorbar\n",
    "\n",
    "    # Tick labels\n",
    "    ax.set_xticks(np.arange(len(classes)))\n",
    "    ax.set_yticks(np.arange(len(classes)))\n",
    "    ax.set_xticklabels(classes, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(classes)\n",
    "\n",
    "    # Annotate each cell with its value\n",
    "    for (i, j), val in np.ndenumerate(cm):\n",
    "        ax.text(j, i, f\"{val:.4f}\", ha='center', va='center')\n",
    "\n",
    "    ax.set_xlabel(\"Predicted label\")\n",
    "    ax.set_ylabel(\"True label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix_with_metrics(cm, class_mapping):\n",
    "    class_vals = [i for i in class_mapping.keys()]\n",
    "    class_names = [class_mapping[i] for i in range(len(class_vals))]\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100  # row-wise percentage (sensitivity)\n",
    "\n",
    "    # Calculate Sensitivity (Recall) and PPV (Precision)\n",
    "    sensitivity = np.diag(cm) / np.sum(cm, axis=1)\n",
    "    ppv = np.diag(cm) / np.sum(cm, axis=0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=False, fmt='g', cmap='Blues', cbar=False, ax=ax)\n",
    "\n",
    "    # Labels inside the boxes\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            count = cm[i, j]\n",
    "            perc = cm_perc[i, j]\n",
    "            ax.text(j + 0.5, i + 0.3, f\"{count:.4f}\", \n",
    "                    ha='center', va='center', color='black', fontsize=10)\n",
    "            ax.text(j + 0.5, i + 0.7, f\"{perc:.0f}%\", \n",
    "                    ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('Prediction', fontsize=12)\n",
    "    ax.set_ylabel('Reference', fontsize=12)\n",
    "\n",
    "    # Set ticks\n",
    "    ax.set_xticks(np.arange(len(class_vals)) + 0.5)\n",
    "    ax.set_yticks(np.arange(len(class_vals)) + 0.5)\n",
    "\n",
    "    ax.set_xticklabels(class_names, rotation=0, ha=\"center\")\n",
    "    ax.set_yticklabels(class_names, rotation=0)\n",
    "\n",
    "    # Plot PPV (precision) on top\n",
    "    for j, p in enumerate(ppv):\n",
    "        ax.text(j + 0.5, -0.2, f\"{int(p*100) if not np.isnan(p) else 0}%\", \n",
    "                ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "    # Plot Sensitivity (recall) on right\n",
    "    for i, s in enumerate(sensitivity):\n",
    "        ax.text(len(class_names) + 0.1, i + 0.5, f\"{int(s*100) if not np.isnan(s) else 0}%\", \n",
    "                ha='left', va='center', color='black', fontsize=10)\n",
    "\n",
    "    # Center \"PPV\" label on top\n",
    "    ax.text(len(class_vals) / 2, -0.5, \"PPV\", fontsize=14, ha='center', va='center')\n",
    "\n",
    "    # Center \"Sensitivity\" label on the right\n",
    "    ax.text(len(class_vals) + 0.5, len(class_vals) / 2, \"Sensitivity\", fontsize=14, ha='center', va='center', rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5286811",
   "metadata": {},
   "source": [
    "# Dataset creation / loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a7387c",
   "metadata": {},
   "source": [
    "### Separate subjects into train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d632ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_64Hz = '/gpfs/data/oermannlab/users/slj9342/dl4med_25/data/physionet.org/files/dreamt/2.0.0/data_64Hz/' # working with 64Hz data\n",
    "max_length = 2493810 # found experimentally, takes a while to compute\n",
    "\n",
    "participant_info_df = pd.read_csv('/gpfs/data/oermannlab/users/slj9342/dl4med_25/data/physionet.org/files/dreamt/2.0.0/participant_info.csv')\n",
    "subjects_all = participant_info_df['SID']\n",
    "\n",
    "subjects_all_shuffled = participant_info_df['SID'].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "subjects_train = subjects_all_shuffled[:int(len(subjects_all_shuffled)*0.8)]\n",
    "subjects_val = subjects_all_shuffled[int(len(subjects_all_shuffled)*0.8):int(len(subjects_all_shuffled)*0.9)]\n",
    "subjects_test = subjects_all_shuffled[int(len(subjects_all_shuffled)*0.9):]\n",
    "print(f\"number of subjects in train: {len(subjects_train)}\")\n",
    "print(f\"number of subjects in val: {len(subjects_val)}\")\n",
    "print(f\"number of subjects in test: {len(subjects_test)}\")\n",
    "\n",
    "fraction = 0.3\n",
    "subjects_train_small = subjects_train[:int(len(subjects_train)*fraction)]\n",
    "subjects_val_small = subjects_val[:int(len(subjects_val)*fraction)]\n",
    "subjects_test_small = subjects_test[:int(len(subjects_test)*fraction)]\n",
    "print(f\"number of subjects in small train: {len(subjects_train_small)}\")\n",
    "print(f\"number of subjects in small val: {len(subjects_val_small)}\")\n",
    "print(f\"number of subjects in small test: {len(subjects_test_small)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62a9d4",
   "metadata": {},
   "source": [
    "### Construct train, val, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3e09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_features = ['BVP','ACC']\n",
    "lf_features = ['TIMESTAMP','TEMP','EDA','HR','IBI']\n",
    "hf_freq = 32\n",
    "lf_freq = 0.2\n",
    "chunk_duration = 6000 # 6000s = 100 minutes\n",
    "chunk_stride = 3000 # 3000s = 50 minutes\n",
    "train_dataset = DualFreqDataset(subjects_list=subjects_train,\n",
    "                                data_dir=datadir_64Hz,\n",
    "                                chunk_duration=chunk_duration,\n",
    "                                chunk_stride=chunk_stride,\n",
    "                                high_freq=hf_freq,\n",
    "                                low_freq=lf_freq,\n",
    "                                hf_features=hf_features,\n",
    "                                lf_features=lf_features)\n",
    "print(f\"number of chunks in train: {len(train_dataset)}\")\n",
    "val_dataset = DualFreqDataset(subjects_list=subjects_val,\n",
    "                              data_dir=datadir_64Hz,\n",
    "                              chunk_duration=chunk_duration,\n",
    "                              chunk_stride=chunk_stride,\n",
    "                              high_freq=hf_freq,\n",
    "                              low_freq=lf_freq,\n",
    "                              hf_features=hf_features,\n",
    "                              lf_features=lf_features)\n",
    "print(f\"number of chunks in val: {len(val_dataset)}\")\n",
    "test_dataset = DualFreqDataset(subjects_list=subjects_test,\n",
    "                               data_dir=datadir_64Hz,\n",
    "                               chunk_duration=chunk_duration,\n",
    "                               chunk_stride=chunk_stride,\n",
    "                               high_freq=hf_freq,\n",
    "                               low_freq=lf_freq,\n",
    "                               hf_features=hf_features,\n",
    "                               lf_features=lf_features)\n",
    "print(f\"number of chunks in test: {len(test_dataset)}\")\n",
    "train_dataset_small = DualFreqDataset(subjects_list=subjects_train_small,\n",
    "                                       data_dir=datadir_64Hz,\n",
    "                                       chunk_duration=chunk_duration,\n",
    "                                       chunk_stride=chunk_stride,\n",
    "                                       high_freq=hf_freq,\n",
    "                                       low_freq=lf_freq,\n",
    "                                       hf_features=hf_features,\n",
    "                                       lf_features=lf_features)\n",
    "print(f\"number of chunks in small train: {len(train_dataset_small)}\")\n",
    "val_dataset_small = DualFreqDataset(subjects_list=subjects_val_small,\n",
    "                                     data_dir=datadir_64Hz,\n",
    "                                     chunk_duration=chunk_duration,\n",
    "                                     chunk_stride=chunk_stride,\n",
    "                                     high_freq=hf_freq,\n",
    "                                     low_freq=lf_freq,\n",
    "                                     hf_features=hf_features,\n",
    "                                     lf_features=lf_features)\n",
    "print(f\"number of chunks in small val: {len(val_dataset_small)}\")\n",
    "test_dataset_small = DualFreqDataset(subjects_list=subjects_test_small,\n",
    "                                      data_dir=datadir_64Hz,\n",
    "                                      chunk_duration=chunk_duration,\n",
    "                                      chunk_stride=chunk_stride,\n",
    "                                      high_freq=hf_freq,\n",
    "                                      low_freq=lf_freq,\n",
    "                                      hf_features=hf_features,\n",
    "                                      lf_features=lf_features)\n",
    "print(f\"number of chunks in small test: {len(test_dataset_small)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ec509",
   "metadata": {},
   "source": [
    "### Save Dataset Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceba77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset chunks\n",
    "torch.save(train_dataset.chunks, 'DualFreqDatasets/train_dataset_chunks.pt')\n",
    "torch.save(val_dataset.chunks, 'DualFreqDatasets/val_dataset_chunks.pt')\n",
    "torch.save(test_dataset.chunks, 'DualFreqDatasets/test_dataset_chunks.pt')\n",
    "torch.save(train_dataset_small.chunks, 'DualFreqDatasets/train_dataset_small_chunks.pt')\n",
    "torch.save(val_dataset_small.chunks, 'DualFreqDatasets/val_dataset_small_chunks.pt')\n",
    "torch.save(test_dataset_small.chunks, 'DualFreqDatasets/test_dataset_small_chunks.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e4c60",
   "metadata": {},
   "source": [
    "### Load Saved Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f596d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved datasets\n",
    "hf_features = ['BVP','ACC']\n",
    "lf_features = ['TIMESTAMP','TEMP','EDA','HR','IBI']\n",
    "hf_freq = 32\n",
    "lf_freq = 0.2\n",
    "chunk_duration = 6000 # 10 minutes\n",
    "chunk_stride = 3000 # 2.5 minutes\n",
    "train_dataset = DualFreqDataset(subjects_list=[],\n",
    "                                data_dir=None,\n",
    "                                chunk_duration=chunk_duration,\n",
    "                                chunk_stride=chunk_stride,\n",
    "                                high_freq=hf_freq,\n",
    "                                low_freq=lf_freq,\n",
    "                                hf_features=hf_features,\n",
    "                                lf_features=lf_features)\n",
    "train_dataset.chunks = torch.load('DualFreqDatasets/train_dataset_chunks.pt')\n",
    "val_dataset = DualFreqDataset(subjects_list=[],\n",
    "                              data_dir=None,\n",
    "                              chunk_duration=chunk_duration,\n",
    "                              chunk_stride=chunk_stride,\n",
    "                              high_freq=hf_freq,\n",
    "                              low_freq=lf_freq,\n",
    "                              hf_features=hf_features,\n",
    "                              lf_features=lf_features)\n",
    "val_dataset.chunks = torch.load('DualFreqDatasets/val_dataset_chunks.pt')\n",
    "test_dataset = DualFreqDataset(subjects_list=[],\n",
    "                               data_dir=None,\n",
    "                               chunk_duration=chunk_duration,\n",
    "                               chunk_stride=chunk_stride,\n",
    "                               high_freq=hf_freq,\n",
    "                               low_freq=lf_freq,\n",
    "                               hf_features=hf_features,\n",
    "                               lf_features=lf_features)\n",
    "test_dataset.chunks = torch.load('DualFreqDatasets/test_dataset_chunks.pt')\n",
    "train_dataset_small = DualFreqDataset(subjects_list=[],\n",
    "                                       data_dir=None,\n",
    "                                       chunk_duration=chunk_duration,\n",
    "                                       chunk_stride=chunk_stride,\n",
    "                                       high_freq=hf_freq,\n",
    "                                       low_freq=lf_freq,\n",
    "                                       hf_features=hf_features,\n",
    "                                       lf_features=lf_features)\n",
    "train_dataset_small.chunks = torch.load('DualFreqDatasets/train_dataset_small_chunks.pt')\n",
    "val_dataset_small = DualFreqDataset(subjects_list=[],\n",
    "                                     data_dir=None,\n",
    "                                     chunk_duration=chunk_duration,\n",
    "                                     chunk_stride=chunk_stride,\n",
    "                                     high_freq=hf_freq,\n",
    "                                     low_freq=lf_freq,\n",
    "                                     hf_features=hf_features,\n",
    "                                     lf_features=lf_features)\n",
    "val_dataset_small.chunks = torch.load('DualFreqDatasets/val_dataset_small_chunks.pt')\n",
    "test_dataset_small = DualFreqDataset(subjects_list=[],\n",
    "                                      data_dir=None,\n",
    "                                      chunk_duration=chunk_duration,\n",
    "                                      chunk_stride=chunk_stride,\n",
    "                                      high_freq=hf_freq,\n",
    "                                      low_freq=lf_freq,\n",
    "                                      hf_features=hf_features,\n",
    "                                      lf_features=lf_features)\n",
    "test_dataset_small.chunks = torch.load('DualFreqDatasets/test_dataset_small_chunks.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149adfa",
   "metadata": {},
   "source": [
    "# Model Demos (shape compatibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_hf, temp_lf, temp_labels = train_dataset[0]\n",
    "print(f\"temp_hf shape: {temp_hf.shape}\")\n",
    "print(f\"temp_lf shape: {temp_lf.shape}\")\n",
    "print(f\"temp_labels shape: {temp_labels.shape}\")\n",
    "temp_hf = temp_hf.unsqueeze(0)\n",
    "temp_lf = temp_lf.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4259a6ea",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e253126",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMSleepStager(\n",
    "    hf_input_channels=len(hf_features),\n",
    "    lf_input_channels=len(lf_features),\n",
    "    lstm_hidden_size=64,\n",
    "    lstm_num_layers=2,\n",
    "    lstm_bidirectional=True,\n",
    "    dropout=0.1,\n",
    "    num_sleep_stages=4,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    label_smoothing=0.0,\n",
    "    weight_tensor=None,\n",
    "    debug=True\n",
    ")\n",
    "output = model(temp_hf, temp_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee45c8",
   "metadata": {},
   "source": [
    "### TCN Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35798002",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvSleepStager(\n",
    "    hf_input_channels=len(hf_features),\n",
    "    lf_input_channels=len(lf_features),\n",
    "    cnn_output_channels=16,\n",
    "    dropout=0.1,\n",
    "    num_sleep_stages=4,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=5e-5,\n",
    "    weight_tensor=None,\n",
    "    convnet='TCN',\n",
    "    debug=True\n",
    ")\n",
    "output = model(temp_hf, temp_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a56e3",
   "metadata": {},
   "source": [
    "### CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_hf, temp_lf, temp_labels = train_dataset[0]\n",
    "print(f\"temp_hf shape: {temp_hf.shape}\")\n",
    "print(f\"temp_lf shape: {temp_lf.shape}\")\n",
    "print(f\"temp_labels shape: {temp_labels.shape}\")\n",
    "temp_hf = temp_hf.unsqueeze(0)\n",
    "temp_lf = temp_lf.unsqueeze(0)\n",
    "model = DualFreqSleepStager(\n",
    "    hf_input_channels=len(hf_features),\n",
    "    lf_input_channels=len(lf_features),\n",
    "    cnn_output_channels=16,\n",
    "    lstm_hidden_size=64,\n",
    "    lstm_num_layers=2,\n",
    "    lstm_bidirectional=True,\n",
    "    dropout=0.1,\n",
    "    num_sleep_stages=4,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=5e-5,\n",
    "    weight_tensor=None,\n",
    "    convnet='CNN',\n",
    "    debug=True\n",
    ")\n",
    "output = model(temp_hf, temp_lf)\n",
    "print(f\"output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037fbdfa",
   "metadata": {},
   "source": [
    "### TCN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DualFreqSleepStager(\n",
    "    hf_input_channels=len(hf_features),\n",
    "    lf_input_channels=len(lf_features),\n",
    "    cnn_output_channels=16,\n",
    "    lstm_hidden_size=64,\n",
    "    lstm_num_layers=2,\n",
    "    lstm_bidirectional=True,\n",
    "    dropout=0.1,\n",
    "    num_sleep_stages=4,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=5e-5,\n",
    "    weight_tensor=None,\n",
    "    convnet='TCN',\n",
    "    num_tcn_blocks=5,\n",
    "    tcn_kernel_size=3,\n",
    "    debug=True\n",
    ")\n",
    "output = model(temp_hf, temp_lf)\n",
    "print(f\"output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f4c12b",
   "metadata": {},
   "source": [
    "# Get Class Weights\n",
    "We use these for weighted loss, this is to deal with the natural class imbalance in sleep staging data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get class weights for weighted loss\n",
    "all_labels = []\n",
    "for batch in DataLoader(train_dataset, batch_size=1):\n",
    "    labels = batch[2].numpy()\n",
    "    all_labels.extend(labels.flatten())\n",
    "all_labels = np.array(all_labels)\n",
    "valid_labels = all_labels[all_labels != -1]\n",
    "classes = np.arange(4)\n",
    "class_counts = Counter(valid_labels)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=valid_labels\n",
    ")\n",
    "print(f\"Class counts: {class_counts}\")\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef24370f",
   "metadata": {},
   "source": [
    "# LSTM Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d96fb26",
   "metadata": {},
   "source": [
    "## LSTM Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8198b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_optuna_trials = 100 # not enough to cover every combination, but should be enough to find a good one\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(lstm_objective, n_trials=num_optuna_trials)\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"Best trial: {best_trial.number}\")\n",
    "print(f\"Best trial value: {best_trial.value}\")\n",
    "print(f\"Best trial params: {best_trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb74de4",
   "metadata": {},
   "source": [
    "## Train LSTM only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38022d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lstm_hidden_size = best_trial.params['lstm_hidden_size']\n",
    "lstm_num_layers = best_trial.params['lstm_num_layers']\n",
    "dropout = best_trial.params['dropout']\n",
    "learning_rate = best_trial.params['learning_rate']\n",
    "weight_decay = best_trial.params['weight_decay']\n",
    "label_smoothing = best_trial.params['label_smoothing']\n",
    "'''\n",
    "\n",
    "lstm_hidden_size = 256\n",
    "lstm_num_layers = 2\n",
    "dropout = 0.21312\n",
    "learning_rate = 0.0048844\n",
    "weight_decay = 0.0000078942\n",
    "label_smoothing = 0.0050416\n",
    "\n",
    "\n",
    "num_runs = 10\n",
    "max_epochs = 50\n",
    "for runNo in range(num_runs):\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"LSTM-Sleep-Stager\",\n",
    "        name=f\"best-params-{runNo}\",\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint( # selected model hyperparams by best val loss, now selecting by best val kappa\n",
    "        monitor='val_cohen_kappa',\n",
    "        dirpath='checkpoints/LSTM/',\n",
    "        filename=f'best-checkpoint-{runNo}',\n",
    "        save_top_k=1,\n",
    "        mode='max'\n",
    "    )\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_cohen_kappa',\n",
    "        patience=10,\n",
    "        verbose=True,\n",
    "        mode='max'\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        devices=1,\n",
    "        accelerator='gpu',\n",
    "        logger=wandb_logger,\n",
    "        log_every_n_steps=1,\n",
    "        precision=\"16-mixed\",\n",
    "        #callbacks=[checkpoint_callback, early_stop_callback]\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "    model = LSTMSleepStager(\n",
    "        hf_input_channels=len(hf_features),\n",
    "        lf_input_channels=len(lf_features),\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_num_layers=lstm_num_layers,\n",
    "        lstm_bidirectional=True,\n",
    "        dropout=dropout,\n",
    "        num_sleep_stages=4,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing=label_smoothing,\n",
    "        weight_tensor=torch.tensor(class_weights, dtype=torch.float32),\n",
    "        debug=False\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    wandb.finish()\n",
    "    clear_output()\n",
    "    # load best model\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    best_model = LSTMSleepStager.load_from_checkpoint(best_model_path)\n",
    "    # save the model\n",
    "    torch.save(best_model.state_dict(), f'models/LSTM/best_model_{runNo}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7637985",
   "metadata": {},
   "source": [
    "## Test LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce329211",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 10\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "dropout = 0.21312\n",
    "label_smoothing = 0.0050416\n",
    "learning_rate = 0.0048844\n",
    "lstm_hidden_size = 256\n",
    "lstm_num_layers = 2\n",
    "weight_decay = 0.0000078942\n",
    "\n",
    "test_aurocs = []\n",
    "test_accuracies = []\n",
    "test_kappas = []\n",
    "test_confusions = []\n",
    "\n",
    "for runNo in range(num_runs):\n",
    "    model = LSTMSleepStager(\n",
    "        hf_input_channels=len(hf_features),\n",
    "        lf_input_channels=len(lf_features),\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_num_layers=lstm_num_layers,\n",
    "        lstm_bidirectional=True,\n",
    "        dropout=dropout,\n",
    "        num_sleep_stages=4,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing=label_smoothing,\n",
    "        weight_tensor=torch.tensor(class_weights, dtype=torch.float32),\n",
    "        debug=False\n",
    "    )\n",
    "    model.load_state_dict(torch.load(f'models/LSTM/best_model_{runNo}.pth'))\n",
    "\n",
    "    test_acc, test_kappa, test_auroc, test_cm = evaluate_model(model, test_loader)\n",
    "    test_aurocs.append(test_auroc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    test_kappas.append(test_kappa)\n",
    "    test_confusions.append(test_cm)\n",
    "\n",
    "# print results\n",
    "print(f\"Test AUROC: {np.mean(test_aurocs):.4f} +/- {np.std(test_aurocs):.4f}\")\n",
    "print(f\"Test Accuracy: {np.mean(test_accuracies):.4f} +/- {np.std(test_accuracies):.4f}\")\n",
    "print(f\"Test Kappa: {np.mean(test_kappas):.4f} +/- {np.std(test_kappas):.4f}\")\n",
    "\n",
    "# get mean confusion matrix\n",
    "tcn_mean_confusion = np.mean(test_confusions, axis=0)\n",
    "# display mean confusion matrix\n",
    "plot_confusion_matrix_with_metrics(test_confusions[-1], class_mapping={0:'W',1:'Light',2:'Deep',3:'REM'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2f5f67",
   "metadata": {},
   "source": [
    "# TCN Only Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ef141",
   "metadata": {},
   "source": [
    "## TCN Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce91a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 21:48:28,101] Trial 24 finished with value: 1.2368443012237549 and parameters: {'cnn_output_channels': 64, 'dropout': 0.19866418585334414, 'learning_rate': 0.00019700816849326218, 'weight_decay': 1.1606336549334584e-05, 'label_smoothing': 0.11783564587360429, 'num_tcn_blocks': 7, 'tcn_kernel_size': 5}. Best is trial 11 with value: 1.2068884372711182.\n",
      "/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4m ...\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/run-20250429_214828-omhztd4n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lakej98-nyu-langone-health/TCN-Sleep-Stager/runs/omhztd4n' target=\"_blank\">optuna-25</a></strong> to <a href='https://wandb.ai/lakej98-nyu-langone-health/TCN-Sleep-Stager' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lakej98-nyu-langone-health/TCN-Sleep-Stager' target=\"_blank\">https://wandb.ai/lakej98-nyu-langone-health/TCN-Sleep-Stager</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lakej98-nyu-langone-health/TCN-Sleep-Stager/runs/omhztd4n' target=\"_blank\">https://wandb.ai/lakej98-nyu-langone-health/TCN-Sleep-Stager/runs/omhztd4n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /gpfs/data/oermannlab/users/slj9342/dl4med_final_project/checkpoints/TCN/Optuna exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                  | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | cnn             | HFFeatureExtractorTCN | 1.3 M  | train\n",
      "1 | kappa           | MulticlassCohenKappa  | 0      | train\n",
      "2 | classifier      | Linear                | 516    | train\n",
      "3 | train_criterion | CrossEntropyLoss      | 0      | train\n",
      "4 | val_criterion   | CrossEntropyLoss      | 0      | train\n",
      "------------------------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.030     Total estimated model params size (MB)\n",
      "64        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.002886533737182617,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17cb113f870142659da4a6b2008ced4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.002801656723022461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fc373f944d425283c44f7857c9162a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_optuna_trials = 100 # not enough to cover every combination, but should be enough to find a good one\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(tcn_objective, n_trials=num_optuna_trials)\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"Best trial: {best_trial.number}\")\n",
    "print(f\"Best trial value: {best_trial.value}\")\n",
    "print(f\"Best trial params: {best_trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cbab3a",
   "metadata": {},
   "source": [
    "## Train TCN Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f29bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn_output_channels = 16\n",
    "dropout = 0.21312\n",
    "label_smoothing = 0.0050416\n",
    "learning_rate = 0.0048844\n",
    "num_tcn_blocks = 7\n",
    "tcn_kernel_size = 5\n",
    "weight_decay = 0.0000078942\n",
    "\n",
    "\n",
    "num_runs = 2\n",
    "max_epochs = 50\n",
    "for runNo in range(num_runs):\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"TCN-Sleep-Stager\",\n",
    "        name=f\"best-params-{runNo}\",\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint( # selected model hyperparams by best val loss, now selecting by best val kappa\n",
    "        monitor='val_cohen_kappa',\n",
    "        dirpath='checkpoints/TCN/',\n",
    "        filename='best-checkpoint',\n",
    "        save_top_k=1,\n",
    "        mode='max'\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        devices=1,\n",
    "        accelerator='gpu',\n",
    "        logger=wandb_logger,\n",
    "        log_every_n_steps=1,\n",
    "        precision=\"16-mixed\",\n",
    "        callbacks=[checkpoint_callback] # not using early stopping for now\n",
    "    )\n",
    "    model = ConvSleepStager(\n",
    "        hf_input_channels=len(hf_features),\n",
    "        lf_input_channels=len(lf_features),\n",
    "        cnn_output_channels=cnn_output_channels,\n",
    "        dropout=dropout,\n",
    "        num_sleep_stages=4,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing=label_smoothing,\n",
    "        weight_tensor=torch.tensor(class_weights, dtype=torch.float32),\n",
    "        convnet='TCN',\n",
    "        num_tcn_blocks=num_tcn_blocks,\n",
    "        tcn_kernel_size=tcn_kernel_size,\n",
    "        debug=False\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    wandb.finish()\n",
    "    clear_output()\n",
    "    # load best model\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    best_model = ConvSleepStager.load_from_checkpoint(best_model_path)\n",
    "    # save the model\n",
    "    torch.save(best_model.state_dict(), f'models/TCN/best_model_{runNo}.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b3ef7f",
   "metadata": {},
   "source": [
    "## Test TCN Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ac8902",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 2\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "cnn_output_channels = 16\n",
    "dropout = 0.21312\n",
    "label_smoothing = 0.0050416\n",
    "learning_rate = 0.0048844\n",
    "num_tcn_blocks = 7\n",
    "tcn_kernel_size = 5\n",
    "weight_decay = 0.0000078942\n",
    "\n",
    "test_aurocs = []\n",
    "test_accuracies = []\n",
    "test_kappas = []\n",
    "test_confusions = []\n",
    "\n",
    "for runNo in range(num_runs):\n",
    "    model = ConvSleepStager(\n",
    "        hf_input_channels=len(hf_features),\n",
    "        lf_input_channels=len(lf_features),\n",
    "        cnn_output_channels=cnn_output_channels,\n",
    "        dropout=dropout,\n",
    "        num_sleep_stages=4,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing=label_smoothing,\n",
    "        weight_tensor=torch.tensor(class_weights, dtype=torch.float32),\n",
    "        convnet='TCN',\n",
    "        num_tcn_blocks=num_tcn_blocks,\n",
    "        tcn_kernel_size=tcn_kernel_size,\n",
    "        debug=False\n",
    "    )\n",
    "    model.load_state_dict(torch.load(f'models/TCN/best_model_{runNo}.pth'))\n",
    "\n",
    "    test_acc, test_kappa, test_auroc, test_cm = evaluate_model(model, test_loader)\n",
    "    test_aurocs.append(test_auroc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    test_kappas.append(test_kappa)\n",
    "    test_confusions.append(test_cm)\n",
    "\n",
    "# print results\n",
    "print(f\"Test AUROC: {np.mean(test_aurocs):.4f} +/- {np.std(test_aurocs):.4f}\")\n",
    "print(f\"Test Accuracy: {np.mean(test_accuracies):.4f} +/- {np.std(test_accuracies):.4f}\")\n",
    "print(f\"Test Kappa: {np.mean(test_kappas):.4f} +/- {np.std(test_kappas):.4f}\")\n",
    "\n",
    "# display mean confusion matrix\n",
    "plot_confusion_matrix_with_metrics(test_confusions[-1], class_mapping={0:'W',1:'Light',2:'Deep',3:'REM'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572e156b",
   "metadata": {},
   "source": [
    "# TCN-LSTM Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c533efb",
   "metadata": {},
   "source": [
    "## TCN-LSTM Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcec3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_optuna_trials = 100 # not enough to cover every combination, but should be enough to find a good one\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(tcn_lstm_objective, n_trials=num_optuna_trials)\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"Best trial: {best_trial.number}\")\n",
    "print(f\"Best trial value: {best_trial.value}\")\n",
    "print(f\"Best trial params: {best_trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7f98ed",
   "metadata": {},
   "source": [
    "## Train TCN-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf1e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn_output_channels = 16\n",
    "dropout = 0.21312\n",
    "label_smoothing = 0.0050416\n",
    "learning_rate = 0.0048844\n",
    "lstm_hidden_size = 256\n",
    "lstm_num_layers = 2\n",
    "num_tcn_blocks = 7\n",
    "tcn_kernel_size = 5\n",
    "weight_decay = 0.0000078942\n",
    "\n",
    "\n",
    "num_runs = 10\n",
    "max_epochs = 50\n",
    "for runNo in range(num_runs):\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"TCN-LSTM-Sleep-Stager\",\n",
    "        name=f\"best-params-{runNo}\",\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint( # selected model hyperparams by best val loss, now selecting by best val kappa\n",
    "        monitor='val_cohen_kappa',\n",
    "        dirpath='checkpoints/TCN-LSTM/',\n",
    "        filename='best-checkpoint',\n",
    "        save_top_k=1,\n",
    "        mode='max'\n",
    "    )\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_cohen_kappa',\n",
    "        patience=10,\n",
    "        verbose=True,\n",
    "        mode='max'\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        devices=1,\n",
    "        accelerator='gpu',\n",
    "        logger=wandb_logger,\n",
    "        log_every_n_steps=1,\n",
    "        precision=\"16-mixed\",\n",
    "        #callbacks=[checkpoint_callback, early_stop_callback]\n",
    "        callbacks=[checkpoint_callback] # not using early stopping for now\n",
    "    )\n",
    "    model = DualFreqSleepStager(\n",
    "        hf_input_channels=len(hf_features),\n",
    "        lf_input_channels=len(lf_features),\n",
    "        cnn_output_channels=cnn_output_channels,\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_num_layers=lstm_num_layers,\n",
    "        lstm_bidirectional=True,\n",
    "        dropout=dropout,\n",
    "        num_sleep_stages=4,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing=label_smoothing,\n",
    "        weight_tensor=torch.tensor(class_weights, dtype=torch.float32),\n",
    "        convnet='TCN',\n",
    "        num_tcn_blocks=num_tcn_blocks,\n",
    "        tcn_kernel_size=tcn_kernel_size,\n",
    "        debug=False\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    wandb.finish()\n",
    "    clear_output()\n",
    "    # load best model\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    best_model = DualFreqSleepStager.load_from_checkpoint(best_model_path)\n",
    "    # save the model\n",
    "    torch.save(best_model.state_dict(), f'models/TCN-LSTM/best_model_{runNo}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f8f52",
   "metadata": {},
   "source": [
    "## Test TCN-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa34158",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 10\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "cnn_output_channels = 16\n",
    "dropout = 0.21312\n",
    "label_smoothing = 0.0050416\n",
    "learning_rate = 0.0048844\n",
    "lstm_hidden_size = 256\n",
    "lstm_num_layers = 2\n",
    "num_tcn_blocks = 7\n",
    "tcn_kernel_size = 5\n",
    "weight_decay = 0.0000078942\n",
    "\n",
    "\n",
    "\n",
    "test_aurocs = []\n",
    "test_accuracies = []\n",
    "test_kappas = []\n",
    "test_confusions = []\n",
    "\n",
    "for runNo in range(num_runs):\n",
    "    model = DualFreqSleepStager(\n",
    "        hf_input_channels=len(hf_features),\n",
    "        lf_input_channels=len(lf_features),\n",
    "        cnn_output_channels=cnn_output_channels,\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_num_layers=lstm_num_layers,\n",
    "        lstm_bidirectional=True,\n",
    "        dropout=dropout,\n",
    "        num_sleep_stages=4,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing=label_smoothing,\n",
    "        weight_tensor=torch.tensor(class_weights, dtype=torch.float32),\n",
    "        convnet='TCN',\n",
    "        num_tcn_blocks=num_tcn_blocks,\n",
    "        tcn_kernel_size=tcn_kernel_size,\n",
    "        debug=False\n",
    "    )\n",
    "    model.load_state_dict(torch.load(f'models/TCN-LSTM/best_model_{runNo}.pth'))\n",
    "\n",
    "    test_acc, test_kappa, test_auroc, test_cm = evaluate_model(model, test_loader)\n",
    "    test_aurocs.append(test_auroc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    test_kappas.append(test_kappa)\n",
    "    test_confusions.append(test_cm)\n",
    "\n",
    "# print results\n",
    "print(f\"Test AUROC: {np.mean(test_aurocs):.4f} +/- {np.std(test_aurocs):.4f}\")\n",
    "print(f\"Test Accuracy: {np.mean(test_accuracies):.4f} +/- {np.std(test_accuracies):.4f}\")\n",
    "print(f\"Test Kappa: {np.mean(test_kappas):.4f} +/- {np.std(test_kappas):.4f}\")\n",
    "\n",
    "# get mean confusion matrix\n",
    "tcn_mean_confusion = np.mean(test_confusions, axis=0)\n",
    "# display mean confusion matrix\n",
    "plot_confusion_matrix_with_metrics(test_confusions[-1], class_mapping={0:'W',1:'Light',2:'Deep',3:'REM'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ee87e",
   "metadata": {},
   "source": [
    "# CNN-LSTM Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a786f0",
   "metadata": {},
   "source": [
    "## CNN-LSTM Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_optuna_trials = 100 # not enough to cover every combination, but should be enough to find a good one\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(cnn_objective, n_trials=num_optuna_trials)\n",
    "best_trial = study.best_trial\n",
    "clear_output()\n",
    "print(f\"Best trial: {best_trial.number}\")\n",
    "print(f\"Best trial value: {best_trial.value}\")\n",
    "print(f\"Best trial params: {best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ebffe",
   "metadata": {},
   "source": [
    "## Train and Test CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97e0bb",
   "metadata": {},
   "source": [
    "### Train CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a5edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cnn_output_channels = best_trial.params['cnn_output_channels']\n",
    "lstm_hidden_size = best_trial.params['lstm_hidden_size']\n",
    "num_layers = best_trial.params['num_layers']\n",
    "dropout = best_trial.params['dropout']\n",
    "learning_rate = best_trial.params['learning_rate']\n",
    "weight_decay = best_trial.params['weight_decay']\n",
    "label_smoothing = best_trial.params['label_smoothing']\n",
    "'''\n",
    "cnn_output_channels = 64\n",
    "lstm_hidden_size = 256\n",
    "lstm_num_layers = 6\n",
    "dropout = 0.3348076952070481\n",
    "learning_rate = 0.0010265829979403838\n",
    "weight_decay = 4.969225017183202e-06\n",
    "label_smoothing = 0.12009173751056051\n",
    "\n",
    "\n",
    "test_aurocs = []\n",
    "test_accuracies = []\n",
    "test_kappas = []\n",
    "test_confusions = []\n",
    "\n",
    "num_runs = 10\n",
    "max_epochs = 50\n",
    "for runNo in range(num_runs):\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"CNN-LSTM-Dual-Freq-Sleep-Stager\",\n",
    "        name=f\"best-params-{runNo}\",\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_cohen_kappa',\n",
    "        dirpath='checkpoints/CNN-LSTM/',\n",
    "        filename='best-checkpoint',\n",
    "        save_top_k=1,\n",
    "        mode='max'\n",
    "    )\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_cohen_kappa',\n",
    "        patience=10,\n",
    "        verbose=True,\n",
    "        mode='max'\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        devices=1,\n",
    "        accelerator='gpu',\n",
    "        logger=wandb_logger,\n",
    "        log_every_n_steps=1,\n",
    "        precision=\"16-mixed\",\n",
    "        #callbacks=[checkpoint_callback, early_stop_callback]\n",
    "        callbacks=[checkpoint_callback] # not using early stopping for now\n",
    "    )\n",
    "    model = DualFreqSleepStager(\n",
    "        hf_input_channels=len(hf_features),\n",
    "        lf_input_channels=len(lf_features),\n",
    "        cnn_output_channels=cnn_output_channels,\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_num_layers=lstm_num_layers,\n",
    "        lstm_bidirectional=True,\n",
    "        dropout=dropout,\n",
    "        num_sleep_stages=4,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing=label_smoothing,\n",
    "        weight_tensor=torch.tensor(class_weights, dtype=torch.float32),\n",
    "        convnet='CNN',\n",
    "        debug=False\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    wandb.finish()\n",
    "    clear_output()\n",
    "    # load best model\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    best_model = DualFreqSleepStager.load_from_checkpoint(best_model_path)\n",
    "\n",
    "    # test the model\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    test_acc, test_kappa, test_auroc, test_cm = evaluate_model(best_model, test_loader)\n",
    "    test_aurocs.append(test_auroc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    test_kappas.append(test_kappa)\n",
    "    test_confusions.append(test_cm)\n",
    "\n",
    "    # save the model\n",
    "    torch.save(best_model.state_dict(), f'models/CNN-LSTM/best_model_{runNo}.pth')\n",
    "\n",
    "\n",
    "print(f\"Test AUROC: {np.mean(test_aurocs):.4f} +/- {np.std(test_aurocs):.4f}\")\n",
    "print(f\"Test Accuracy: {np.mean(test_accuracies):.4f} +/- {np.std(test_accuracies):.4f}\")\n",
    "print(f\"Test Kappa: {np.mean(test_kappas):.4f} +/- {np.std(test_kappas):.4f}\")\n",
    "\n",
    "# get mean confusion matrix\n",
    "cnn_mean_confusion = np.mean(test_confusions, axis=0)\n",
    "# display mean confusion matrix\n",
    "plot_confusion_matrix(cnn_mean_confusion, classes=['W','N1','N2','N3','R'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc13189",
   "metadata": {},
   "source": [
    "### Test CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b4fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 10\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "cnn_output_channels = 64\n",
    "lstm_hidden_size = 256\n",
    "lstm_num_layers = 6\n",
    "dropout = 0.3348076952070481\n",
    "learning_rate = 0.0010265829979403838\n",
    "weight_decay = 4.969225017183202e-06\n",
    "label_smoothing = 0.12009173751056051\n",
    "\n",
    "\n",
    "test_aurocs = []\n",
    "test_accuracies = []\n",
    "test_kappas = []\n",
    "test_confusions = []\n",
    "\n",
    "for runNo in range(num_runs):\n",
    "    model = DualFreqSleepStager(\n",
    "        hf_input_channels=len(hf_features),\n",
    "        lf_input_channels=len(lf_features),\n",
    "        cnn_output_channels=cnn_output_channels,\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_num_layers=lstm_num_layers,\n",
    "        lstm_bidirectional=True,\n",
    "        dropout=dropout,\n",
    "        num_sleep_stages=4,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing=label_smoothing,\n",
    "        weight_tensor=torch.tensor(class_weights, dtype=torch.float32),\n",
    "        convnet='CNN',\n",
    "        debug=False\n",
    "    )\n",
    "    model.load_state_dict(torch.load(f'models/CNN-LSTM/best_model_{runNo}.pth'))\n",
    "    test_acc, test_kappa, test_auroc, test_cm = evaluate_model(model, test_loader)\n",
    "    test_aurocs.append(test_auroc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    test_kappas.append(test_kappa)\n",
    "    test_confusions.append(test_cm)\n",
    "\n",
    "print(f\"Test AUROC: {np.mean(test_aurocs):.4f} +/- {np.std(test_aurocs):.4f}\")\n",
    "print(f\"Test Accuracy: {np.mean(test_accuracies):.4f} +/- {np.std(test_accuracies):.4f}\")\n",
    "print(f\"Test Kappa: {np.mean(test_kappas):.4f} +/- {np.std(test_kappas):.4f}\")\n",
    "\n",
    "# get mean confusion matrix\n",
    "cnn_mean_confusion = np.mean(test_confusions, axis=0)\n",
    "# display last confusion matrix\n",
    "#plot_confusion_matrix(test_confusions[-1,:,:], classes=['W','N1','N2','N3','R'])\n",
    "plot_confusion_matrix_with_metrics(test_confusions[-1], class_mapping={0:'W',1:'Light',2:'Deep',3:'REM'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02516e4a",
   "metadata": {},
   "source": [
    "# Compare Inference Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bac86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "temp_hf, temp_lf, temp_labels = train_dataset[0]\n",
    "temp_hf = temp_hf.unsqueeze(0).to(device)\n",
    "temp_lf = temp_lf.unsqueeze(0).to(device)\n",
    "# LSTM Only\n",
    "lstm_hidden_size = 256\n",
    "lstm_num_layers = 2\n",
    "dropout = 0.21312\n",
    "learning_rate = 0.0048844\n",
    "weight_decay = 0.0000078942\n",
    "label_smoothing = 0.0050416\n",
    "lstm_model = LSTMSleepStager(\n",
    "        hf_input_channels=len(hf_features),\n",
    "        lf_input_channels=len(lf_features),\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_num_layers=lstm_num_layers,\n",
    "        lstm_bidirectional=True,\n",
    "        dropout=dropout,\n",
    "        num_sleep_stages=4,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing=label_smoothing,\n",
    "        weight_tensor=torch.tensor(class_weights, dtype=torch.float32),\n",
    "        debug=False\n",
    "    )\n",
    "lstm_model.eval()\n",
    "lstm_model.to(device)\n",
    "\n",
    "# CNN-LSTM\n",
    "cnn_output_channels = 64\n",
    "lstm_hidden_size = 256\n",
    "lstm_num_layers = 6\n",
    "dropout = 0.3348076952070481\n",
    "learning_rate = 0.0010265829979403838\n",
    "weight_decay = 4.969225017183202e-06\n",
    "label_smoothing = 0.12009173751056051\n",
    "\n",
    "cnn_model = DualFreqSleepStager(\n",
    "        hf_input_channels=len(hf_features),\n",
    "        lf_input_channels=len(lf_features),\n",
    "        cnn_output_channels=cnn_output_channels,\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_num_layers=lstm_num_layers,\n",
    "        lstm_bidirectional=True,\n",
    "        dropout=dropout,\n",
    "        num_sleep_stages=4,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing=label_smoothing,\n",
    "        weight_tensor=torch.tensor(class_weights, dtype=torch.float32),\n",
    "        convnet='CNN',\n",
    "        debug=False\n",
    "    )\n",
    "cnn_model.eval()\n",
    "cnn_model.to(device)\n",
    "\n",
    "# TCN-LSTM\n",
    "cnn_output_channels = 16\n",
    "dropout = 0.21312\n",
    "label_smoothing = 0.0050416\n",
    "learning_rate = 0.0048844\n",
    "lstm_hidden_size = 256\n",
    "lstm_num_layers = 2\n",
    "num_tcn_blocks = 7\n",
    "tcn_kernel_size = 5\n",
    "weight_decay = 0.0000078942\n",
    "\n",
    "tcn_model = DualFreqSleepStager(\n",
    "        hf_input_channels=len(hf_features),\n",
    "        lf_input_channels=len(lf_features),\n",
    "        cnn_output_channels=cnn_output_channels,\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        lstm_num_layers=lstm_num_layers,\n",
    "        lstm_bidirectional=True,\n",
    "        dropout=dropout,\n",
    "        num_sleep_stages=4,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        label_smoothing=label_smoothing,\n",
    "        weight_tensor=torch.tensor(class_weights, dtype=torch.float32),\n",
    "        convnet='TCN',\n",
    "        num_tcn_blocks=num_tcn_blocks,\n",
    "        tcn_kernel_size=tcn_kernel_size,\n",
    "        debug=False\n",
    "    )\n",
    "tcn_model.eval()\n",
    "tcn_model.to(device)\n",
    "\n",
    "lstm_times = []\n",
    "cnn_times = []\n",
    "tcn_times = []\n",
    "num_trials = 100\n",
    "with torch.no_grad():\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        cnn_model(temp_hf, temp_lf)\n",
    "        tcn_model(temp_hf, temp_lf)\n",
    "    torch.cuda.synchronize()\n",
    "    # measure time\n",
    "    start = time.time()\n",
    "    for _ in range(num_trials):\n",
    "        cnn_model(temp_hf, temp_lf)\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    cnn_times.append((end - start) / 100)\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(num_trials):\n",
    "        tcn_model(temp_hf, temp_lf)\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    tcn_times.append((end - start) / 100)\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(num_trials):\n",
    "        lstm_model(temp_hf, temp_lf)\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    lstm_times.append((end - start) / 100)\n",
    "print(f\"cnn time: {np.mean(cnn_times):.4f} +/- {np.std(cnn_times):.4f}\")\n",
    "print(f\"tcn time: {np.mean(tcn_times):.4f} +/- {np.std(tcn_times):.4f}\")\n",
    "print(f\"lstm time: {np.mean(lstm_times):.4f} +/- {np.std(lstm_times):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl4med_25)",
   "language": "python",
   "name": "dl4med_25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
