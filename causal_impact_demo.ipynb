{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b171763c",
   "metadata": {},
   "source": [
    "# Investigation of the impact of causality constraints on a CNN-LSTM sleep staging model\n",
    "\n",
    "### Project goals:\n",
    "##### Classify sleep stages using multi-modal sensor data (BVP, accelerometer, timestamps, temperature).\n",
    "##### Compare model performance and computation between non-causal versus causal architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402878f",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4da426bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchmetrics.classification import MulticlassCohenKappa\n",
    "from IPython.display import clear_output\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "import wandb\n",
    "from x_transformers import ContinuousTransformerWrapper, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53dc944f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2917620/190206057.py:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  demo_df = pd.read_csv(demo_csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TIMESTAMP', 'BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP', 'EDA', 'HR',\n",
      "       'IBI', 'Sleep_Stage', 'Obstructive_Apnea', 'Central_Apnea', 'Hypopnea',\n",
      "       'Multiple_Events'],\n",
      "      dtype='object')\n",
      "TIMESTAMP            float64\n",
      "BVP                  float64\n",
      "ACC_X                float64\n",
      "ACC_Y                 object\n",
      "ACC_Z                float64\n",
      "TEMP                 float64\n",
      "EDA                  float64\n",
      "HR                   float64\n",
      "IBI                  float64\n",
      "Sleep_Stage           object\n",
      "Obstructive_Apnea    float64\n",
      "Central_Apnea        float64\n",
      "Hypopnea             float64\n",
      "Multiple_Events      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# demo csv, get columns\n",
    "demo_csv_path = '/gpfs/data/oermannlab/users/slj9342/dl4med_25/data/physionet.org/files/dreamt/2.0.0/data_64Hz/S016_whole_df.csv'\n",
    "demo_df = pd.read_csv(demo_csv_path)\n",
    "print(demo_df.columns)\n",
    "col_dtypes = demo_df.dtypes\n",
    "print(col_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d51dae",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e38a702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TIMESTAMP', 'BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP', 'EDA', 'HR', 'IBI', 'Sleep_Stage', 'Obstructive_Apnea', 'Central_Apnea', 'Hypopnea', 'Multiple_Events']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2917620/2388715554.py:24: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(datadir_64Hz, file))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTAMP            float64\n",
      "BVP                  float64\n",
      "ACC_X                float64\n",
      "ACC_Y                 object\n",
      "ACC_Z                float64\n",
      "TEMP                 float64\n",
      "EDA                  float64\n",
      "HR                   float64\n",
      "IBI                  float64\n",
      "Sleep_Stage           object\n",
      "Obstructive_Apnea    float64\n",
      "Central_Apnea        float64\n",
      "Hypopnea             float64\n",
      "Multiple_Events      float64\n",
      "dtype: object\n",
      "❌ Column ACC_Y failed: Unable to parse string \"N1\" at position 2054657\n",
      "❌ Column Sleep_Stage failed: Unable to parse string \"P\" at position 0\n"
     ]
    }
   ],
   "source": [
    "datadir_64Hz = '/gpfs/data/oermannlab/users/slj9342/dl4med_25/data/physionet.org/files/dreamt/2.0.0/data_64Hz/' # working with 64Hz data\n",
    "\n",
    "dtype_dict = {\n",
    "    'TIMESTAMP': np.float32,\n",
    "    'BVP': np.float32,\n",
    "    'ACC_X': np.float32,\n",
    "    'ACC_Y': np.float32,\n",
    "    'ACC_Z': np.float32,\n",
    "    'TEMP': np.float32,\n",
    "    'EDA': np.float32,\n",
    "    'HR': np.float32,\n",
    "    'IBI': np.float32,  # <-- missing but important\n",
    "    'Sleep_Stage': 'category',\n",
    "    'Obstructive_Apnea': 'Int64',  # Nullable integer\n",
    "    'Central_Apnea': 'Int64',\n",
    "    'Hypopnea': 'Int64',\n",
    "    'Multiple_Events': 'Int64'\n",
    "}\n",
    "\n",
    "file = 'S016_whole_df.csv'\n",
    "df_head = pd.read_csv(os.path.join(datadir_64Hz, file), nrows=5)\n",
    "print(df_head.columns.tolist())\n",
    "\n",
    "df = pd.read_csv(os.path.join(datadir_64Hz, file))\n",
    "print(df.dtypes)\n",
    "\n",
    "df = pd.read_csv(os.path.join(datadir_64Hz, file), low_memory=False)\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        pd.to_numeric(df[col], errors='raise')\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Column {col} failed: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a23638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max sequence length\n",
    "def safe_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "numeric_columns = [\n",
    "    'TIMESTAMP', 'BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP',\n",
    "    'EDA', 'HR', 'IBI'\n",
    "]\n",
    "converters = {col: safe_float for col in numeric_columns}\n",
    "'''\n",
    "max_length = 0\n",
    "for file in os.listdir(datadir_64Hz):\n",
    "    if file.endswith('_whole_df.csv'):\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(datadir_64Hz, file),\n",
    "            dtype={'Sleep_Stage': 'category'},\n",
    "            converters=converters,\n",
    "            low_memory=True\n",
    "        )\n",
    "        max_length = max(max_length, len(df))\n",
    "print(f\"Max sequence length: {max_length}\")\n",
    "'''\n",
    "max_length = 2493810"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fcf3ce",
   "metadata": {},
   "source": [
    "### Split subjects into train, val, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a0a89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of subjects in train: 80\n",
      "number of subjects in val: 10\n",
      "number of subjects in test: 10\n"
     ]
    }
   ],
   "source": [
    "participant_info_df = pd.read_csv('/gpfs/data/oermannlab/users/slj9342/dl4med_25/data/physionet.org/files/dreamt/2.0.0/participant_info.csv')\n",
    "subjects_all = participant_info_df['SID']\n",
    "\n",
    "subjects_all_shuffled = participant_info_df['SID'].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "subjects_train = subjects_all_shuffled[:int(len(subjects_all_shuffled)*0.8)]\n",
    "subjects_val = subjects_all_shuffled[int(len(subjects_all_shuffled)*0.8):int(len(subjects_all_shuffled)*0.9)]\n",
    "subjects_test = subjects_all_shuffled[int(len(subjects_all_shuffled)*0.9):]\n",
    "print(f\"number of subjects in train: {len(subjects_train)}\")\n",
    "print(f\"number of subjects in val: {len(subjects_val)}\")\n",
    "print(f\"number of subjects in test: {len(subjects_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059ce4b",
   "metadata": {},
   "source": [
    "### Non-windowed dataset class\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ba2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLEEP_STAGE_MAPPING = {\n",
    "    \"W\": 0,    # Wake\n",
    "    \"N1\": 1,   # non-REM stage 1\n",
    "    \"N2\": 2,   # non-REM stage 2\n",
    "    \"N3\": 3,   # non-REM stage 3\n",
    "    \"R\": 4,    # REM\n",
    "    \"Missing\": -1  # Missing label\n",
    "}\n",
    "\n",
    "def forward_fill(x):\n",
    "    \"\"\"\n",
    "    Performs forward fill on a tensor. If x is 1D (shape [T]),\n",
    "    it's temporarily unsqueezed to [T, 1] and then processed.\n",
    "    Assumes the first value is valid, or fills it with zero if needed.\n",
    "    \"\"\"\n",
    "    single_channel = False\n",
    "    if x.dim() == 1:\n",
    "        x = x.unsqueeze(1)\n",
    "        single_channel = True\n",
    "    \n",
    "    T, C = x.shape\n",
    "    for c in range(C):\n",
    "        # Optionally, handle the first element if it's NaN\n",
    "        if torch.isnan(x[0, c]):\n",
    "            x[0, c] = 0.0  # or choose another default value\n",
    "        for t in range(1, T):\n",
    "            if torch.isnan(x[t, c]):\n",
    "                x[t, c] = x[t - 1, c]\n",
    "    \n",
    "    if single_channel:\n",
    "        x = x.squeeze(1)\n",
    "    return x\n",
    "\n",
    "class SleepDataset(Dataset):\n",
    "    def __init__(self, subjects_list, data_dir, max_length, downsample_freq=64, debug=False):\n",
    "        self.subjects = [{} for _ in range(len(subjects_list))]\n",
    "        self.downsample = int(64 // downsample_freq)  # Downsample factor\n",
    "        self.max_length = int(max_length // self.downsample)\n",
    "\n",
    "        for subjectNo, SID in enumerate(subjects_list):\n",
    "            # Load the data for each subject\n",
    "            file_path = os.path.join(data_dir, f\"{SID}_whole_df.csv\")\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(\n",
    "                    file_path,\n",
    "                    dtype={'Sleep_Stage': 'category'},\n",
    "                    converters=converters,\n",
    "                    low_memory=True\n",
    "                )\n",
    "                if debug:\n",
    "                    print(f\"loaded data for {SID}:\")\n",
    "\n",
    "                # Downsample the data if needed\n",
    "                if self.downsample != 1:\n",
    "                    df = df.iloc[::self.downsample].reset_index(drop=True)\n",
    "                    if debug:\n",
    "                        print(f\"After downsampling by factor {self.downsample}, rows: {len(df)}\")\n",
    "                \n",
    "                df = df[df['Sleep_Stage'] != 'P'] # remove data before PSG start\n",
    "                for col in ['ACC_X', 'ACC_Y', 'ACC_Z','BVP', 'TEMP', 'TIMESTAMP']:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                ACC = np.sqrt(df['ACC_X']**2 + df['ACC_Y']**2 + df['ACC_Z']**2) # assuming its unlikely each acc channel really carries important information\n",
    "                df_X = df[['TIMESTAMP', 'BVP', 'TEMP']].copy()\n",
    "                df_X['ACC'] = ACC\n",
    "                # Normalize the features (z-score normalization per subject)\n",
    "                TEMP_norm = (df_X['TEMP'] - df_X['TEMP'].mean()) / df_X['TEMP'].std()\n",
    "                df_X['TEMP'] = TEMP_norm\n",
    "                BVP_norm = (df_X['BVP'] - df_X['BVP'].mean()) / df_X['BVP'].std()\n",
    "                df_X['BVP'] = BVP_norm\n",
    "                df['Sleep_Stage'] = df['Sleep_Stage'].astype(str).str.strip()\n",
    "                df_Y = df['Sleep_Stage'].map(SLEEP_STAGE_MAPPING)\n",
    "                \n",
    "                # Pad/truncate the data to the downsampled max_length\n",
    "                if len(df_X) > self.max_length:\n",
    "                    if debug:\n",
    "                        print(f\"Truncating data for {SID} from {len(df_X)} to {self.max_length} samples.\")\n",
    "                    df_X = df_X.iloc[:self.max_length]\n",
    "                    df_Y = df_Y.iloc[:self.max_length]\n",
    "                else:\n",
    "                    padding_length = self.max_length - len(df_X)\n",
    "                    padding = pd.DataFrame(np.nan, index=np.arange(padding_length), columns=df_X.columns)\n",
    "                    df_X = pd.concat([df_X, padding], ignore_index=True)\n",
    "                    df_Y = pd.concat([df_Y, pd.Series([-1] * padding_length)], ignore_index=True)\n",
    "                self.subjects[subjectNo] = {\n",
    "                    'data': df_X.values.astype(np.float32),  # shape: [T, C]\n",
    "                    'labels': df_Y.to_numpy(),                 # shape: [T]\n",
    "                    'SID': SID\n",
    "                }\n",
    "                if debug:\n",
    "                    print(f\"Data shape for {SID}: {df_X.shape}, Labels shape: {df_Y.shape}\")\n",
    "            else:\n",
    "                warning(f\"File {file_path} does not exist. Skipping subject {SID}.\")\n",
    "    def __len__(self):\n",
    "        return len(self.subjects)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject = self.subjects[idx]\n",
    "        data = torch.tensor(subject['data'], dtype=torch.float32)\n",
    "        labels = torch.tensor(subject['labels'], dtype=torch.long)\n",
    "\n",
    "        data = forward_fill(data) # fill NaNs with previous values\n",
    "        labels = forward_fill(labels) # fill NaNs with previous values\n",
    "        return data, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7bda52",
   "metadata": {},
   "source": [
    "### Chunked Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c010b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep stage mapping as before\n",
    "SLEEP_STAGE_MAPPING = {\n",
    "    \"W\": 0,    # Wake\n",
    "    \"N1\": 1,   # non-REM stage 1\n",
    "    \"N2\": 2,   # non-REM stage 2\n",
    "    \"N3\": 3,   # non-REM stage 3\n",
    "    \"R\": 4,    # REM\n",
    "    \"Missing\": -1  # Missing label\n",
    "}\n",
    "\n",
    "def forward_fill(x):\n",
    "    \"\"\"\n",
    "    Performs forward fill on a tensor.\n",
    "    If x is 1D (shape [T]), it is temporarily unsqueezed to [T, 1].\n",
    "    Assumes the first value is valid, or fills it with zero if needed.\n",
    "    \"\"\"\n",
    "    single_channel = False\n",
    "    if x.dim() == 1:\n",
    "        x = x.unsqueeze(1)\n",
    "        single_channel = True\n",
    "\n",
    "    T, C = x.shape\n",
    "    for c in range(C):\n",
    "        if torch.isnan(x[0, c]):\n",
    "            x[0, c] = 0.0\n",
    "        for t in range(1, T):\n",
    "            if torch.isnan(x[t, c]):\n",
    "                x[t, c] = x[t - 1, c]\n",
    "    if single_channel:\n",
    "        x = x.squeeze(1)\n",
    "    return x\n",
    "\n",
    "numeric_columns = [\n",
    "    'TIMESTAMP', 'BVP', 'ACC_X', 'ACC_Y', 'ACC_Z', 'TEMP',\n",
    "    'EDA', 'HR', 'IBI'\n",
    "]\n",
    "converters = {col: safe_float for col in numeric_columns}\n",
    "\n",
    "class SleepChunkDataset(Dataset):\n",
    "    def __init__(self, subjects_list, data_dir, chunk_duration=600, chunk_stride=300, \n",
    "                 downsample_freq=64, feature_columns=None, debug=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            subjects_list (list): List of subject IDs, e.g. [\"SID1\", \"SID2\", ...].\n",
    "            data_dir (str): Directory containing files like \"SID_whole_df.csv\".\n",
    "            chunk_duration (int): Chunk length in seconds.\n",
    "            chunk_stride (int): Stride between chunks in seconds.\n",
    "            downsample_freq (int): Target frequency after downsampling (from 64 Hz).\n",
    "            debug (bool): Whether to print debugging information.\n",
    "            feature_columns (list or None): List of columns to keep (e.g. ['TIMESTAMP', 'BVP', 'ACC', 'TEMP']).\n",
    "                                          If None, a default list is used.\n",
    "        \"\"\"\n",
    "        # Default features; note \"ACC\" is computed from the accelerometer axes.\n",
    "        if feature_columns is None:\n",
    "            self.feature_columns = ['ACC','TIMESTAMP', 'BVP', 'TEMP', 'HR', 'IBI']\n",
    "        else:\n",
    "            self.feature_columns = feature_columns\n",
    "\n",
    "        self.chunks = []\n",
    "        self.downsample = int(64 // downsample_freq)\n",
    "        self.chunk_length = int(chunk_duration * downsample_freq)\n",
    "        self.stride = int(chunk_stride * downsample_freq)\n",
    "\n",
    "        for SID in subjects_list:\n",
    "            file_path = os.path.join(data_dir, f\"{SID}_whole_df.csv\")\n",
    "            if os.path.exists(file_path):\n",
    "                # Load the data. The converters dict can be kept from before if you use it for other columns.\n",
    "                df = pd.read_csv(file_path, dtype={'Sleep_Stage': 'category'},\n",
    "                                 converters=converters, low_memory=True)\n",
    "                if debug:\n",
    "                    print(f\"Loaded data for subject {SID}\")\n",
    "\n",
    "                # Downsample: take every self.downsample-th row.\n",
    "                if self.downsample != 1:\n",
    "                    df = df.iloc[::self.downsample].reset_index(drop=True)\n",
    "                    if debug:\n",
    "                        print(f\"After downsampling (factor {self.downsample}), rows: {len(df)}\")\n",
    "\n",
    "                # Remove rows in the \"Preparation\" phase labeled as 'P'.\n",
    "                df = df[df['Sleep_Stage'] != 'P']\n",
    "\n",
    "                # Convert to numeric for any columns we plan to use (except the computed ones)\n",
    "                for col in df.columns:\n",
    "                    if col in self.feature_columns and col != 'ACC':\n",
    "                        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "                # If 'ACC' is requested, compute it from the three accelerometer axes.\n",
    "                if 'ACC' in self.feature_columns:\n",
    "                    df['ACC'] = np.sqrt(df['ACC_X']**2 + df['ACC_Y']**2 + df['ACC_Z']**2)\n",
    "\n",
    "                # Filter the dataframe to the columns of interest.\n",
    "                df_X = df[self.feature_columns].copy()\n",
    "\n",
    "                # Process sleep stage labels: trim whitespace and map to integer.\n",
    "                df['Sleep_Stage'] = df['Sleep_Stage'].astype(str).str.strip()\n",
    "                df_Y = df['Sleep_Stage'].map(SLEEP_STAGE_MAPPING)\n",
    "\n",
    "                # Convert features and labels to numpy arrays.\n",
    "                data_arr = df_X.values.astype(np.float32)\n",
    "                labels_arr = df_Y.to_numpy()\n",
    "                T = data_arr.shape[0]\n",
    "\n",
    "                # If the record is shorter than one chunk, pad it.\n",
    "                if T < self.chunk_length:\n",
    "                    pad_size = self.chunk_length - T\n",
    "                    padding_data = np.full((pad_size, data_arr.shape[1]), np.nan, dtype=np.float32)\n",
    "                    data_arr = np.concatenate([data_arr, padding_data], axis=0)\n",
    "                    padding_labels = np.full((pad_size,), -1)\n",
    "                    labels_arr = np.concatenate([labels_arr, padding_labels], axis=0)\n",
    "                    T = self.chunk_length\n",
    "\n",
    "                # Create overlapping chunks using a sliding window.\n",
    "                for start in range(0, T - self.chunk_length + 1, self.stride):\n",
    "                    end = start + self.chunk_length\n",
    "                    chunk_data = data_arr[start:end, :]\n",
    "                    chunk_labels = labels_arr[start:end]\n",
    "                    self.chunks.append({\n",
    "                        'data': chunk_data,\n",
    "                        'labels': chunk_labels,\n",
    "                        'SID': SID\n",
    "                    })\n",
    "                if debug:\n",
    "                    num_chunks = (T - self.chunk_length) // self.stride + 1\n",
    "                    print(f\"Subject {SID}: {T} samples processed, generated {num_chunks} chunks\")\n",
    "            else:\n",
    "                print(f\"File {file_path} does not exist. Skipping subject {SID}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.chunks[idx]\n",
    "        data = torch.tensor(chunk['data'], dtype=torch.float32)\n",
    "        labels = torch.tensor(chunk['labels'], dtype=torch.long)\n",
    "        # Forward fill to replace any NaN values with the previous valid value.\n",
    "        data = forward_fill(data)\n",
    "        labels = forward_fill(labels)\n",
    "        return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb17bf",
   "metadata": {},
   "source": [
    "### Construct train, val, and test datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d268017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in train dataset: 80\n",
      "Total samples in val dataset: 10\n",
      "Total samples in test dataset: 10\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "train_dataset_windowed = SleepDataset(subjects_list=subjects_train,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 window_size_ms= 20000, # 20 seconds of data\n",
    "                                 stride_ms=5000,        # 5 seconds overlap\n",
    "                                 downsample_freq=8, # downsample to 16Hz\n",
    "                                 debug=False)\n",
    "val_dataset_windowed = SleepDataset(subjects_list=subjects_val,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 window_size_ms= 20000, # 20 seconds of data\n",
    "                                 stride_ms=5000,        # 5 seconds overlap\n",
    "                                 downsample_freq=8, # downsample to 16Hz\n",
    "                                 debug=False)\n",
    "test_dataset_windowed = SleepDataset(subjects_list=subjects_test,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 window_size_ms= 20000, # 20 seconds of data\n",
    "                                 stride_ms=5000,        # 5 second stride\n",
    "                                 downsample_freq=8, # downsample to 16Hz\n",
    "                                 debug=False)\n",
    "'''\n",
    "\n",
    "train_dataset = SleepDataset(subjects_list=subjects_train,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 max_length=max_length,\n",
    "                                 downsample_freq=8, # downsample to 8Hz\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in train dataset: {len(train_dataset)}\")\n",
    "val_dataset = SleepDataset(subjects_list=subjects_val,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 max_length=max_length,\n",
    "                                 downsample_freq=8, # downsample to 8Hz\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in val dataset: {len(val_dataset)}\")                                 \n",
    "test_dataset = SleepDataset(subjects_list=subjects_test,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 max_length=max_length,\n",
    "                                 downsample_freq=8, # downsample to 8Hz\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in test dataset: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc8ab676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in train chunk dataset: 5107\n",
      "Total samples in val chunk dataset: 641\n",
      "Total samples in test chunk dataset: 665\n"
     ]
    }
   ],
   "source": [
    "train_chunk_dataset = SleepChunkDataset(subjects_list=subjects_train,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 chunk_duration=6000,  # 100 minutes\n",
    "                                 chunk_stride=300,    # 5 minutes\n",
    "                                 downsample_freq=0.2,   # downsample to 0.2 Hz\n",
    "                                 feature_columns=['TIMESTAMP', 'BVP', 'TEMP', 'HR', 'IBI', 'ACC'],\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in train chunk dataset: {len(train_chunk_dataset)}\")\n",
    "val_chunk_dataset = SleepChunkDataset(subjects_list=subjects_val,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 chunk_duration=6000,  # 100 minutes\n",
    "                                 chunk_stride=300,    # 5 minutes\n",
    "                                 downsample_freq=0.2,   # downsample to 0.2 Hz\n",
    "                                 feature_columns=['TIMESTAMP', 'BVP', 'TEMP', 'HR', 'IBI', 'ACC'],\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in val chunk dataset: {len(val_chunk_dataset)}\")\n",
    "test_chunk_dataset = SleepChunkDataset(subjects_list=subjects_test,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 chunk_duration=6000,  # 100 minutes\n",
    "                                 chunk_stride=300,    # 5 minutes\n",
    "                                 downsample_freq=0.2,   # downsample to 0.2 Hz\n",
    "                                 feature_columns=['TIMESTAMP', 'BVP', 'TEMP', 'HR', 'IBI', 'ACC'],\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in test chunk dataset: {len(test_chunk_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92f119ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: Counter({np.int64(2): 3313672, np.int64(0): 1254251, np.int64(4): 700256, np.int64(1): 635351, np.int64(3): 217645})\n",
      "Class weights: [0.97606859 1.92686405 0.36944966 5.62491672 1.74826778]\n"
     ]
    }
   ],
   "source": [
    "# get class weights for weighted loss\n",
    "all_labels = []\n",
    "for batch in DataLoader(train_chunk_dataset, batch_size=1):\n",
    "    labels = batch[1].numpy()\n",
    "    all_labels.extend(labels.flatten())\n",
    "all_labels = np.array(all_labels)\n",
    "valid_labels = all_labels[all_labels != -1]\n",
    "classes = np.unique(valid_labels)\n",
    "class_counts = Counter(valid_labels)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=valid_labels\n",
    ")\n",
    "print(f\"Class counts: {class_counts}\")\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036b8a4",
   "metadata": {},
   "source": [
    "## CNN downsampling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdfebfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence length x input channels -> CNN -> shortened sequence length x num hidden channels -> LSTM -> shortened sequence length x num sleep stages\n",
    "\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self, in_channels=4, cnn_output_channels=128):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=5, stride=2, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv1d(64, 64, kernel_size=5, stride=2, padding=1)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv4 = nn.Conv1d(64, cnn_output_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn = nn.BatchNorm1d(cnn_output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Global average pooling to collapse \n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert not torch.isnan(x).any(), \"NaN detected in CNN input\"\n",
    "        # Expect x of shape (batch, epoch_samples, channels)\n",
    "        x = x.permute(0, 2, 1)  # Rearrange to (batch, channels, epoch_samples)\n",
    "        #print(f\"Input shape after permutation: {x.shape}\")\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        #print(f\"Shape after conv1 and pool1: {x.shape}\")\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        #print(f\"Shape after conv2 and pool2: {x.shape}\")\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        #print(f\"Shape after conv3 and pool3: {x.shape}\")\n",
    "        x = self.relu(self.conv4(x))\n",
    "        #print(f\"Shape after conv4: {x.shape}\")\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class SleepStageLSTM(nn.Module):\n",
    "    def __init__(self, cnn_output_channels=128, hidden_size=64, num_layers=2, num_sleep_stages=5):\n",
    "        super(SleepStageLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=cnn_output_channels,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_size, num_sleep_stages)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert not torch.isnan(x).any(), \"NaN detected in LSTM input\"\n",
    "        # x is of shape (batch, cnn_output_channels, samples)\n",
    "        # LSTM expects input shape of (samples, batch, cnn_output_channels)\n",
    "        x = x.permute(2, 0, 1) # (batch, cnn_output_channels, samples) -> (samples, batch, cnn_output_channels)\n",
    "        #print(f\"Input shape after permutation: {x.shape}\")\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        #print(f\"Shape after LSTM: {lstm_out.shape}\")\n",
    "        # Option 1: produce a prediction for every epoch (each time step)\n",
    "        out = self.fc(lstm_out)   # shape: (batch, num_epochs, num_sleep_stages)\n",
    "        #print(f\"Shape after fully connected layer: {out.shape}\")\n",
    "        \n",
    "        # Option 2: if you want a prediction only for the current epoch,\n",
    "        # you may take the output of the last time step:\n",
    "        #predictions = self.fc(lstm_out[:, -1, :])  # shape: (batch, num_sleep_stages)\n",
    "        return out\n",
    "\n",
    "class OnlineSleepStagingModel(pl.LightningModule):\n",
    "    def __init__(self, in_channels, cnn_output_channels, lstm_hidden_size, num_layers=2, num_sleep_stages=5, learning_rate=0.001, class_weights=None):\n",
    "        super(OnlineSleepStagingModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.feature_extractor = FeatureExtractorCNN(in_channels=in_channels, cnn_output_channels=cnn_output_channels)\n",
    "        self.lstm_model = SleepStageLSTM(cnn_output_channels=cnn_output_channels, hidden_size=lstm_hidden_size, num_layers=num_layers, num_sleep_stages=num_sleep_stages)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Ignore the \"Missing\" label (-1)\n",
    "        if class_weights is not None:\n",
    "            self.criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32), ignore_index=-1)\n",
    "        self.num_sleep_stages = num_sleep_stages\n",
    "        self.cnn_output_channels = cnn_output_channels\n",
    "\n",
    "        self.val_class_counts = Counter()\n",
    "        self.pred_class_counts = Counter()\n",
    "        self.kappa = MulticlassCohenKappa(num_classes=self.num_sleep_stages)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.lstm_model(x) # (samples, batch_size, num_sleep_stages)\n",
    "        assert x.shape[2] == self.num_sleep_stages, \"LSTM output shape != num_sleep_stages\"\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch  # y shape: [batch_size, T]\n",
    "        y_hat = self(x)  # y_hat shape: [output_length, batch_size, num_sleep_stages]\n",
    "        # Check for NaNs in the network output\n",
    "        assert not torch.isnan(y).any(), \"NaN detected in labels\"\n",
    "        assert not torch.isnan(y_hat).any(), \"NaN detected in network output\"\n",
    "    \n",
    "        # Permute to batch first\n",
    "        y_hat = y_hat.permute(1, 0, 2)\n",
    "        output_length = y_hat.shape[1]\n",
    "        y_expanded = y.unsqueeze(1)\n",
    "        # Downsample y to match y_hat\n",
    "        y_resampled = torch.nn.functional.interpolate(\n",
    "            y_expanded.float(),\n",
    "            size = (output_length,),\n",
    "            mode = 'nearest'\n",
    "        )\n",
    "        y_resampled = y_resampled.squeeze(1).long()\n",
    "\n",
    "        # Flatten y_hat and y_resampled for loss calculation\n",
    "        batch_size, output_length, num_sleep_stages = y_hat.shape\n",
    "        y_hat_flat = y_hat.reshape(batch_size * output_length, num_sleep_stages)\n",
    "        y_resampled_flat = y_resampled.reshape(batch_size * output_length)\n",
    "        # Calculate loss\n",
    "        loss = self.criterion(y_hat_flat, y_resampled_flat)\n",
    "        # Check loss for finiteness\n",
    "        assert torch.isfinite(loss), \"Loss is not finite\"\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch  # y shape: [batch_size, T]\n",
    "        y_hat = self(x)  # y_hat shape: [output_length, batch_size, num_sleep_stages]\n",
    "        # Permute to batch first\n",
    "        y_hat = y_hat.permute(1, 0, 2)\n",
    "        output_length = y_hat.shape[1]\n",
    "        y_expanded = y.unsqueeze(1)\n",
    "        # Downsample y to match y_hat\n",
    "        y_resampled = torch.nn.functional.interpolate(\n",
    "            y_expanded.float(),\n",
    "            size = (output_length,),\n",
    "            mode = 'nearest'\n",
    "        )\n",
    "        y_resampled = y_resampled.squeeze(1).long()\n",
    "\n",
    "        # Flatten y_hat and y_resampled for loss calculation\n",
    "        batch_size, output_length, num_sleep_stages = y_hat.shape\n",
    "        y_hat_flat = y_hat.reshape(batch_size * output_length, num_sleep_stages)\n",
    "        y_resampled_flat = y_resampled.reshape(batch_size * output_length)\n",
    "        predictions = torch.argmax(y_hat_flat, dim=1)\n",
    "        # Calculate Cohen's Kappa\n",
    "        assert predictions.shape[0] == y_resampled_flat.shape[0], f\"Predictions and labels have different shapes (dim 0) {predictions.shape[0]} vs {y_resampled_flat.shape[0]}\"\n",
    "        cohen_kappa_score = self.kappa(predictions, y_resampled_flat)\n",
    "        self.log(\"val_cohen_kappa\", cohen_kappa_score, prog_bar=True)\n",
    "        # Calculate loss\n",
    "        loss = self.criterion(y_hat_flat, y_resampled_flat)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "\n",
    "        # Update class counts\n",
    "        # y_resampled_flat: [batch_size * output_length]\n",
    "        # predictions: [batch_size * output_length]\n",
    "\n",
    "        mask = y_resampled_flat != -1\n",
    "        y_valid = y_resampled_flat[mask]\n",
    "        preds_valid = predictions[mask]\n",
    "\n",
    "        if y_valid.numel() > 0:\n",
    "            self.kappa.update(preds_valid, y_valid)\n",
    "            self.val_class_counts.update(y_valid.cpu().tolist())\n",
    "            self.pred_class_counts.update(preds_valid.cpu().tolist())\n",
    "\n",
    "        return loss\n",
    "  \n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Log Cohen's Kappa\n",
    "        if self.kappa.confmat.sum() > 0:\n",
    "            cohen_kappa_score = self.kappa.compute()\n",
    "            self.log(\"val_cohen_kappa\", cohen_kappa_score, prog_bar=True)\n",
    "            self.kappa.reset()\n",
    "        else:\n",
    "            self.log(\"val_cohen_kappa\", 0.0, prog_bar=True)\n",
    "\n",
    "        # W&B class distribution bar plots\n",
    "        class_labels = list(range(self.num_sleep_stages))\n",
    "        val_counts = [self.val_class_counts.get(c, 0) for c in class_labels]\n",
    "        pred_counts = [self.pred_class_counts.get(c, 0) for c in class_labels]\n",
    "\n",
    "        for c in range(self.num_sleep_stages):\n",
    "            self.log(f\"class_count_true/{c}\", self.val_class_counts.get(c, 0), on_epoch=True, prog_bar=False)\n",
    "            self.log(f\"class_count_pred/{c}\", self.pred_class_counts.get(c, 0), on_epoch=True, prog_bar=False)\n",
    "\n",
    "        self.val_class_counts.clear()\n",
    "        self.pred_class_counts.clear()\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "class CNNClassifier(pl.LightningModule):\n",
    "    def __init__(self, in_channels, cnn_output_channels, lstm_hidden_size, num_layers=2, num_sleep_stages=5, learning_rate=0.001):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.feature_extractor = FeatureExtractorCNN(in_channels=in_channels, cnn_output_channels=cnn_output_channels)\n",
    "        self.classifier = nn.Linear(cnn_output_channels, num_sleep_stages)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Ignore the \"Missing\" label (-1)\n",
    "        self.num_sleep_stages = num_sleep_stages\n",
    "        self.cnn_output_channels = cnn_output_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        assert x.shape[2] == self.num_sleep_stages, \"CNN output shape != num_sleep_stages\"\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df131758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1200, 6]) (epoch_samples, channels)\n",
      "Output shape: torch.Size([1, 128, 18]) (batch_size, cnn_output_channels, epoch_samples)\n",
      "Combined model output shape: torch.Size([18, 1, 5]) (samples, batch_size, num_sleep_stages)\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# demo of model elements\n",
    "temp_input = train_chunk_dataset[0][0]\n",
    "print(f\"Input shape: {temp_input.shape} (epoch_samples, channels)\")\n",
    "CNN_model = FeatureExtractorCNN(in_channels=6, cnn_output_channels=128)\n",
    "CNN_model.eval()\n",
    "cnn_output = CNN_model(temp_input.unsqueeze(0))\n",
    "print(f\"Output shape: {cnn_output.shape} (batch_size, cnn_output_channels, epoch_samples)\")\n",
    "LSTM_model = SleepStageLSTM(cnn_output_channels=128, hidden_size=64, num_layers=2, num_sleep_stages=5)\n",
    "LSTM_output = LSTM_model(cnn_output)\n",
    "\n",
    "# demo of combined model\n",
    "combined_model = OnlineSleepStagingModel(in_channels=6, cnn_output_channels=128, lstm_hidden_size=64, num_layers=2, num_sleep_stages=5)\n",
    "combined_model.eval()\n",
    "combined_output = combined_model(temp_input.unsqueeze(0))\n",
    "print(f\"Combined model output shape: {combined_output.shape} (samples, batch_size, num_sleep_stages)\")\n",
    "print(combined_output.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171e2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.66666666666667\n"
     ]
    }
   ],
   "source": [
    "print(temp_input.shape[0] / combined_output.shape[0])\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cdddd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4m ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250415_175254-6dy7vixv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification/runs/6dy7vixv' target=\"_blank\">online_sleep_staging_model</a></strong> to <a href='https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification' target=\"_blank\">https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification/runs/6dy7vixv' target=\"_blank\">https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification/runs/6dy7vixv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /gpfs/data/oermannlab/users/slj9342/dl4med_final_project/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                 | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | feature_extractor | FeatureExtractorCNN  | 38.1 K | train\n",
      "1 | lstm_model        | SleepStageLSTM       | 215 K  | train\n",
      "2 | criterion         | CrossEntropyLoss     | 0      | train\n",
      "3 | kappa             | MulticlassCohenKappa | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "253 K     Trainable params\n",
      "0         Non-trainable params\n",
      "253 K     Total params\n",
      "1.015     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0031719207763671875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adf4b23b754462184ed27bb889c20ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.00498509407043457,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dee47035394459f90859b1e85f2e79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0044803619384765625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617b56a80b964e4699ebf1b6e6dddf53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.548\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.004487752914428711,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac591aceae7c44ab82bce6e2028db78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.0045223236083984375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a813c14541f14258a9d7d7bc150c9f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_noinv_fmt}{postfix}]",
       "colour": null,
       "elapsed": 0.004485130310058594,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fa219e30fc4468ba7080f4c63f44fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 3 records. Best score: 1.548. Signaling Trainer to stop.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>class_count_pred/0</td><td>█▁▄▃</td></tr><tr><td>class_count_pred/1</td><td>█▁▁▁</td></tr><tr><td>class_count_pred/2</td><td>▇█▁▄</td></tr><tr><td>class_count_pred/3</td><td>▁█▅▄</td></tr><tr><td>class_count_pred/4</td><td>▄▁█▇</td></tr><tr><td>class_count_true/0</td><td>▁▁▁▁</td></tr><tr><td>class_count_true/1</td><td>▁▁▁▁</td></tr><tr><td>class_count_true/2</td><td>▁▁▁▁</td></tr><tr><td>class_count_true/3</td><td>▁▁▁▁</td></tr><tr><td>class_count_true/4</td><td>▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▆▆▆▆▆▆▆▆▆▆▆██████████</td></tr><tr><td>train_loss_epoch</td><td>█▄▂▁</td></tr><tr><td>train_loss_step</td><td>▂▆▃▅▄▅█▅▅▃▆▅▆▅▅▆▇▃▅▃▂▅▁▁▅▆▇▆█▄▅▃▆▅▅▅▆▃▅▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>val_cohen_kappa</td><td>▁█▁▆</td></tr><tr><td>val_loss</td><td>▁█▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>class_count_pred/0</td><td>303</td></tr><tr><td>class_count_pred/1</td><td>0</td></tr><tr><td>class_count_pred/2</td><td>1917</td></tr><tr><td>class_count_pred/3</td><td>3549</td></tr><tr><td>class_count_pred/4</td><td>5769</td></tr><tr><td>class_count_true/0</td><td>2476</td></tr><tr><td>class_count_true/1</td><td>1466</td></tr><tr><td>class_count_true/2</td><td>5553</td></tr><tr><td>class_count_true/3</td><td>744</td></tr><tr><td>class_count_true/4</td><td>1299</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>train_loss_epoch</td><td>1.53379</td></tr><tr><td>train_loss_step</td><td>1.4728</td></tr><tr><td>trainer/global_step</td><td>2555</td></tr><tr><td>val_cohen_kappa</td><td>0.05201</td></tr><tr><td>val_loss</td><td>1.55252</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">online_sleep_staging_model</strong> at: <a href='https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification/runs/6dy7vixv' target=\"_blank\">https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification/runs/6dy7vixv</a><br> View project at: <a href='https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification' target=\"_blank\">https://wandb.ai/lakej98-nyu-langone-health/sleep_stage_classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250415_175254-6dy7vixv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the combined model\n",
    "wandb_logger = WandbLogger(name=\"online_sleep_staging_model\", project=\"sleep_stage_classification\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='best-checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    devices=1,\n",
    "    accelerator='gpu',\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback]\n",
    ")\n",
    "model = OnlineSleepStagingModel(in_channels=6, cnn_output_channels=32, lstm_hidden_size=128, num_layers=2, num_sleep_stages=5, learning_rate=1e-3, class_weights=class_weights)\n",
    "train_loader = DataLoader(train_chunk_dataset, batch_size=8, num_workers=8, shuffle=True)\n",
    "val_loader = DataLoader(val_chunk_dataset, batch_size=8, num_workers=8, shuffle=False)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "wandb.finish()\n",
    "# Load the best model\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "best_model = OnlineSleepStagingModel.load_from_checkpoint(best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b731ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    cnn_output_channels = trial.suggest_categorical(\"cnn_output_channels\", [8, 16, 32, 64])\n",
    "    lstm_hidden_size = trial.suggest_categorical(\"lstm_hidden_size\", [32, 64, 128])\n",
    "    num_layers = 2\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "\n",
    "    wandb_logger = WandbLogger(name=f\"CNN{cnn_output_channels}_hs{lstm_hidden_size}_lr{learning_rate}\", project=\"optuna_sleep_stage_classification\")\n",
    "    # DataLoaders (resample based on batch size)\n",
    "    train_loader = DataLoader(train_chunk_dataset, batch_size=16, num_workers=8, shuffle=True)\n",
    "val_loader = DataLoader(val_chunk_dataset, batch_size=16, num_workers=8, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model = OnlineSleepStagingModel(\n",
    "        in_channels=6,\n",
    "        cnn_output_channels=cnn_output_channels,\n",
    "        lstm_hidden_size=lstm_hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        num_sleep_stages=5,\n",
    "        learning_rate=learning_rate,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "\n",
    "    # Trainer with pruning callback\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,\n",
    "        devices=1,\n",
    "        accelerator=\"gpu\",\n",
    "        logger=wandb_logger,\n",
    "        enable_checkpointing=False,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    wandb.finish()\n",
    "    clear_output()\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "best_trial = study.best_trial\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988a5280",
   "metadata": {},
   "source": [
    "## CNN To Sleep Transformer approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3067a",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd221abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in train chunk dataset: 5107\n",
      "Total samples in val chunk dataset: 641\n",
      "Total samples in test chunk dataset: 665\n"
     ]
    }
   ],
   "source": [
    "train_chunk_dataset_noacc = SleepChunkDataset(subjects_list=subjects_train,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 chunk_duration=6000,  # 100 minutes\n",
    "                                 chunk_stride=300,    # 5 minutes\n",
    "                                 downsample_freq=0.2,   # downsample to 0.2 Hz\n",
    "                                 feature_columns=['TIMESTAMP', 'BVP', 'TEMP', 'HR', 'IBI'],\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in train chunk dataset: {len(train_chunk_dataset)}\")\n",
    "val_chunk_dataset_noacc = SleepChunkDataset(subjects_list=subjects_val,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 chunk_duration=6000,  # 100 minutes\n",
    "                                 chunk_stride=300,    # 5 minutes\n",
    "                                 downsample_freq=0.2,   # downsample to 0.2 Hz\n",
    "                                 feature_columns=['TIMESTAMP', 'BVP', 'TEMP', 'HR', 'IBI'],\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in val chunk dataset: {len(val_chunk_dataset)}\")\n",
    "test_chunk_dataset_noacc = SleepChunkDataset(subjects_list=subjects_test,\n",
    "                                 data_dir=datadir_64Hz,\n",
    "                                 chunk_duration=6000,  # 100 minutes\n",
    "                                 chunk_stride=300,    # 5 minutes\n",
    "                                 downsample_freq=0.2,   # downsample to 0.2 Hz\n",
    "                                 feature_columns=['TIMESTAMP', 'BVP', 'TEMP', 'HR', 'IBI'],\n",
    "                                 debug=False)\n",
    "print(f\"Total samples in test chunk dataset: {len(test_chunk_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a8cf03",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from x_transformers import Encoder, Decoder, ContinuousTransformerWrapper\n",
    "\n",
    "# Example PositionalEncoding module (you provided earlier)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, T, d_model]\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class Seq2SeqSleepStagerContinuous(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads, num_enc_layers,\n",
    "                 num_dec_layers, d_ff, dropout, num_classes, max_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Number of continuous features per timestep.\n",
    "            d_model (int): Transformer model dimension.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            num_enc_layers (int): Number of encoder layers.\n",
    "            num_dec_layers (int): Number of decoder layers.\n",
    "            d_ff (int): Feed-forward dimension (e.g., 256).\n",
    "            dropout (float): Dropout rate.\n",
    "            num_classes (int): Number of sleep stage labels.\n",
    "            max_length (int): Maximum sequence length.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # The continuous encoder; it internally projects the input features.\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            attn_layers=Encoder(\n",
    "                dim=d_model,\n",
    "                depth=num_enc_layers,\n",
    "                heads=num_heads,\n",
    "                ff_mult=d_ff // d_model,\n",
    "                attn_dropout=dropout\n",
    "            ),\n",
    "            dim_in = input_dim,\n",
    "            max_seq_len=max_length  # must be an integer\n",
    "        )\n",
    "        \n",
    "        # For the decoder, we assume sleep stage labels are discrete tokens.\n",
    "        # For example, if you have 5 sleep stages and a special SOS token:\n",
    "        vocab_size = num_classes + 1\n",
    "        self.decoder_embed = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Standard decoder with causal (masked) self-attention.\n",
    "        self.decoder = Decoder(\n",
    "            dim=d_model,\n",
    "            depth=num_dec_layers,\n",
    "            heads=num_heads,\n",
    "            ff_mult=d_ff // d_model,\n",
    "            attn_dropout=dropout,\n",
    "            cross_attend=True,\n",
    "        )\n",
    "        \n",
    "        # Positional encoding for decoder inputs.\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len=max_length)\n",
    "        \n",
    "        # Final linear projection maps decoder outputs to logits over tokens.\n",
    "        self.out_proj = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src (Tensor): Continuous input tensor of shape [B, src_len, input_dim]\n",
    "            tgt (Tensor): Target sequence tensor (token indices) of shape [B, tgt_len].\n",
    "                          For training, use teacher forcing with the target shifted right.\n",
    "        Returns:\n",
    "            logits (Tensor): Output logits of shape [B, tgt_len, vocab_size]\n",
    "        \"\"\"\n",
    "        # Process continuous input using the continuous encoder.\n",
    "        memory = self.encoder(src)  # shape: [B, src_len, d_model]\n",
    "        \n",
    "        # Embed the target token sequence.\n",
    "        dec_inp = self.decoder_embed(tgt)  # [B, tgt_len, d_model]\n",
    "        dec_inp = self.positional_encoding(dec_inp)  # add positional info\n",
    "        \n",
    "        # Decode using the encoder outputs as context.\n",
    "        dec_out = self.decoder(dec_inp, context=memory)  # [B, tgt_len, d_model]\n",
    "        logits = self.out_proj(dec_out)  # [B, tgt_len, vocab_size]\n",
    "        return logits\n",
    "\n",
    "class Seq2SeqSleepStagerPL(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (nn.Module): An instance of Seq2SeqSleepStagerContinuous.\n",
    "            learning_rate (float): Learning rate for the optimizer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        # Use cross-entropy loss; adjust ignore_index if needed.\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        return self.model(src, tgt)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Expect batch to be a tuple: (src, tgt).\n",
    "        src, tgt = batch\n",
    "        # In teacher forcing, the decoder input is the target sequence without its last token.\n",
    "        # The target for loss is the target sequence without its first token.\n",
    "        logits = self(src, tgt[:, :-1])\n",
    "        target = tgt[:, 1:]\n",
    "        # Flatten to compute token-level loss.\n",
    "        loss = self.criterion(logits.reshape(-1, logits.shape[-1]), target.reshape(-1))\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src, tgt = batch\n",
    "        logits = self(src, tgt[:, :-1])\n",
    "        target = tgt[:, 1:]\n",
    "        loss = self.criterion(logits.reshape(-1, logits.shape[-1]), target.reshape(-1))\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07fa05b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 300, 6])\n"
     ]
    }
   ],
   "source": [
    "# Demo of Transformer model\n",
    "# Example usage:\n",
    "\n",
    "# Example hyperparameters.\n",
    "batch_size = 2\n",
    "src_len = 300  # e.g., number of timesteps in sensor signal\n",
    "tgt_len = 300   # e.g., output token length (sleep stage sequence)\n",
    "input_dim = 6\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "num_enc_layers = 2\n",
    "num_dec_layers = 2\n",
    "d_ff = 256\n",
    "dropout = 0.1\n",
    "num_classes = 5\n",
    "max_length = 500  # maximum sequence length\n",
    "\n",
    "# Create dummy inputs:\n",
    "src = torch.randn(batch_size, src_len, input_dim)  # continuous sensor data\n",
    "tgt = torch.randint(0, num_classes + 1, (batch_size, tgt_len))  # discrete token indices\n",
    "\n",
    "model = Seq2SeqSleepStagerContinuous(input_dim, d_model, num_heads, num_enc_layers,\n",
    "                                           num_dec_layers, d_ff, dropout, num_classes, max_length)\n",
    "output_logits = model(src, tgt)\n",
    "print(\"Output shape:\", output_logits.shape)  # Expected: [B, tgt_len, vocab_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa3323",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63d99b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4m ...\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TransformerWrapper.__init__() missing 1 required keyword-only argument: 'num_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 24\u001b[0m\n\u001b[1;32m     10\u001b[0m early_stop_callback \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[1;32m     11\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     13\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     17\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     18\u001b[0m     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback, early_stop_callback]\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerSleepStager\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_ff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjusted for downsampling\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_chunk_dataset_noacc, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_chunk_dataset_noacc, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[26], line 22\u001b[0m, in \u001b[0;36mTransformerSleepStager.__init__\u001b[0;34m(self, input_dim, num_heads, num_layers, d_model, d_ff, max_length, dropout, num_classes)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding \u001b[38;5;241m=\u001b[39m PositionalEncoding(d_model, max_length) \u001b[38;5;66;03m# To do: handle max length here\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mff_mult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43md_ff\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_flash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(d_model, num_classes)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: TransformerWrapper.__init__() missing 1 required keyword-only argument: 'num_tokens'"
     ]
    }
   ],
   "source": [
    "# Train the transformer\n",
    "wandb_logger = WandbLogger(name=\"online_sleep_staging_model\", project=\"sleep_stage_classification\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='best-checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    devices=1,\n",
    "    accelerator='gpu',\n",
    "    logger=wandb_logger,\n",
    "    precision=16,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback]\n",
    ")\n",
    "model = TransformerSleepStager(\n",
    "    input_dim=6,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    d_model=128,\n",
    "    d_ff=256,\n",
    "    max_length= int(max_length // (64 // 0.2)),  # Adjusted for downsampling\n",
    "    dropout=0.1,\n",
    "    num_classes=5\n",
    ")\n",
    "train_loader = DataLoader(train_chunk_dataset_noacc, batch_size=16, num_workers=8, shuffle=True)\n",
    "val_loader = DataLoader(val_chunk_dataset_noacc, batch_size=16, num_workers=8, shuffle=False)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "wandb.finish()\n",
    "# Load the best model\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "best_model = OnlineSleepStagingModel.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d3983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project overview\n",
    "# construct dataset class\n",
    "# - train CNN -> LSTM sleep staging model with causal conv and unidirectional LSTM\n",
    "# - train CNN -> LSTM sleep staging model with non-causal conv and bidirectional LSTM\n",
    "# - compare performance at 64Hz\n",
    "# - compare performance and computational cost at lower sampling rates\n",
    "\n",
    "# unstructured notes\n",
    "# - likely best to combine ACC columns into a single variable. Can't imagine they offer much additional information\n",
    "# - CNN model should use 2D CNNs to extract features across both channels and short-term time windows\n",
    "# - will need to pad data on both sides to ensure input and output sequences for the CNN are the same length\n",
    "\n",
    "\n",
    "# CNN model - input: seq_len x num_channels -> output: seq_len x hidden_size\n",
    "\n",
    "\n",
    "# LSTM model - input: seq_len x hidden_size -> output: seq_len x num_classes\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl4med_25)",
   "language": "python",
   "name": "dl4med_25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
