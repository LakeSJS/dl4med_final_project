/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /gpfs/data/oermannlab/users/slj9342/dl4med_final_project/checkpoints exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                | Type                         | Params | Mode
-----------------------------------------------------------------------------
0 | encoder             | ContinuousTransformerWrapper | 1.4 M  | train
1 | decoder_embed       | Embedding                    | 768    | train
2 | decoder             | Decoder                      | 657 K  | train
3 | positional_encoding | PositionalEncoding           | 0      | train
4 | out_proj            | Linear                       | 774    | train
5 | criterion           | CrossEntropyLoss             | 0      | train
-----------------------------------------------------------------------------
2.1 M     Trainable params
0         Non-trainable params
2.1 M     Total params
8.218     Total estimated model params size (MB)
169       Modules in train mode
0         Modules in eval mode
Max length: 2493810
Max length after downsampling: 7817
/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4m ...
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /gpfs/data/oermannlab/users/slj9342/dl4med_final_project/checkpoints exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                | Type                         | Params | Mode
-----------------------------------------------------------------------------
0 | encoder             | ContinuousTransformerWrapper | 1.4 M  | train
1 | decoder_embed       | Embedding                    | 768    | train
2 | decoder             | Decoder                      | 657 K  | train
3 | positional_encoding | PositionalEncoding           | 0      | train
4 | out_proj            | Linear                       | 774    | train
5 | criterion           | CrossEntropyLoss             | 0      | train
-----------------------------------------------------------------------------
2.1 M     Trainable params
0         Non-trainable params
2.1 M     Total params
8.218     Total estimated model params size (MB)
169       Modules in train mode
0         Modules in eval mode
Monitored metric val_loss = nan is not finite. Previous best value was inf. Signaling Trainer to stop.
