/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /gpfs/data/oermannlab/users/slj9342/dl4med_final_project/checkpoints exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name              | Type                | Params | Mode
------------------------------------------------------------------
0 | feature_extractor | FeatureExtractorCNN | 34.6 K | train
1 | lstm_model        | SleepStageLSTM      | 54.6 K | train
2 | criterion         | CrossEntropyLoss    | 0      | train
------------------------------------------------------------------
89.2 K    Trainable params
0         Non-trainable params
89.2 K    Total params
0.357     Total estimated model params size (MB)
14        Modules in train mode
0         Modules in eval mode
/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4m ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name              | Type                | Params | Mode
------------------------------------------------------------------
0 | feature_extractor | FeatureExtractorCNN | 34.6 K | train
1 | lstm_model        | SleepStageLSTM      | 54.6 K | train
2 | criterion         | CrossEntropyLoss    | 0      | train
------------------------------------------------------------------
89.2 K    Trainable params
0         Non-trainable params
89.2 K    Total params
0.357     Total estimated model params size (MB)
14        Modules in train mode
0         Modules in eval mode
EXECUTABLE/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/bin/pythonEXECUTABLE
Input shape: torch.Size([120, 6]) (epoch_samples, channels)
/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4m ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /gpfs/data/oermannlab/users/slj9342/dl4med_final_project/checkpoints exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name              | Type                | Params | Mode
------------------------------------------------------------------
0 | feature_extractor | FeatureExtractorCNN | 35.0 K | train
1 | lstm_model        | SleepStageLSTM      | 54.6 K | train
2 | criterion         | CrossEntropyLoss    | 0      | train
------------------------------------------------------------------
89.6 K    Trainable params
0         Non-trainable params
89.6 K    Total params
0.358     Total estimated model params size (MB)
14        Modules in train mode
0         Modules in eval mode
Input shape: torch.Size([120, 6]) (epoch_samples, channels)
Output shape: torch.Size([1, 128, 1]) (batch_size, cnn_output_channels, epoch_samples)
Input shape: torch.Size([120, 6]) (epoch_samples, channels)
Output shape: torch.Size([1, 128, 1]) (batch_size, cnn_output_channels, epoch_samples)
Input shape: torch.Size([120, 6]) (epoch_samples, channels)
Output shape: torch.Size([1, 128, 1]) (batch_size, cnn_output_channels, epoch_samples)
Combined model output shape: torch.Size([1, 1, 5]) (samples, batch_size, num_sleep_stages)
1
EXECUTABLE/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/bin/pythonEXECUTABLE
Input shape: torch.Size([120, 6]) (epoch_samples, channels)
Output shape: torch.Size([1, 128, 1]) (batch_size, cnn_output_channels, epoch_samples)
Combined model output shape: torch.Size([1, 1, 5]) (samples, batch_size, num_sleep_stages)
1
/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4m ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
/gpfs/data/oermannlab/users/slj9342/.conda/envs/dl4med_25/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /gpfs/data/oermannlab/users/slj9342/dl4med_final_project/checkpoints exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name              | Type                | Params | Mode
------------------------------------------------------------------
0 | feature_extractor | FeatureExtractorCNN | 35.0 K | train
1 | lstm_model        | SleepStageLSTM      | 54.6 K | train
2 | criterion         | CrossEntropyLoss    | 0      | train
------------------------------------------------------------------
89.6 K    Trainable params
0         Non-trainable params
89.6 K    Total params
0.358     Total estimated model params size (MB)
14        Modules in train mode
0         Modules in eval mode
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name              | Type                | Params | Mode
------------------------------------------------------------------
0 | feature_extractor | FeatureExtractorCNN | 35.0 K | train
1 | lstm_model        | SleepStageLSTM      | 54.6 K | train
2 | criterion         | CrossEntropyLoss    | 0      | train
------------------------------------------------------------------
89.6 K    Trainable params
0         Non-trainable params
89.6 K    Total params
0.358     Total estimated model params size (MB)
14        Modules in train mode
0         Modules in eval mode
120.0
